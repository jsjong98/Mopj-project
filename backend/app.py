from flask import Flask, request, jsonify, send_file, make_response, Blueprint
from flask_cors import CORS, cross_origin
from statsmodels.tsa.statespace.varmax import VARMAX
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, precision_score, recall_score
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import f1_score, classification_report
import os
import json
import warnings
import random
import traceback
from datetime import datetime, timedelta
import matplotlib
matplotlib.use('Agg')  # ì„œë²„ì—ì„œ GUI ë°±ì—”ë“œ ì‚¬ìš© ì•ˆ í•¨
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from matplotlib.gridspec import GridSpec
import seaborn as sns
from werkzeug.utils import secure_filename
import io
import base64
import tempfile
import time
from threading import Thread
import logging
import calendar
import shutil
import optuna
import csv
from pathlib import Path
import math
import logging
import glob
import time
import xlwings as xw

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("app.log", encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)  # â† ì´ ì¤„ì´ ìˆëŠ”ì§€ í™•ì¸

# VARMAX ê´€ë ¨ import (ì„ íƒì  ê°€ì ¸ì˜¤ê¸°)
try:
    from statsmodels.tsa.statespace.varmax import VARMAX
    VARMAX_AVAILABLE = True
except ImportError:
    VARMAX_AVAILABLE = False
    logger.warning("VARMAX not available. Please install statsmodels.")


# ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸°
warnings.filterwarnings('ignore')

# ëœë¤ ì‹œë“œ ì„¤ì •
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed_all(SEED)  # GPU ì‚¬ìš© ì‹œ
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

def set_seed(seed=SEED):
    """
    ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ì‹œë“œë¥¼ ê³ ì •í•˜ì—¬ ì¼ê´€ëœ ì˜ˆì¸¡ ê²°ê³¼ ë³´ì¥
    """
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    
    # PyTorchì˜ deterministic ë™ì‘ ê°•ì œ
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    
    # Optuna ì‹œë“œ ì„¤ì • (í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”ìš©)
    try:
        import optuna
        # Optuna 2.x ë²„ì „ í˜¸í™˜
        if hasattr(optuna.samplers, 'RandomSampler'):
            optuna.samplers.RandomSampler(seed=seed)
        # ë ˆê±°ì‹œ ì§€ì›
        if hasattr(optuna.samplers, '_random'):
            optuna.samplers._random.seed(seed)
    except Exception as e:
        logger.debug(f"Optuna ì‹œë“œ ì„¤ì • ì‹¤íŒ¨: {e}")
    
    logger.debug(f"ğŸ¯ ëœë¤ ì‹œë“œ {seed}ë¡œ ê³ ì •ë¨")

# ë””ë ‰í† ë¦¬ ì„¤ì • - íŒŒì¼ë³„ ìºì‹œ ì‹œìŠ¤í…œ
UPLOAD_FOLDER = 'uploads'
HOLIDAY_DIR = 'holidays'
CACHE_ROOT_DIR = 'cache'  # ğŸ”‘ ìƒˆë¡œìš´ íŒŒì¼ë³„ ìºì‹œ ë£¨íŠ¸
PREDICTIONS_DIR = 'predictions'  # ê¸°ë³¸ ì˜ˆì¸¡ ë””ë ‰í† ë¦¬ (í˜¸í™˜ì„±ìš©)

# ê¸°ë³¸ ë””ë ‰í† ë¦¬ ìƒì„± (ìµœì†Œí•œë§Œ ìœ ì§€)
for d in [UPLOAD_FOLDER, CACHE_ROOT_DIR, PREDICTIONS_DIR]:
    os.makedirs(d, exist_ok=True)

def get_file_cache_dirs(file_path=None):
    """
    íŒŒì¼ë³„ ìºì‹œ ë””ë ‰í† ë¦¬ êµ¬ì¡°ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
    ğŸ¯ ê° íŒŒì¼ë§ˆë‹¤ ë…ë¦½ì ì¸ ëª¨ë¸, ì˜ˆì¸¡, ì‹œê°í™” ìºì‹œ ì œê³µ
    """
    try:
        if not file_path:
            file_path = prediction_state.get('current_file', None)
        
        # Debug: file cache directory setup
        
        if not file_path:
            logger.warning(f"âš ï¸ No file path provided and no current_file in prediction_state")
            # ê¸°ë³¸ ìºì‹œ ë””ë ‰í† ë¦¬ ë°˜í™˜ (íŒŒì¼ë³„ ìºì‹œ ì—†ì´)
            default_cache_root = Path(CACHE_ROOT_DIR) / 'default'
            dirs = {
                'root': default_cache_root,
                'models': default_cache_root / 'models',
                'predictions': default_cache_root / 'predictions',
                'plots': default_cache_root / 'static' / 'plots',
                'ma_plots': default_cache_root / 'static' / 'ma_plots',
                'accumulated': default_cache_root / 'accumulated'
            }
            
            # Create default directories
            for name, dir_path in dirs.items():
                try:
                    dir_path.mkdir(parents=True, exist_ok=True)
                except Exception as e:
                    logger.error(f"âŒ Failed to create default {name} directory {dir_path}: {str(e)}")
            
            logger.warning(f"âš ï¸ Using default cache directory")
            return dirs
        
        if not os.path.exists(file_path):
            logger.error(f"âŒ File does not exist: {file_path}")
            raise ValueError(f"File does not exist: {file_path}")
        
        # Generate file cache directory
        file_content_hash = get_data_content_hash(file_path)
        
        if not file_content_hash:
            logger.error(f"âŒ Failed to get content hash for file: {file_path}")
            raise ValueError(f"Unable to generate content hash for file: {file_path}")
        
        file_name = Path(file_path).stem
        file_dir_name = f"{file_content_hash[:12]}_{file_name}"
        file_cache_root = Path(CACHE_ROOT_DIR) / file_dir_name
        
        dirs = {
            'root': file_cache_root,
            'models': file_cache_root / 'models',
            'predictions': file_cache_root / 'predictions',
            'plots': file_cache_root / 'static' / 'plots',
            'ma_plots': file_cache_root / 'static' / 'ma_plots',
            'accumulated': file_cache_root / 'accumulated'
        }
        
        # Create cache directories
        for name, dir_path in dirs.items():
            try:
                dir_path.mkdir(parents=True, exist_ok=True)
            except Exception as e:
                logger.error(f"âŒ Failed to create {name} directory {dir_path}: {str(e)}")
        
        return dirs
        
    except Exception as e:
        logger.error(f"âŒ Error in get_file_cache_dirs: {str(e)}")
        logger.error(traceback.format_exc())
        raise e  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì˜ˆì™¸ ì „íŒŒ

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',  # ë” ê°„ê²°í•œ ë¡œê·¸ í¬ë§·
    handlers=[
        logging.FileHandler("app.log", encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def check_gpu_availability():
    """GPU ì‚¬ìš© ê°€ëŠ¥ì„± ë° í˜„ì¬ ë””ë°”ì´ìŠ¤ ì •ë³´ë¥¼ í™•ì¸í•˜ê³  ë¡œê¹…í•˜ëŠ” í•¨ìˆ˜"""
    try:
        logger.info("=" * 60)
        logger.info("ğŸ” GPU ë° ë””ë°”ì´ìŠ¤ ì •ë³´ í™•ì¸")
        logger.info("=" * 60)
        
        # CUDA ì‚¬ìš© ê°€ëŠ¥ì„± í™•ì¸
        cuda_available = torch.cuda.is_available()
        logger.info(f"ğŸ”§ CUDA ì‚¬ìš© ê°€ëŠ¥: {cuda_available}")
        
        if cuda_available:
            # GPU ê°œìˆ˜ ë° ì •ë³´
            gpu_count = torch.cuda.device_count()
            logger.info(f"ğŸ® ì‚¬ìš© ê°€ëŠ¥í•œ GPU ê°œìˆ˜: {gpu_count}")
            
            # ê° GPU ì •ë³´ ì¶œë ¥
            for i in range(gpu_count):
                try:
                    gpu_name = torch.cuda.get_device_name(i)
                    gpu_props = torch.cuda.get_device_properties(i)
                    gpu_memory = gpu_props.total_memory / 1024**3  # GB
                    
                    # ì¶”ê°€ ì •ë³´ ìˆ˜ì§‘ (ì•ˆì „í•œ ë°©ë²•)
                    compute_capability = f"{getattr(gpu_props, 'major', 0)}.{getattr(gpu_props, 'minor', 0)}"
                    
                    logger.info(f"  ğŸ“± GPU {i}: {gpu_name} ({gpu_memory:.1f}GB, Compute {compute_capability})")
                    
                    # ë©€í‹°í”„ë¡œì„¸ì„œ ê°œìˆ˜ (ì¡´ì¬í•˜ëŠ” ê²½ìš°)
                    if hasattr(gpu_props, 'multiprocessor_count'):
                        mp_count = gpu_props.multiprocessor_count
                        logger.info(f"    ğŸ”§ ë©€í‹°í”„ë¡œì„¸ì„œ: {mp_count}ê°œ")
                    elif hasattr(gpu_props, 'multi_processor_count'):
                        mp_count = gpu_props.multi_processor_count
                        logger.info(f"    ğŸ”§ ë©€í‹°í”„ë¡œì„¸ì„œ: {mp_count}ê°œ")
                        
                except Exception as e:
                    logger.warning(f"  âš ï¸ GPU {i} ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
                    logger.info(f"  ğŸ“± GPU {i}: ì •ë³´ í™•ì¸ ë¶ˆê°€")
            
            # í˜„ì¬ GPU ë””ë°”ì´ìŠ¤
            current_device = torch.cuda.current_device()
            current_gpu_name = torch.cuda.get_device_name(current_device)
            logger.info(f"ğŸ¯ í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ GPU: {current_device} ({current_gpu_name})")
            
                    # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated(current_device) / 1024**3
            cached = torch.cuda.memory_reserved(current_device) / 1024**3
            logger.info(f"ğŸ’¾ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {allocated:.2f}GB (í• ë‹¹) / {cached:.2f}GB (ìºì‹œ)")
            
            # ê°„ë‹¨í•œ GPU í…ŒìŠ¤íŠ¸ ìˆ˜í–‰
            try:
                logger.info("ğŸ§ª GPU ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
                test_tensor = torch.randn(1000, 1000, device=current_device)
                test_result = torch.matmul(test_tensor, test_tensor.T)
                
                # í…ŒìŠ¤íŠ¸ í›„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¬í™•ì¸
                allocated_after = torch.cuda.memory_allocated(current_device) / 1024**3
                cached_after = torch.cuda.memory_reserved(current_device) / 1024**3
                logger.info(f"âœ… GPU í…ŒìŠ¤íŠ¸ ì™„ë£Œ! í…ŒìŠ¤íŠ¸ í›„ ë©”ëª¨ë¦¬: {allocated_after:.2f}GB (í• ë‹¹) / {cached_after:.2f}GB (ìºì‹œ)")
                
                # í…ŒìŠ¤íŠ¸ í…ì„œ ì •ë¦¬
                del test_tensor, test_result
                torch.cuda.empty_cache()
                
                # ì •ë¦¬ í›„ ë©”ëª¨ë¦¬ ìƒíƒœ
                allocated_final = torch.cuda.memory_allocated(current_device) / 1024**3
                cached_final = torch.cuda.memory_reserved(current_device) / 1024**3
                logger.info(f"ğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬ í›„: {allocated_final:.2f}GB (í• ë‹¹) / {cached_final:.2f}GB (ìºì‹œ)")
                
            except Exception as e:
                logger.error(f"âŒ GPU í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
        
        # ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ ê²°ì •
        device = torch.device('cuda' if cuda_available else 'cpu')
        logger.info(f"âš¡ ëª¨ë¸ í•™ìŠµ/ì˜ˆì¸¡ì— ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤: {device}")
        
        # PyTorch ë²„ì „ ì •ë³´
        logger.info(f"ğŸ”¢ PyTorch ë²„ì „: {torch.__version__}")
        
        # CUDNN ì •ë³´ (CUDA ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
        if cuda_available:
            try:
                logger.info(f"ğŸ”§ cuDNN ë²„ì „: {torch.backends.cudnn.version()}")
                logger.info(f"ğŸ”§ cuDNN í™œì„±í™”: {torch.backends.cudnn.enabled}")
            except Exception as e:
                logger.warning(f"âš ï¸ cuDNN ì •ë³´ í™•ì¸ ì‹¤íŒ¨: {str(e)}")
                
            # GPU ì†ì„± ë””ë²„ê¹… ì •ë³´ (ì²« ë²ˆì§¸ GPUë§Œ)
            if gpu_count > 0:
                try:
                    props = torch.cuda.get_device_properties(0)
                    available_attrs = [attr for attr in dir(props) if not attr.startswith('_')]
                    logger.info(f"ğŸ” ì‚¬ìš© ê°€ëŠ¥í•œ GPU ì†ì„±ë“¤: {available_attrs}")
                except Exception as e:
                    logger.warning(f"âš ï¸ GPU ì†ì„± í™•ì¸ ì‹¤íŒ¨: {str(e)}")
        
        logger.info("=" * 60)
        
        return device, cuda_available
        
    except Exception as e:
        logger.error(f"âŒ GPU ì •ë³´ í™•ì¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        logger.error(traceback.format_exc())
        return torch.device('cpu'), False

def get_detailed_gpu_utilization():
    """nvidia-smië¥¼ ì‚¬ìš©í•˜ì—¬ ìƒì„¸í•œ GPU í™œìš©ë¥ ì„ í™•ì¸í•˜ëŠ” í•¨ìˆ˜"""
    try:
        import subprocess
        
        # ê¸°ë³¸ í™œìš©ë¥  ì •ë³´
        basic_result = subprocess.run([
            'nvidia-smi', 
            '--query-gpu=utilization.gpu,utilization.memory,temperature.gpu,power.draw,power.limit',
            '--format=csv,noheader,nounits'
        ], capture_output=True, text=True, timeout=5)
        
        # ìƒì„¸ í™œìš©ë¥  ì •ë³´ (Encoder, Decoder ë“±)
        detailed_result = subprocess.run([
            'nvidia-smi', 
            '--query-gpu=utilization.gpu,utilization.memory,utilization.encoder,utilization.decoder',
            '--format=csv,noheader,nounits'
        ], capture_output=True, text=True, timeout=5)
        
        # ì‹¤í–‰ ì¤‘ì¸ í”„ë¡œì„¸ìŠ¤ ì •ë³´
        process_result = subprocess.run([
            'nvidia-smi', 
            '--query-compute-apps=pid,process_name,used_gpu_memory',
            '--format=csv,noheader,nounits'
        ], capture_output=True, text=True, timeout=5)
        
        gpu_stats = []
        
        if basic_result.returncode == 0 and basic_result.stdout.strip():
            basic_lines = basic_result.stdout.strip().split('\n')
            detailed_lines = detailed_result.stdout.strip().split('\n') if detailed_result.returncode == 0 else []
            
            for i, line in enumerate(basic_lines):
                parts = line.split(', ')
                if len(parts) >= 3:
                    gpu_util = parts[0].strip()
                    mem_util = parts[1].strip()
                    temp = parts[2].strip()
                    power_draw = parts[3].strip() if len(parts) > 3 else 'N/A'
                    power_limit = parts[4].strip() if len(parts) > 4 else 'N/A'
                    
                    # ìƒì„¸ ì •ë³´ ì¶”ê°€
                    encoder_util = 'N/A'
                    decoder_util = 'N/A'
                    
                    if i < len(detailed_lines):
                        detailed_parts = detailed_lines[i].split(', ')
                        if len(detailed_parts) >= 4:
                            encoder_util = detailed_parts[2].strip()
                            decoder_util = detailed_parts[3].strip()
                    
                    gpu_stat = {
                        'gpu_id': i,
                        'gpu_utilization': gpu_util,
                        'memory_utilization': mem_util,
                        'encoder_utilization': encoder_util,
                        'decoder_utilization': decoder_util,
                        'temperature': temp,
                        'power_draw': power_draw,
                        'power_limit': power_limit,
                        'measurement_method': 'nvidia-smi',
                        'timestamp': time.time()
                    }
                    
                    gpu_stats.append(gpu_stat)
        
        # ì‹¤í–‰ ì¤‘ì¸ í”„ë¡œì„¸ìŠ¤ ì •ë³´ ì¶”ê°€
        if process_result.returncode == 0 and process_result.stdout.strip():
            process_lines = process_result.stdout.strip().split('\n')
            compute_processes = []
            for line in process_lines:
                parts = line.split(', ')
                if len(parts) >= 3:
                    compute_processes.append({
                        'pid': parts[0].strip(),
                        'name': parts[1].strip(),
                        'gpu_memory_mb': parts[2].strip()
                    })
            
            # ì²« ë²ˆì§¸ GPUì— í”„ë¡œì„¸ìŠ¤ ì •ë³´ ì¶”ê°€
            if gpu_stats:
                gpu_stats[0]['compute_processes'] = compute_processes
        
        return gpu_stats
        
    except Exception as e:
        logger.warning(f"âš ï¸ ìƒì„¸ GPU í™œìš©ë¥  í™•ì¸ ì‹¤íŒ¨: {str(e)}")
        return None

def get_gpu_utilization():
    """nvidia-smië¥¼ ì‚¬ìš©í•˜ì—¬ GPU í™œìš©ë¥ ì„ í™•ì¸í•˜ëŠ” í•¨ìˆ˜ (ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€)"""
    detailed_stats = get_detailed_gpu_utilization()
    if detailed_stats:
        # ê¸°ì¡´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        return [{
            'gpu_id': stat['gpu_id'],
            'gpu_utilization': stat['gpu_utilization'],
            'memory_utilization': stat['memory_utilization'],
            'temperature': stat['temperature'],
            'power_draw': stat['power_draw'],
            'power_limit': stat['power_limit']
        } for stat in detailed_stats]
    return None

def compare_gpu_monitoring_methods():
    """ë‹¤ì–‘í•œ GPU ëª¨ë‹ˆí„°ë§ ë°©ë²•ì„ ë¹„êµí•˜ëŠ” í•¨ìˆ˜"""
    comparison_results = {
        'nvidia_smi': None,
        'torch_cuda': None,
        'monitoring_notes': []
    }
    
    try:
        # nvidia-smi ê²°ê³¼
        nvidia_stats = get_detailed_gpu_utilization()
        if nvidia_stats:
            comparison_results['nvidia_smi'] = nvidia_stats[0]  # ì²« ë²ˆì§¸ GPU
            comparison_results['monitoring_notes'].append(
                "nvidia-smi: CUDA ì—°ì‚° í™œìš©ë¥  ì¸¡ì • (ML/AI ì‘ì—…ì— ì •í™•)"
            )
        
        # PyTorch CUDA ì •ë³´
        if torch.cuda.is_available():
            device_id = torch.cuda.current_device()
            allocated = torch.cuda.memory_allocated(device_id) / 1024**3
            cached = torch.cuda.memory_reserved(device_id) / 1024**3
            total = torch.cuda.get_device_properties(device_id).total_memory / 1024**3
            
            comparison_results['torch_cuda'] = {
                'allocated_memory_gb': round(allocated, 3),
                'cached_memory_gb': round(cached, 3),
                'total_memory_gb': round(total, 1),
                'memory_usage_percent': round((allocated / total) * 100, 2)
            }
            comparison_results['monitoring_notes'].append(
                "PyTorch CUDA: ì‹¤ì œ PyTorch í…ì„œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰"
            )
        
        comparison_results['monitoring_notes'].extend([
            "Windows ì‘ì—… ê´€ë¦¬ì: ì£¼ë¡œ 3D ê·¸ë˜í”½ ì—”ì§„ í™œìš©ë¥  (CUDAì™€ ë‹¤ë¦„)",
            "nvidia-smi GPU í™œìš©ë¥ : CUDA ì—°ì‚° í™œìš©ë¥  (ML/AI ì‘ì—…)",
            "nvidia-smi Encoder/Decoder: ë¹„ë””ì˜¤ ì¸ì½”ë”©/ë””ì½”ë”© í™œìš©ë¥ ",
            "ì¸¡ì • ì‹œì ì— ë”°ë¼ ìˆœê°„ì ì¸ ë³€í™”ê°€ í´ ìˆ˜ ìˆìŒ"
        ])
        
    except Exception as e:
        comparison_results['error'] = str(e)
    
    return comparison_results

def log_device_usage(device, context=""):
    """íŠ¹ì • ìƒí™©ì—ì„œì˜ ë””ë°”ì´ìŠ¤ ì‚¬ìš© ì •ë³´ë¥¼ ë¡œê¹…í•˜ëŠ” í•¨ìˆ˜"""
    try:
        context_str = f"[{context}] " if context else ""
        logger.info(f"ğŸ¯ {context_str}ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤: {device}")
        
        if device.type == 'cuda' and torch.cuda.is_available():
            device_id = device.index if device.index is not None else torch.cuda.current_device()
            allocated = torch.cuda.memory_allocated(device_id) / 1024**3
            cached = torch.cuda.memory_reserved(device_id) / 1024**3
            total = torch.cuda.get_device_properties(device_id).total_memory / 1024**3
            
            logger.info(f"ğŸ’¾ {context_str}GPU ë©”ëª¨ë¦¬: {allocated:.3f}GB ì‚¬ìš© / {total:.1f}GB ì „ì²´ (ìºì‹œ: {cached:.3f}GB)")
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ê³„ì‚° ë° ìƒíƒœ í‘œì‹œ
            usage_percentage = (allocated / total) * 100
            cache_percentage = (cached / total) * 100
            
            if allocated > 0.001:  # 1MB ì´ìƒ ì‚¬ìš© ì¤‘ì¸ ê²½ìš°
                logger.info(f"ğŸ“Š {context_str}ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : {usage_percentage:.2f}% (ìºì‹œ: {cache_percentage:.2f}%)")
                
                if usage_percentage > 80:
                    logger.warning(f"âš ï¸ {context_str}GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ ì´ ë†’ìŠµë‹ˆë‹¤: {usage_percentage:.1f}%")
                elif usage_percentage > 50:
                    logger.info(f"ğŸ“ˆ {context_str}GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : {usage_percentage:.1f}% (ì •ìƒ)")
            else:
                logger.info(f"ğŸ’­ {context_str}í˜„ì¬ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì—†ìŒ (ëŒ€ê¸° ìƒíƒœ)")
            
            # GPU í™œìš©ë¥  í™•ì¸ (ìƒì„¸)
            detailed_stats = get_detailed_gpu_utilization()
            if detailed_stats and len(detailed_stats) > device_id:
                stat = detailed_stats[device_id]
                logger.info(f"âš¡ {context_str}CUDA í™œìš©ë¥ : {stat['gpu_utilization']}% (ë©”ëª¨ë¦¬: {stat['memory_utilization']}%)")
                logger.info(f"ğŸ¬ {context_str}Encoder: {stat['encoder_utilization']}%, Decoder: {stat['decoder_utilization']}%")
                logger.info(f"ğŸŒ¡ï¸ {context_str}GPU ì˜¨ë„: {stat['temperature']}Â°C, ì „ë ¥: {stat['power_draw']}/{stat['power_limit']}W")
                
                # ì‹¤í–‰ ì¤‘ì¸ í”„ë¡œì„¸ìŠ¤ ì •ë³´
                if 'compute_processes' in stat and stat['compute_processes']:
                    process_count = len(stat['compute_processes'])
                    logger.info(f"ğŸ”„ {context_str}CUDA í”„ë¡œì„¸ìŠ¤: {process_count}ê°œ")
                    for proc in stat['compute_processes'][:3]:  # ìµœëŒ€ 3ê°œê¹Œì§€ë§Œ í‘œì‹œ
                        logger.info(f"    ğŸ“± PID {proc['pid']}: {proc['name']} ({proc['gpu_memory_mb']}MB)")
                
                # ë‚®ì€ í™œìš©ë¥  ë¶„ì„ ë° ì„¤ëª…
                try:
                    gpu_util_num = float(stat['gpu_utilization'])
                    if gpu_util_num < 10:
                        logger.warning(f"âš ï¸ {context_str}CUDA í™œìš©ë¥ ì´ ë§¤ìš° ë‚®ìŠµë‹ˆë‹¤: {gpu_util_num}%")
                        logger.info(f"ğŸ’¡ {context_str}ì°¸ê³ : ì‘ì—… ê´€ë¦¬ìì˜ GPUëŠ” 3D ê·¸ë˜í”½ì„, nvidia-smiëŠ” CUDA ì—°ì‚°ì„ ì¸¡ì •í•©ë‹ˆë‹¤")
                        logger.info(f"ğŸ’¡ {context_str}ML/AI ì‘ì—…ì—ì„œëŠ” nvidia-smiì˜ CUDA í™œìš©ë¥ ì´ ì •í™•í•©ë‹ˆë‹¤")
                    elif gpu_util_num < 30:
                        logger.info(f"ğŸ“‰ {context_str}CUDA í™œìš©ë¥ ì´ ë‚®ìŠµë‹ˆë‹¤: {gpu_util_num}% - ë°°ì¹˜ í¬ê¸° ì¦ê°€ ê³ ë ¤")
                    else:
                        logger.info(f"âœ… {context_str}CUDA í™œìš©ë¥ ì´ ì–‘í˜¸í•©ë‹ˆë‹¤: {gpu_util_num}%")
                except:
                    pass
                
            # GPU í™œì„± í”„ë¡œì„¸ìŠ¤ ìˆ˜ í™•ì¸ (ê°€ëŠ¥í•œ ê²½ìš°)
            try:
                import subprocess
                result = subprocess.run(['nvidia-smi', '--query-compute-apps=pid,process_name,used_memory', '--format=csv,noheader,nounits'], 
                                      capture_output=True, text=True, timeout=5)
                if result.returncode == 0 and result.stdout.strip():
                    active_processes = len(result.stdout.strip().split('\n'))
                    logger.info(f"ğŸ”„ {context_str}GPUì—ì„œ ì‹¤í–‰ ì¤‘ì¸ í”„ë¡œì„¸ìŠ¤: {active_processes}ê°œ")
            except:
                pass  # nvidia-smiê°€ ì—†ê±°ë‚˜ ì‹¤í–‰ ì‹¤íŒ¨ ì‹œ ë¬´ì‹œ
                
        elif device.type == 'cpu':
            logger.info(f"ğŸ–¥ï¸ {context_str}CPU ëª¨ë“œë¡œ ì‹¤í–‰ ì¤‘")
            
    except Exception as e:
        logger.error(f"âŒ ë””ë°”ì´ìŠ¤ ì‚¬ìš© ì •ë³´ ë¡œê¹… ì¤‘ ì˜¤ë¥˜: {str(e)}")

# GPU ì •ë³´ í™•ì¸ ë° ê¸°ë³¸ ë””ë°”ì´ìŠ¤ ì„¤ì •
DEFAULT_DEVICE, CUDA_AVAILABLE = check_gpu_availability()

# Flask ì„¤ì •
app = Flask(__name__)
CORS(app, resources={r"/api/*": {"origins": "*"}}, supports_credentials=False)
app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER
app.config['MAX_CONTENT_LENGTH'] = 32 * 1024 * 1024  # ìµœëŒ€ íŒŒì¼ í¬ê¸° 32MBë¡œ ì¦ê°€

# ì „ì—­ ìƒíƒœ ë³€ìˆ˜ì— ìƒˆ í•„ë“œ ì¶”ê°€
prediction_state = {
    'current_data': None,
    'latest_predictions': None,
    'latest_interval_scores': None,
    'latest_attention_data': None,
    'latest_ma_results': None,
    'latest_plots': None,  # ì¶”ê°€
    'latest_metrics': None,  # ì¶”ê°€
    'current_date': None,
    'current_file': None,  # ì¶”ê°€: í˜„ì¬ íŒŒì¼ ê²½ë¡œ
    'is_predicting': False,  # LSTM ì˜ˆì¸¡ ìƒíƒœ
    'prediction_progress': 0,
    'prediction_start_time': None,  # ì˜ˆì¸¡ ì‹œì‘ ì‹œê°„
    'error': None,
    'selected_features': None,
    'feature_importance': None,
    'semimonthly_period': None,
    'next_semimonthly_period': None,
    'accumulated_predictions': [],
    'accumulated_metrics': {},
    'prediction_dates': [],
    'accumulated_consistency_scores': {},
    # VARMAX ê´€ë ¨ ìƒíƒœ ë³€ìˆ˜ (ë…ë¦½ì ì¸ ìƒíƒœ ê´€ë¦¬)
    'varmax_predictions': None,
    'varmax_metrics': None,
    'varmax_ma_results': None,
    'varmax_selected_features': None,
    'varmax_current_date': None,
    'varmax_model_info': None,
    'varmax_plots': None,
    'varmax_is_predicting': False,  # ğŸ†• VARMAX ë…ë¦½ ì˜ˆì¸¡ ìƒíƒœ
    'varmax_prediction_progress': 0,  # ğŸ†• VARMAX ë…ë¦½ ì§„í–‰ë¥ 
    'varmax_prediction_start_time': None,  # ğŸ†• VARMAX ì˜ˆì¸¡ ì‹œì‘ ì‹œê°„
    'varmax_error': None,  # ğŸ†• VARMAX ë…ë¦½ ì—ëŸ¬ ìƒíƒœ
}

# ë°ì´í„° ë¡œë”ì˜ ì›Œì»¤ ì‹œë“œ ê³ ì •ì„ ìœ„í•œ í•¨ìˆ˜
def seed_worker(worker_id):
    """DataLoader worker ì‹œë“œ ê³ ì •"""
    # ê¸°ì¡´ ì‹œë“œ ê³ ì • ë°©ì‹ ìœ ì§€í•˜ë˜ ê°•í™”
    set_seed(SEED)

# ë°ì´í„° ë¡œë”ì˜ ìƒì„±ì ì‹œë“œ ê³ ì •
g = torch.Generator()
g.manual_seed(SEED)

def calculate_estimated_time_remaining(start_time, current_progress):
    """
    ì˜ˆì¸¡ ì‹œì‘ ì‹œê°„ê³¼ í˜„ì¬ ì§„í–‰ë¥ ì„ ê¸°ë°˜ìœ¼ë¡œ ë‚¨ì€ ì‹œê°„ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
    
    Args:
        start_time: ì˜ˆì¸¡ ì‹œì‘ ì‹œê°„ (time.time() ê°’)
        current_progress: í˜„ì¬ ì§„í–‰ë¥  (0-100)
    
    Returns:
        dict: {
            'estimated_remaining_seconds': int,
            'estimated_remaining_text': str,
            'elapsed_time_seconds': int,
            'elapsed_time_text': str
        }
    """
    if not start_time or current_progress <= 0:
        return {
            'estimated_remaining_seconds': None,
            'estimated_remaining_text': 'ê³„ì‚° ì¤‘...',
            'elapsed_time_seconds': 0,
            'elapsed_time_text': '0ì´ˆ'
        }
    
    current_time = time.time()
    elapsed_time = current_time - start_time
    
    # ì§„í–‰ë¥ ì´ 100% ì´ìƒì´ë©´ ì™„ë£Œ
    if current_progress >= 100:
        return {
            'estimated_remaining_seconds': 0,
            'estimated_remaining_text': 'ì™„ë£Œ',
            'elapsed_time_seconds': int(elapsed_time),
            'elapsed_time_text': format_time_duration(int(elapsed_time))
        }
    
    # ì§„í–‰ë¥ ì„ ê¸°ë°˜ìœ¼ë¡œ ì´ ì˜ˆìƒ ì‹œê°„ ê³„ì‚°
    estimated_total_time = elapsed_time * (100 / current_progress)
    estimated_remaining_time = estimated_total_time - elapsed_time
    
    # ìŒìˆ˜ê°€ ë˜ì§€ ì•Šë„ë¡ ë³´ì •
    estimated_remaining_time = max(0, estimated_remaining_time)
    
    return {
        'estimated_remaining_seconds': int(estimated_remaining_time),
        'estimated_remaining_text': format_time_duration(int(estimated_remaining_time)),
        'elapsed_time_seconds': int(elapsed_time),
        'elapsed_time_text': format_time_duration(int(elapsed_time))
    }

def format_time_duration(seconds):
    """ì‹œê°„ì„ ì‚¬ëŒì´ ì½ê¸° ì‰¬ìš´ í˜•íƒœë¡œ í¬ë§·íŒ…"""
    if seconds < 60:
        return f"{seconds}ì´ˆ"
    elif seconds < 3600:
        minutes = seconds // 60
        remaining_seconds = seconds % 60
        if remaining_seconds > 0:
            return f"{minutes}ë¶„ {remaining_seconds}ì´ˆ"
        else:
            return f"{minutes}ë¶„"
    else:
        hours = seconds // 3600
        remaining_minutes = (seconds % 3600) // 60
        if remaining_minutes > 0:
            return f"{hours}ì‹œê°„ {remaining_minutes}ë¶„"
        else:
            return f"{hours}ì‹œê°„"

#######################################################################
# ëª¨ë¸ ë° ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
#######################################################################

# ë‚ ì§œ í¬ë§·íŒ… ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
def format_date(date_obj, format_str='%Y-%m-%d'):
    """ë‚ ì§œ ê°ì²´ë¥¼ ë¬¸ìì—´ë¡œ ì•ˆì „í•˜ê²Œ ë³€í™˜"""
    try:
        # pandas Timestamp ë˜ëŠ” datetime.datetime
        if hasattr(date_obj, 'strftime'):
            return date_obj.strftime(format_str)
        
        # numpy.datetime64
        elif isinstance(date_obj, np.datetime64):
            # ë‚ ì§œ í¬ë§·ì´ 'YYYY-MM-DD'ì¸ ê²½ìš°
            return str(date_obj)[:10]
        
        # ë¬¸ìì—´ì¸ ê²½ìš° ì´ë¯¸ ë‚ ì§œ í˜•ì‹ì´ë¼ë©´ ì¶”ê°€ ì²˜ë¦¬
        elif isinstance(date_obj, str):
            # GMT í˜•ì‹ì´ë©´ íŒŒì‹±í•˜ì—¬ ë³€í™˜
            if 'GMT' in date_obj:
                parsed_date = datetime.strptime(date_obj, '%a, %d %b %Y %H:%M:%S GMT')
                return parsed_date.strftime(format_str)
            return date_obj[:10] if len(date_obj) > 10 else date_obj
        
        # ê·¸ ì™¸ ê²½ìš°
        else:
            return str(date_obj)
    
    except Exception as e:
        logger.warning(f"ë‚ ì§œ í¬ë§·íŒ… ì˜¤ë¥˜: {str(e)}")
        return str(date_obj)

# ğŸ”§ ìŠ¤ë§ˆíŠ¸ íŒŒì¼ ìºì‹œ ì‹œìŠ¤í…œ í•¨ìˆ˜ë“¤
def calculate_file_hash(file_path, chunk_size=8192):
    """íŒŒì¼ ë‚´ìš©ì˜ SHA256 í•´ì‹œë¥¼ ê³„ì‚°"""
    import hashlib
    
    hash_sha256 = hashlib.sha256()
    try:
        with open(file_path, 'rb') as f:
            while chunk := f.read(chunk_size):
                hash_sha256.update(chunk)
        return hash_sha256.hexdigest()
    except Exception as e:
        logger.error(f"File hash calculation failed: {str(e)}")
        return None

# íŒŒì¼ í•´ì‹œ ìºì‹œ ì¶”ê°€ (ë©”ëª¨ë¦¬ ìºì‹±ìœ¼ë¡œ ì„±ëŠ¥ ìµœì í™”)
_file_hash_cache = {}
_cache_lookup_index = {}  # ë¹ ë¥¸ ìºì‹œ ê²€ìƒ‰ì„ ìœ„í•œ ì¸ë±ìŠ¤

# ğŸ”§ DataFrame ë©”ëª¨ë¦¬ ìºì‹œ (ì¤‘ë³µ ë¡œë”© ë°©ì§€)
_dataframe_cache = {}
_cache_expiry_seconds = 120  # 2ë¶„ê°„ ìºì‹œ ìœ ì§€

def get_data_content_hash(file_path):
    """ë°ì´í„° íŒŒì¼(CSV/Excel)ì˜ ì „ì²˜ë¦¬ëœ ë‚´ìš©ìœ¼ë¡œ í•´ì‹œ ìƒì„± (ìºì‹± ìµœì í™”)"""
    import hashlib
    import os
    
    try:
        # íŒŒì¼ ìˆ˜ì • ì‹œê°„ ê¸°ë°˜ ìºì‹œ í™•ì¸
        if file_path in _file_hash_cache:
            cached_mtime, cached_hash = _file_hash_cache[file_path]
            current_mtime = os.path.getmtime(file_path)
            
            # íŒŒì¼ì´ ìˆ˜ì •ë˜ì§€ ì•Šì•˜ë‹¤ë©´ ìºì‹œëœ í•´ì‹œ ë°˜í™˜
            if abs(current_mtime - cached_mtime) < 1.0:  # 1ì´ˆ ì´ë‚´ ì°¨ì´ëŠ” ë¬´ì‹œ
                logger.debug(f"ğŸ“‹ Using cached hash for {os.path.basename(file_path)}")
                return cached_hash
        
        # íŒŒì¼ì´ ìˆ˜ì •ë˜ì—ˆê±°ë‚˜ ìºì‹œê°€ ì—†ëŠ” ê²½ìš° ìƒˆë¡œ ê³„ì‚°
        logger.info(f"ğŸ”„ Calculating new hash for {os.path.basename(file_path)}")
        
        # íŒŒì¼ í˜•ì‹ì— ë§ê²Œ ë¡œë“œ
        file_ext = os.path.splitext(file_path.lower())[1]
        if file_ext == '.csv':
            df = pd.read_csv(file_path)
        else:
            # Excel íŒŒì¼ì¸ ê²½ìš° load_data í•¨ìˆ˜ ì‚¬ìš© (ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¡œ í•´ì‹œ ìƒì„±)
            df = load_data(file_path)
            # ì¸ë±ìŠ¤ê°€ Dateì¸ ê²½ìš° ì»¬ëŸ¼ìœ¼ë¡œ ë³µì›
            if df.index.name == 'Date':
                df = df.reset_index()
        
        if 'Date' in df.columns:
            # ë‚ ì§œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ì¼ê´€ëœ í•´ì‹œ ìƒì„±
            df = df.sort_values('Date')
        
        # ğŸ”§ DataFrameì„ ì•ˆì „í•˜ê²Œ ë¬¸ìì—´ë¡œ ë³€í™˜
        # ë¬´í•œëŒ€ë‚˜ NaN ê°’ì„ ì²˜ë¦¬í•˜ì—¬ í•´ì‹œ ê³„ì‚° ì˜¤ë¥˜ ë°©ì§€
        df_for_hash = df.copy()
        
        # ë¬´í•œëŒ€ì™€ NaN ê°’ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
        df_for_hash = df_for_hash.replace([np.inf, -np.inf], 'inf')
        df_for_hash = df_for_hash.fillna('nan')
        
        # ëª¨ë“  ì»¬ëŸ¼ì„ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ì•ˆì „í•œ í•´ì‹œ ê³„ì‚°
        try:
            content_str = df_for_hash.to_string()
        except Exception as str_error:
            logger.warning(f"DataFrame to_string failed, using alternative method: {str(str_error)}")
            # ëŒ€ì•ˆ: ê° ì»¬ëŸ¼ì„ ê°œë³„ì ìœ¼ë¡œ ë¬¸ìì—´ë¡œ ë³€í™˜
            content_parts = []
            for col in df_for_hash.columns:
                try:
                    col_str = str(col) + ":" + str(df_for_hash[col].tolist())
                    content_parts.append(col_str)
                except Exception:
                    content_parts.append(f"{col}:error")
            content_str = "|".join(content_parts)
        
        file_hash = hashlib.sha256(content_str.encode('utf-8', errors='ignore')).hexdigest()[:16]  # ì§§ì€ í•´ì‹œ ì‚¬ìš©
        
        # ìºì‹œ ì €ì¥
        _file_hash_cache[file_path] = (os.path.getmtime(file_path), file_hash)
        
        return file_hash
    except Exception as e:
        logger.error(f"Data content hash calculation failed: {str(e)}")
        # í•´ì‹œ ê³„ì‚°ì— ì‹¤íŒ¨í•˜ë©´ íŒŒì¼ ê¸°ë³¸ í•´ì‹œë¥¼ ì‚¬ìš©
        try:
            return calculate_file_hash(file_path)[:16]
        except Exception:
            return None

def build_cache_lookup_index():
    """ìºì‹œ ë””ë ‰í† ë¦¬ì˜ ì¸ë±ìŠ¤ë¥¼ ë¹Œë“œí•˜ì—¬ ë¹ ë¥¸ ê²€ìƒ‰ ê°€ëŠ¥"""
    global _cache_lookup_index
    
    try:
        _cache_lookup_index = {}
        cache_root = Path(CACHE_ROOT_DIR)
        
        if not cache_root.exists():
            return
        
        for file_dir in cache_root.iterdir():
            if not file_dir.is_dir() or file_dir.name == "default":
                continue
            
            predictions_dir = file_dir / 'predictions'
            if not predictions_dir.exists():
                continue
            
            prediction_files = list(predictions_dir.glob("prediction_start_*_meta.json"))
            
            for meta_file in prediction_files:
                try:
                    with open(meta_file, 'r', encoding='utf-8') as f:
                        meta_data = json.load(f)
                    
                    file_hash = meta_data.get('file_content_hash')
                    data_end_date = meta_data.get('data_end_date')
                    
                    if file_hash and data_end_date:
                        semimonthly = get_semimonthly_period(pd.to_datetime(data_end_date))
                        cache_key = f"{file_hash}_{semimonthly}"
                        
                        _cache_lookup_index[cache_key] = {
                            'meta_file': str(meta_file),
                            'predictions_dir': str(predictions_dir),
                            'data_end_date': data_end_date,
                            'semimonthly': semimonthly
                        }
                        
                except Exception:
                    continue
                    
        logger.info(f"ğŸ“Š Built cache lookup index with {len(_cache_lookup_index)} entries")
        
    except Exception as e:
        logger.error(f"Failed to build cache lookup index: {str(e)}")
        _cache_lookup_index = {}

def refresh_cache_index():
    """ìºì‹œ ì¸ë±ìŠ¤ë¥¼ ìƒˆë¡œê³ ì¹¨ (ìƒˆë¡œìš´ ìºì‹œ íŒŒì¼ì´ ìƒì„±ëœ í›„ í˜¸ì¶œ)"""
    global _cache_lookup_index
    logger.info("ğŸ”„ Refreshing cache lookup index...")
    build_cache_lookup_index()

def clear_cache_memory():
    """ë©”ëª¨ë¦¬ ìºì‹œë¥¼ í´ë¦¬ì–´ (ë©”ëª¨ë¦¬ ì ˆì•½ìš©)"""
    global _file_hash_cache, _cache_lookup_index
    _file_hash_cache.clear()
    _cache_lookup_index.clear()
    logger.info("ğŸ§¹ Cleared memory cache")

def check_data_extension(old_file_path, new_file_path):
    """
    ìƒˆ íŒŒì¼ì´ ê¸°ì¡´ íŒŒì¼ì˜ ìˆœì°¨ì  í™•ì¥(ê¸°ì¡´ ë°ì´í„° ì´í›„ì—ë§Œ ìƒˆ í–‰ ì¶”ê°€)ì¸ì§€ ì—„ê²©í•˜ê²Œ í™•ì¸
    
    âš ï¸ ì¤‘ìš”: ë‹¤ìŒ ê²½ìš°ë§Œ í™•ì¥ìœ¼ë¡œ ì¸ì •:
    1. ê¸°ì¡´ ë°ì´í„°ì™€ ì •í™•íˆ ë™ì¼í•œ ë¶€ë¶„ì´ ìˆìŒ
    2. ìƒˆ ë°ì´í„°ê°€ ê¸°ì¡´ ë°ì´í„°ì˜ ë§ˆì§€ë§‰ ë‚ ì§œ ì´í›„ì—ë§Œ ì¶”ê°€ë¨
    3. ê¸°ì¡´ ë°ì´í„°ì˜ ì‹œì‘/ì¤‘ê°„ ë‚ ì§œê°€ ë³€ê²½ë˜ì§€ ì•ŠìŒ
    
    Returns:
    --------
    dict: {
        'is_extension': bool,
        'new_rows_count': int,
        'base_hash': str,  # ê¸°ì¡´ ë°ì´í„° ë¶€ë¶„ì˜ í•´ì‹œ
        'old_start_date': str,
        'old_end_date': str,
        'new_start_date': str,
        'new_end_date': str,
        'validation_details': dict
    }
    """
    try:
        # íŒŒì¼ í˜•ì‹ì— ë§ê²Œ ë¡œë“œ (ğŸ”§ ìºì‹œ í™œìš©)
        def load_file_safely(filepath, is_new_file=False):
            file_ext = os.path.splitext(filepath.lower())[1]
            if file_ext == '.csv':
                return pd.read_csv(filepath)
            else:
                # Excel íŒŒì¼ì¸ ê²½ìš° load_data í•¨ìˆ˜ ì‚¬ìš© (ìºì‹œ í™œìš©)
                df = load_data(filepath, use_cache=True)
                # ì¸ë±ìŠ¤ê°€ Dateì¸ ê²½ìš° ì»¬ëŸ¼ìœ¼ë¡œ ë³µì›
                if df.index.name == 'Date':
                    df = df.reset_index()
                return df
        
        logger.info(f"ğŸ” [EXTENSION_CHECK] Loading data files for comparison...")
        old_df = load_file_safely(old_file_path, is_new_file=False)
        new_df = load_file_safely(new_file_path, is_new_file=True)
        
        # ë‚ ì§œ ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸
        if 'Date' not in old_df.columns or 'Date' not in new_df.columns:
            return {
                'is_extension': False, 
                'new_rows_count': 0,
                'validation_details': {'error': 'No Date column found'}
            }
        
        # ë‚ ì§œë¡œ ì •ë ¬
        old_df = old_df.sort_values('Date').reset_index(drop=True)
        new_df = new_df.sort_values('Date').reset_index(drop=True)
        
        # ë‚ ì§œë¥¼ datetimeìœ¼ë¡œ ë³€í™˜
        old_df['Date'] = pd.to_datetime(old_df['Date'])
        new_df['Date'] = pd.to_datetime(new_df['Date'])
        
        # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
        old_start_date = old_df['Date'].iloc[0]
        old_end_date = old_df['Date'].iloc[-1]
        new_start_date = new_df['Date'].iloc[0]
        new_end_date = new_df['Date'].iloc[-1]
        
        logger.info(f"ğŸ” [EXTENSION_CHECK] Old data: {old_start_date.strftime('%Y-%m-%d')} ~ {old_end_date.strftime('%Y-%m-%d')} ({len(old_df)} rows)")
        logger.info(f"ğŸ” [EXTENSION_CHECK] New data: {new_start_date.strftime('%Y-%m-%d')} ~ {new_end_date.strftime('%Y-%m-%d')} ({len(new_df)} rows)")
        
        # âœ… ê²€ì¦ 1: ìƒˆ íŒŒì¼ì´ ë” ê¸¸ì–´ì•¼ í•¨
        if len(new_df) <= len(old_df):
            logger.info(f"âŒ [EXTENSION_CHECK] New file is not longer ({len(new_df)} <= {len(old_df)})")
            return {
                'is_extension': False, 
                'new_rows_count': 0,
                'old_start_date': old_start_date.strftime('%Y-%m-%d'),
                'old_end_date': old_end_date.strftime('%Y-%m-%d'),
                'new_start_date': new_start_date.strftime('%Y-%m-%d'),
                'new_end_date': new_end_date.strftime('%Y-%m-%d'),
                'validation_details': {'reason': 'New file is not longer than old file'}
            }
        
        # âœ… ê²€ì¦ 2: ìƒˆ íŒŒì¼ì´ ë” ê¸¸ê±°ë‚˜ ìµœì†Œí•œ ê°™ì€ ê¸¸ì´ì—¬ì•¼ í•¨ (ê³¼ê±° ë°ì´í„° í—ˆìš©)
        # ê³¼ê±° ë°ì´í„°ê°€ í¬í•¨ëœ ê²½ìš°ë„ í—ˆìš©í•˜ë„ë¡ ë³€ê²½
        logger.info(f"ğŸ“… [EXTENSION_CHECK] Date ranges - Old: {old_start_date} ~ {old_end_date}, New: {new_start_date} ~ {new_end_date}")
        
        # âœ… ê²€ì¦ 3: ìƒˆ ë°ì´í„°ê°€ ê¸°ì¡´ ë°ì´í„°ë³´ë‹¤ ë” ë§ì€ ì •ë³´ë¥¼ í¬í•¨í•´ì•¼ í•¨ (ì™„í™”ëœ ì¡°ê±´)
        # ê³¼ê±° ë°ì´í„° í™•ì¥ ë˜ëŠ” ë¯¸ë˜ ë°ì´í„° í™•ì¥ ë‘˜ ë‹¤ í—ˆìš©
        has_more_data = (new_start_date < old_start_date) or (new_end_date > old_end_date) or (len(new_df) > len(old_df))
        if not has_more_data:
            logger.info(f"âŒ [EXTENSION_CHECK] New data doesn't provide additional information")
            return {
                'is_extension': False,
                'new_rows_count': 0,
                'old_start_date': old_start_date.strftime('%Y-%m-%d'),
                'old_end_date': old_end_date.strftime('%Y-%m-%d'),
                'new_start_date': new_start_date.strftime('%Y-%m-%d'),
                'new_end_date': new_end_date.strftime('%Y-%m-%d'),
                'validation_details': {'reason': 'New data does not provide additional information beyond existing data'}
            }
        
        # âœ… ê²€ì¦ 4: ê¸°ì¡´ ë°ì´í„°ì˜ ëª¨ë“  ë‚ ì§œê°€ ìƒˆ ë°ì´í„°ì— í¬í•¨ë˜ì–´ì•¼ í•¨
        old_dates = set(old_df['Date'].dt.strftime('%Y-%m-%d'))
        new_dates = set(new_df['Date'].dt.strftime('%Y-%m-%d'))
        
        missing_dates = old_dates - new_dates
        if missing_dates:
            logger.info(f"âŒ [EXTENSION_CHECK] Some old dates are missing in new data: {missing_dates}")
            return {
                'is_extension': False,
                'new_rows_count': 0,
                'old_start_date': old_start_date.strftime('%Y-%m-%d'),
                'old_end_date': old_end_date.strftime('%Y-%m-%d'),
                'new_start_date': new_start_date.strftime('%Y-%m-%d'),
                'new_end_date': new_end_date.strftime('%Y-%m-%d'),
                'validation_details': {'reason': f'Missing dates from old data: {list(missing_dates)}'}
            }
        
        # âœ… ê²€ì¦ 5: ì»¬ëŸ¼ì´ ë™ì¼í•´ì•¼ í•¨
        if list(old_df.columns) != list(new_df.columns):
            logger.info(f"âŒ [EXTENSION_CHECK] Column structure differs")
            return {
                'is_extension': False,
                'new_rows_count': 0,
                'old_start_date': old_start_date.strftime('%Y-%m-%d'),
                'old_end_date': old_end_date.strftime('%Y-%m-%d'),
                'new_start_date': new_start_date.strftime('%Y-%m-%d'),
                'new_end_date': new_end_date.strftime('%Y-%m-%d'),
                'validation_details': {'reason': 'Column structure differs'}
            }
        
        # âœ… ê²€ì¦ 6: ê¸°ì¡´ ë°ì´í„° ë¶€ë¶„ì´ ì •í™•íˆ ë™ì¼í•œì§€ í™•ì¸ (ê´€ëŒ€í•œ ì¡°ê±´ìœ¼ë¡œ ì™„í™”)
        logger.info(f"ğŸ” [EXTENSION_CHECK] Comparing overlapping data...")
        logger.info(f"  ğŸ“Š Checking {len(old_df)} existing dates...")
        
        # ğŸ”§ ê´€ëŒ€í•œ í™•ì¥ ê²€ì¦: ìƒ˜í”Œë§ ë°©ì‹ìœ¼ë¡œ ë³€ê²½ (ì „ì²´ ë°ì´í„°ê°€ ì•„ë‹Œ ì¼ë¶€ë§Œ ê²€ì‚¬)
        sample_size = min(50, len(old_df))  # ìµœëŒ€ 50ê°œ ë‚ ì§œë§Œ ê²€ì‚¬
        sample_indices = list(range(0, len(old_df), max(1, len(old_df) // sample_size)))
        
        logger.info(f"  ğŸ”¬ Sampling {len(sample_indices)} dates out of {len(old_df)} for validation...")
        
        # ê¸°ì¡´ ë°ì´í„°ì˜ ê° ë‚ ì§œì— í•´ë‹¹í•˜ëŠ” ìƒˆ ë°ì´í„° í–‰ ì°¾ê¸°
        data_matches = True
        mismatch_details = []
        checked_dates = 0
        mismatched_dates = 0
        allowed_mismatches = max(1, len(sample_indices) // 10)  # 10% ì •ë„ì˜ ë¯¸ìŠ¤ë§¤ì¹˜ëŠ” í—ˆìš©
        
        for idx in sample_indices:
            if idx >= len(old_df):
                continue
                
            old_row = old_df.iloc[idx]
            old_date = old_row['Date']
            old_date_str = old_date.strftime('%Y-%m-%d')
            checked_dates += 1
            
            # ìƒˆ ë°ì´í„°ì—ì„œ í•´ë‹¹ ë‚ ì§œ ì°¾ê¸°
            new_matching_rows = new_df[new_df['Date'] == old_date]
            
            if len(new_matching_rows) == 0:
                data_matches = False
                mismatch_details.append(f"Date {old_date_str} missing in new data")
                break
            elif len(new_matching_rows) > 1:
                data_matches = False
                mismatch_details.append(f"Duplicate date {old_date_str} in new data")
                break
            
            new_row = new_matching_rows.iloc[0]
            
            # ìˆ˜ì¹˜ ì»¬ëŸ¼ ë¹„êµ (Date ì œì™¸) - ì™„í™”ëœ ì¡°ê±´
            numeric_cols = old_df.select_dtypes(include=[np.number]).columns
            for col in numeric_cols:
                old_val = old_row[col]
                new_val = new_row[col]
                
                # NaN ê°’ ì²˜ë¦¬
                if pd.isna(old_val) and pd.isna(new_val):
                    continue
                elif pd.isna(old_val) or pd.isna(new_val):
                    data_matches = False
                    mismatch_details.append(f"NaN mismatch on {old_date_str}, column {col}: {old_val} != {new_val}")
                    break
                
                # ìˆ˜ì¹˜ ë¹„êµ - ìƒëŒ€ì ìœ¼ë¡œ ê´€ëŒ€í•œ ì¡°ê±´ (0.01% ì˜¤ì°¨ í—ˆìš©)
                if not np.allclose([old_val], [new_val], rtol=1e-4, atol=1e-6, equal_nan=True):
                    # ì¶”ê°€ ê²€ì¦: ì •ìˆ˜ê°’ì´ ì†Œìˆ˜ì ìœ¼ë¡œ ë³€í™˜ëœ ê²½ìš° í—ˆìš© (ì˜ˆ: 100 vs 100.0)
                    try:
                        if abs(float(old_val) - float(new_val)) < 1e-6:
                            continue
                    except:
                        pass
                    
                    mismatch_details.append(f"Value mismatch on {old_date_str}, column {col}: {old_val} != {new_val}")
                    mismatched_dates += 1
                    # ğŸ”§ ê´€ëŒ€í•œ ì¡°ê±´: ì¦‰ì‹œ ì¤‘ë‹¨í•˜ì§€ ì•Šê³  í—ˆìš© í•œë„ê¹Œì§€ ê³„ì† ê²€ì‚¬
                    if mismatched_dates > allowed_mismatches:
                        data_matches = False
                        break
            
            if not data_matches:
                break
            
            # ë¬¸ìì—´ ì»¬ëŸ¼ ë¹„êµ (Date ì œì™¸) - ì™„í™”ëœ ì¡°ê±´
            str_cols = old_df.select_dtypes(include=['object']).columns
            str_cols = [col for col in str_cols if col != 'Date']
            for col in str_cols:
                old_str = str(old_row[col]).strip() if not pd.isna(old_row[col]) else ''
                new_str = str(new_row[col]).strip() if not pd.isna(new_row[col]) else ''
                
                if old_str != new_str:
                    mismatch_details.append(f"String mismatch on {old_date_str}, column {col}: '{old_str}' != '{new_str}'")
                    mismatched_dates += 1
                    # ğŸ”§ ê´€ëŒ€í•œ ì¡°ê±´: í—ˆìš© í•œë„ê¹Œì§€ ê³„ì† ê²€ì‚¬
                    if mismatched_dates > allowed_mismatches:
                        data_matches = False
                        break
            
            if not data_matches:
                break
        
        # ğŸ”§ ê´€ëŒ€í•œ ê²€ì¦ ê²°ê³¼ í‰ê°€
        logger.info(f"  âœ… Checked {checked_dates} sample dates, {mismatched_dates} mismatches found (allowed: {allowed_mismatches})")
        if mismatch_details:
            logger.info(f"  âš ï¸ Sample mismatches: {mismatch_details[:3]}...")
        
        if not data_matches:
            logger.info(f"âŒ [EXTENSION_CHECK] Too many data mismatches ({mismatched_dates} > {allowed_mismatches})")
            return {
                'is_extension': False,
                'new_rows_count': 0,
                'old_start_date': old_start_date.strftime('%Y-%m-%d'),
                'old_end_date': old_end_date.strftime('%Y-%m-%d'),
                'new_start_date': new_start_date.strftime('%Y-%m-%d'),
                'new_end_date': new_end_date.strftime('%Y-%m-%d'),
                'validation_details': {
                    'reason': f'Too many data mismatches: {mismatched_dates} > {allowed_mismatches}',
                    'mismatches_found': mismatched_dates,
                    'allowed_mismatches': allowed_mismatches,
                    'sample_details': mismatch_details[:5]
                }
            }
        elif mismatched_dates > 0:
            logger.info(f"âš ï¸ [EXTENSION_CHECK] Minor mismatches found but within tolerance ({mismatched_dates} <= {allowed_mismatches})")
        
        # âœ… ê²€ì¦ 7: ìƒˆë¡œ ì¶”ê°€ëœ ë°ì´í„° ë¶„ì„ (ê³¼ê±°/ë¯¸ë˜ ë°ì´í„° ëª¨ë‘ í—ˆìš©)
        new_only_dates = new_dates - old_dates
        
        # í™•ì¥ ìœ í˜• ë¶„ì„
        extension_type = []
        if new_start_date < old_start_date:
            past_dates = len([d for d in new_only_dates if pd.to_datetime(d) < old_start_date])
            extension_type.append(f"ê³¼ê±° ë°ì´í„° {past_dates}ê°œ ì¶”ê°€")
        if new_end_date > old_end_date:
            future_dates = len([d for d in new_only_dates if pd.to_datetime(d) > old_end_date])
            extension_type.append(f"ë¯¸ë˜ ë°ì´í„° {future_dates}ê°œ ì¶”ê°€")
        
        extension_desc = " + ".join(extension_type) if extension_type else "ë°ì´í„° ë³´ì™„"
        
        # âœ… ëª¨ë“  ê²€ì¦ í†µê³¼: ë°ì´í„° í™•ì¥ìœ¼ë¡œ ì¸ì • (ê³¼ê±°/ë¯¸ë˜ ëª¨ë‘ í—ˆìš©)
        new_rows_count = len(new_only_dates)
        base_hash = get_data_content_hash(old_file_path)
        
        logger.info(f"âœ… [EXTENSION_CHECK] Valid data extension: {extension_desc} (+{new_rows_count} new dates)")
        
        return {
            'is_extension': True,
            'new_rows_count': new_rows_count,
            'base_hash': base_hash,
            'old_start_date': old_start_date.strftime('%Y-%m-%d'),
            'old_end_date': old_end_date.strftime('%Y-%m-%d'),
            'new_start_date': new_start_date.strftime('%Y-%m-%d'),
            'new_end_date': new_end_date.strftime('%Y-%m-%d'),
            'validation_details': {
                'reason': f'Valid data extension: {extension_desc}',
                'new_dates_added': sorted(list(new_only_dates)),
                'extension_type': extension_type
            }
        }
        
    except Exception as e:
        logger.error(f"Data extension check failed: {str(e)}")
        return {
            'is_extension': False, 
            'new_rows_count': 0,
            'old_start_date': None,
            'old_end_date': None,
            'new_start_date': None,
            'new_end_date': None,
            'validation_details': {'error': str(e)}
        }

def find_existing_cache_range(file_path):
    """
    ê¸°ì¡´ íŒŒì¼ì˜ ìºì‹œì—ì„œ ì‚¬ìš©ëœ ë°ì´í„° ë²”ìœ„ ì •ë³´ë¥¼ ì°¾ëŠ” í•¨ìˆ˜
    
    Returns:
    --------
    dict or None: {'start_date': 'YYYY-MM-DD', 'cutoff_date': 'YYYY-MM-DD'} ë˜ëŠ” None
    """
    try:
        # íŒŒì¼ì— ëŒ€ì‘í•˜ëŠ” ìºì‹œ ë””ë ‰í† ë¦¬ ì°¾ê¸°
        cache_dirs = get_file_cache_dirs(file_path)
        predictions_dir = Path(cache_dirs['predictions'])
        
        if not predictions_dir.exists():
            return None
            
        # ìµœê·¼ ë©”íƒ€ íŒŒì¼ì—ì„œ ë°ì´í„° ë²”ìœ„ ì •ë³´ í™•ì¸
        meta_files = list(predictions_dir.glob("*_meta.json"))
        if not meta_files:
            return None
            
        # ê°€ì¥ ìµœê·¼ ë©”íƒ€ íŒŒì¼ ì„ íƒ
        latest_meta = max(meta_files, key=lambda f: f.stat().st_mtime)
        
        with open(latest_meta, 'r', encoding='utf-8') as f:
            meta_data = json.load(f)
            
        # ë°ì´í„° ë²”ìœ„ ì •ë³´ ì¶”ì¶œ
        model_config = meta_data.get('model_config', {})
        lstm_config = model_config.get('lstm', {})
        
        start_date = lstm_config.get('data_start_date')
        cutoff_date = lstm_config.get('data_cutoff_date') or meta_data.get('data_end_date')
        
        if start_date and cutoff_date:
            return {
                'start_date': start_date,
                'cutoff_date': cutoff_date,
                'meta_file': str(latest_meta)
            }
            
        return None
        
    except Exception as e:
        logger.warning(f"Failed to find cache range for {file_path}: {str(e)}")
        return None

def find_compatible_cache_file(new_file_path, intended_data_range=None, cached_df=None):
    """
    ìƒˆ íŒŒì¼ê³¼ í˜¸í™˜ë˜ëŠ” ê¸°ì¡´ ìºì‹œë¥¼ ì°¾ëŠ” í•¨ìˆ˜ (ë°ì´í„° ë²”ìœ„ ê³ ë ¤)
    
    ğŸ”§ í•µì‹¬ ê°œì„ :
    - íŒŒì¼ ë‚´ìš© + ì‚¬ìš© ë°ì´í„° ë²”ìœ„ë¥¼ ëª¨ë‘ ê³ ë ¤
    - ê°™ì€ íŒŒì¼ì´ë¼ë„ ë‹¤ë¥¸ ë°ì´í„° ë²”ìœ„ë©´ ìƒˆ ì˜ˆì¸¡ìœ¼ë¡œ ì¸ì‹
    - ì‚¬ìš©ì ì˜ë„ë¥¼ ë°˜ì˜í•œ ìŠ¤ë§ˆíŠ¸ ìºì‹œ ë§¤ì¹­
    - ì¤‘ë³µ ë¡œë”© ë°©ì§€ë¥¼ ìœ„í•œ ìºì‹œëœ DataFrame ì¬ì‚¬ìš©
    
    Parameters:
    -----------
    new_file_path : str
        ìƒˆ íŒŒì¼ ê²½ë¡œ
    intended_data_range : dict, optional
        ì‚¬ìš©ìê°€ ì˜ë„í•œ ë°ì´í„° ë²”ìœ„ {'start_date': 'YYYY-MM-DD', 'cutoff_date': 'YYYY-MM-DD'}
    cached_df : DataFrame, optional
        ì´ë¯¸ ë¡œë”©ëœ DataFrame (ì¤‘ë³µ ë¡œë”© ë°©ì§€)
    
    Returns:
    --------
    dict: {
        'found': bool,
        'cache_type': str,  # 'exact_with_range', 'extension', 'partial', 'range_mismatch'
        'cache_files': list,
        'compatibility_info': dict
    }
    """
    try:
        # ğŸ”§ ìºì‹œëœ DataFrameì´ ìˆìœ¼ë©´ ì¬ì‚¬ìš©, ì—†ìœ¼ë©´ ìƒˆë¡œ ë¡œë”©
        if cached_df is not None:
            logger.info(f"ğŸ”„ [CACHE_OPTIMIZATION] Using cached DataFrame (avoiding duplicate load)")
            new_df = cached_df.copy()
        else:
            logger.info(f"ğŸ“ [CACHE_COMPATIBILITY] Loading data for cache check...")
            # ìƒˆ íŒŒì¼ì˜ ë°ì´í„° ë¶„ì„ (íŒŒì¼ í˜•ì‹ì— ë§ê²Œ)
            file_ext = os.path.splitext(new_file_path.lower())[1]
            if file_ext == '.csv':
                new_df = pd.read_csv(new_file_path)
            else:
                # Excel íŒŒì¼ì¸ ê²½ìš° load_data í•¨ìˆ˜ ì‚¬ìš©
                new_df = load_data(new_file_path)
                # ì¸ë±ìŠ¤ê°€ Dateì¸ ê²½ìš° ì»¬ëŸ¼ìœ¼ë¡œ ë³µì›
                if new_df.index.name == 'Date':
                    new_df = new_df.reset_index()
        
        if 'Date' not in new_df.columns:
            return {'found': False, 'cache_type': None, 'reason': 'No Date column'}
            
        new_df['Date'] = pd.to_datetime(new_df['Date'])
        new_start_date = new_df['Date'].min()
        new_end_date = new_df['Date'].max()
        new_hash = get_data_content_hash(new_file_path)
        
        logger.info(f"ğŸ” [ENHANCED_CACHE] Analyzing new file:")
        logger.info(f"  ğŸ“… Full date range: {new_start_date.strftime('%Y-%m-%d')} ~ {new_end_date.strftime('%Y-%m-%d')}")
        logger.info(f"  ğŸ“Š Records: {len(new_df)}")
        logger.info(f"  ğŸ”‘ Hash: {new_hash[:12] if new_hash else 'None'}...")
        
        # ì‚¬ìš©ì ì˜ë„ ë°ì´í„° ë²”ìœ„ í™•ì¸
        if intended_data_range:
            intended_start = pd.to_datetime(intended_data_range.get('start_date', new_start_date))
            intended_cutoff = pd.to_datetime(intended_data_range.get('cutoff_date', new_end_date))
            logger.info(f"  ğŸ¯ Intended range: {intended_start.strftime('%Y-%m-%d')} ~ {intended_cutoff.strftime('%Y-%m-%d')}")
        else:
            intended_start = new_start_date
            intended_cutoff = new_end_date
            logger.info(f"  ğŸ¯ Using full range (no specific intention provided)")
        
        compatible_caches = []
        
        # 1. uploads í´ë”ì˜ íŒŒì¼ë“¤ ê²€ì‚¬ (ë°ì´í„° ë²”ìœ„ ê³ ë ¤)
        upload_dir = Path(UPLOAD_FOLDER)
        existing_files = list(upload_dir.glob('*.csv')) + list(upload_dir.glob('*.xlsx')) + list(upload_dir.glob('*.xls'))
        
        logger.info(f"ğŸ” [ENHANCED_CACHE] Checking {len(existing_files)} upload files with range consideration...")
        
        for existing_file in existing_files:
            if existing_file.name == os.path.basename(new_file_path):
                continue
                
            try:
                # íŒŒì¼ í•´ì‹œ í™•ì¸
                existing_hash = get_data_content_hash(str(existing_file))
                if existing_hash == new_hash:
                    logger.info(f"ğŸ“„ [ENHANCED_CACHE] Same file content found: {existing_file.name}")
                    
                    # ğŸ”‘ ê°™ì€ íŒŒì¼ì´ì§€ë§Œ ë°ì´í„° ë²”ìœ„ ì˜ë„ í™•ì¸
                    # ê¸°ì¡´ ìºì‹œì˜ ë°ì´í„° ë²”ìœ„ ì •ë³´ë¥¼ ì°¾ì•„ì•¼ í•¨
                    existing_cache_range = find_existing_cache_range(str(existing_file))
                    
                    if existing_cache_range and intended_data_range:
                        cache_start = existing_cache_range.get('start_date')
                        cache_cutoff = existing_cache_range.get('cutoff_date') 
                        
                        if cache_start and cache_cutoff:
                            cache_start = pd.to_datetime(cache_start)
                            cache_cutoff = pd.to_datetime(cache_cutoff)
                            
                            # ë°ì´í„° ë²”ìœ„ ë¹„êµ
                            range_match = (
                                abs((intended_start - cache_start).days) <= 30 and 
                                abs((intended_cutoff - cache_cutoff).days) <= 30
                            )
                            
                            if range_match:
                                logger.info(f"âœ… [ENHANCED_CACHE] Exact match with same intended range!")
                                return {
                                    'found': True,
                                    'cache_type': 'exact_with_range',
                                    'cache_files': [str(existing_file)],
                                    'compatibility_info': {
                                        'match_type': 'file_hash_and_range',
                                        'cache_range': existing_cache_range,
                                        'intended_range': intended_data_range
                                    }
                                }
                            else:
                                logger.info(f"âš ï¸ [ENHANCED_CACHE] Same file but different intended range:")
                                logger.info(f"    ğŸ’¾ Cached range: {cache_start.strftime('%Y-%m-%d')} ~ {cache_cutoff.strftime('%Y-%m-%d')}")
                                logger.info(f"    ğŸ¯ Intended range: {intended_start.strftime('%Y-%m-%d')} ~ {intended_cutoff.strftime('%Y-%m-%d')}")
                                logger.info(f"    ğŸ”„ Will create new cache for different range")
                                # ê°™ì€ íŒŒì¼ì´ì§€ë§Œ ë‹¤ë¥¸ ë²”ìœ„ ì˜ë„ â†’ ìƒˆ ì˜ˆì¸¡ í•„ìš”
                                continue
                    
                    # ë²”ìœ„ ì •ë³´ê°€ ì—†ìœ¼ë©´ ê¸°ì¡´ ë¡œì§ ì ìš©
                    logger.info(f"âœ… [ENHANCED_CACHE] Exact file match (no range info): {existing_file.name}")
                    return {
                        'found': True,
                        'cache_type': 'exact',
                        'cache_files': [str(existing_file)],
                        'compatibility_info': {'match_type': 'file_hash_only'}
                    }
                
                # í™•ì¥ íŒŒì¼ í™•ì¸ (ê¸°ì¡´ ë¡œì§ ìœ ì§€) - ë””ë²„ê¹… ê°•í™”
                logger.info(f"ğŸ” [EXTENSION_CHECK] Testing extension: {existing_file.name} â†’ {os.path.basename(new_file_path)}")
                extension_info = check_data_extension(str(existing_file), new_file_path)
                
                logger.info(f"ğŸ“Š [EXTENSION_RESULT] is_extension: {extension_info['is_extension']}")
                if extension_info.get('validation_details'):
                    logger.info(f"ğŸ“Š [EXTENSION_RESULT] reason: {extension_info['validation_details'].get('reason', 'N/A')}")
                
                if extension_info['is_extension']:
                    logger.info(f"ğŸ“ˆ [ENHANCED_CACHE] Found extension base: {existing_file.name} (+{extension_info.get('new_rows_count', 0)} rows)")
                    return {
                        'found': True,
                        'cache_type': 'extension', 
                        'cache_files': [str(existing_file)],
                        'compatibility_info': extension_info
                    }
                else:
                    logger.info(f"âŒ [EXTENSION_CHECK] Not an extension: {extension_info['validation_details'].get('reason', 'Unknown reason')}")
                    
            except Exception as e:
                logger.warning(f"Error checking upload file {existing_file}: {str(e)}")
                continue
        
        # 2. ğŸ”§ ìºì‹œ ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  ì˜ˆì¸¡ ê²°ê³¼ ê²€ì‚¬ (ì‹ ê·œ)
        cache_root = Path(CACHE_ROOT_DIR)
        if not cache_root.exists():
            logger.info("âŒ [ENHANCED_CACHE] No cache directory found")
            return {'found': False, 'cache_type': None}
            
        logger.info(f"ğŸ” [ENHANCED_CACHE] Scanning cache directories...")
        
        for file_cache_dir in cache_root.iterdir():
            if not file_cache_dir.is_dir():
                continue
                
            predictions_dir = file_cache_dir / 'predictions'
            if not predictions_dir.exists():
                continue
                
            # predictions_index.csv íŒŒì¼ì—ì„œ ìºì‹œëœ ì˜ˆì¸¡ë“¤ì˜ ë‚ ì§œ ë²”ìœ„ í™•ì¸
            index_file = predictions_dir / 'predictions_index.csv'
            if not index_file.exists():
                continue
                
            try:
                cache_index = pd.read_csv(index_file)
                if 'data_end_date' not in cache_index.columns:
                    continue
                    
                cache_index['data_end_date'] = pd.to_datetime(cache_index['data_end_date'])
                cache_start = cache_index['data_end_date'].min()
                cache_end = cache_index['data_end_date'].max()
                
                logger.info(f"  ğŸ“ {file_cache_dir.name}: {cache_start.strftime('%Y-%m-%d')} ~ {cache_end.strftime('%Y-%m-%d')} ({len(cache_index)} predictions)")
                
                # ë‚ ì§œ ë²”ìœ„ ì¤‘ë³µ í™•ì¸
                overlap_start = max(new_start_date, cache_start)
                overlap_end = min(new_end_date, cache_end)
                
                if overlap_start <= overlap_end:
                    overlap_days = (overlap_end - overlap_start).days + 1
                    new_total_days = (new_end_date - new_start_date).days + 1
                    coverage_ratio = overlap_days / new_total_days
                    
                    logger.info(f"    ğŸ“Š Overlap: {overlap_days} days ({coverage_ratio:.1%} coverage)")
                    
                    if coverage_ratio >= 0.7:  # 70% ì´ìƒ ê²¹ì¹˜ë©´ í˜¸í™˜ ê°€ëŠ¥
                        compatible_caches.append({
                            'cache_dir': str(file_cache_dir),
                            'predictions_dir': str(predictions_dir),
                            'coverage_ratio': coverage_ratio,
                            'overlap_days': overlap_days,
                            'cache_range': (cache_start, cache_end),
                            'prediction_count': len(cache_index)
                        })
                        
            except Exception as e:
                logger.warning(f"Error analyzing cache {file_cache_dir.name}: {str(e)}")
                continue
        
        # 3. í˜¸í™˜ ê°€ëŠ¥í•œ ìºì‹œ ê²°ê³¼ ì²˜ë¦¬
        if compatible_caches:
            # ì»¤ë²„ë¦¬ì§€ ë¹„ìœ¨ë¡œ ì •ë ¬ (ë†’ì€ ìˆœ)
            compatible_caches.sort(key=lambda x: x['coverage_ratio'], reverse=True)
            best_cache = compatible_caches[0]
            
            logger.info(f"ğŸ¯ [ENHANCED_CACHE] Found {len(compatible_caches)} compatible cache(s)")
            logger.info(f"  ğŸ¥‡ Best: {Path(best_cache['cache_dir']).name} ({best_cache['coverage_ratio']:.1%} coverage)")
            
            if best_cache['coverage_ratio'] >= 0.95:  # 95% ì´ìƒì´ë©´ ê±°ì˜ ì™„ì „
                cache_type = 'near_complete'
            elif len(compatible_caches) > 1:  # ì—¬ëŸ¬ ìºì‹œ ì¡°í•© ê°€ëŠ¥
                cache_type = 'multi_cache' 
            else:
                cache_type = 'partial'
                
            return {
                'found': True,
                'cache_type': cache_type,
                'cache_files': [cache['predictions_dir'] for cache in compatible_caches],
                'compatibility_info': {
                    'best_coverage': best_cache['coverage_ratio'],
                    'total_compatible_caches': len(compatible_caches),
                    'date_ranges': [(c['cache_range'][0].strftime('%Y-%m-%d'), 
                                   c['cache_range'][1].strftime('%Y-%m-%d')) for c in compatible_caches]
                }
            }
        
        logger.info("âŒ [ENHANCED_CACHE] No compatible cache found")
        return {'found': False, 'cache_type': None}
        
    except Exception as e:
        logger.error(f"Enhanced cache compatibility check failed: {str(e)}")
        return {'found': False, 'cache_type': None, 'error': str(e)}

def create_proper_column_names(file_path, sheet_name):
    """í—¤ë” 3í–‰ì„ ì½ì–´ì„œ ì ì ˆí•œ ì—´ ì´ë¦„ ìƒì„±"""
    # í—¤ë” 3í–‰ì„ ì½ì–´ì˜´
    header_rows = pd.read_excel(file_path, sheet_name=sheet_name, header=None, nrows=3)
    
    # ê° ì—´ë³„ë¡œ ì ì ˆí•œ ì´ë¦„ ìƒì„±
    column_names = []
    prev_main_category = None  # ì´ì „ ë©”ì¸ ì¹´í…Œê³ ë¦¬ ì €ì¥
    
    for col_idx in range(header_rows.shape[1]):
        values = [str(header_rows.iloc[i, col_idx]).strip() 
                 for i in range(3) 
                 if pd.notna(header_rows.iloc[i, col_idx]) and str(header_rows.iloc[i, col_idx]).strip() != 'nan']
        
        # ì²« ë²ˆì§¸ í–‰ì˜ ê°’ì´ ìˆìœ¼ë©´ ë©”ì¸ ì¹´í…Œê³ ë¦¬ë¡œ ì €ì¥
        if pd.notna(header_rows.iloc[0, col_idx]) and str(header_rows.iloc[0, col_idx]).strip() != 'nan':
            prev_main_category = str(header_rows.iloc[0, col_idx]).strip()
        
        # ì—´ ì´ë¦„ ìƒì„± ë¡œì§
        if 'Date' in values:
            column_names.append('Date')
        else:
            # ê°’ì´ í•˜ë‚˜ë„ ì—†ëŠ” ê²½ìš°
            if not values:
                column_names.append(f'Unnamed_{col_idx}')
                continue
                
            # ë©”ì¸ ì¹´í…Œê³ ë¦¬ê°€ ìˆê³ , í˜„ì¬ ê°’ë“¤ì— í¬í•¨ë˜ì§€ ì•Šì€ ê²½ìš° ì¶”ê°€
            if prev_main_category and prev_main_category not in values:
                values.insert(0, prev_main_category)
            
            # íŠ¹ìˆ˜ ì¼€ì´ìŠ¤ ì²˜ë¦¬ (ì˜ˆ: WS, Naphtha ë“±)
            if 'WS' in values and 'SG-Korea' in values:
                column_names.append('WS_SG-Korea')
            elif 'Naphtha' in values and 'Platts' in values:
                column_names.append('Naphtha_Platts_' + '_'.join([v for v in values if v not in ['Naphtha', 'Platts']]))
            else:
                column_names.append('_'.join(values))
    
    return column_names

def remove_high_missing_columns(data, threshold=70):
    """ë†’ì€ ê²°ì¸¡ì¹˜ ë¹„ìœ¨ì„ ê°€ì§„ ì—´ ì œê±°"""
    missing_ratio = (data.isnull().sum() / len(data)) * 100
    high_missing_cols = missing_ratio[missing_ratio >= threshold].index
    
    print(f"\n=== {threshold}% ì´ìƒ ê²°ì¸¡ì¹˜ê°€ ìˆì–´ ì œê±°ë  ì—´ ëª©ë¡ ===")
    for col in high_missing_cols:
        print(f"- {col}: {missing_ratio[col]:.1f}%")
    
    cleaned_data = data.drop(columns=high_missing_cols)
    print(f"\nì›ë³¸ ë°ì´í„° í˜•íƒœ: {data.shape}")
    print(f"ì •ì œëœ ë°ì´í„° í˜•íƒœ: {cleaned_data.shape}")
    
    return cleaned_data

def clean_text_values_advanced(data):
    """ê³ ê¸‰ í…ìŠ¤íŠ¸ ê°’ ì •ì œ (ì‰¼í‘œ ì†Œìˆ˜ì  ì²˜ë¦¬ í¬í•¨)"""
    cleaned_data = data.copy()
    
    def fix_comma_decimal(value_str):
        """ì‰¼í‘œë¡œ ëœ ì†Œìˆ˜ì ì„ ì ìœ¼ë¡œ ë³€ê²½í•˜ëŠ” í•¨ìˆ˜"""
        if not isinstance(value_str, str) or ',' not in value_str:
            return value_str
            
        import re
        
        # íŒ¨í„´ 1: ë‹¨ìˆœ ì†Œìˆ˜ì  ì‰¼í‘œ (ì˜ˆ: "123,45")
        if re.match(r'^-?\d+,\d{1,3}$', value_str):
            return value_str.replace(',', '.')
            
        # íŒ¨í„´ 2: ì²œ ë‹¨ìœ„ êµ¬ë¶„ì + ì†Œìˆ˜ì  ì‰¼í‘œ (ì˜ˆ: "1.234,56")
        if re.match(r'^-?\d{1,3}(\.\d{3})*,\d{1,3}$', value_str):
            # ë§ˆì§€ë§‰ ì‰¼í‘œë§Œ ì†Œìˆ˜ì ìœ¼ë¡œ ë³€ê²½
            last_comma_pos = value_str.rfind(',')
            return value_str[:last_comma_pos] + '.' + value_str[last_comma_pos+1:]
            
        # íŒ¨í„´ 3: ì‰¼í‘œë§Œ ì²œ ë‹¨ìœ„ êµ¬ë¶„ìë¡œ ì‚¬ìš© (ì˜ˆ: "1,234,567")
        if re.match(r'^-?\d{1,3}(,\d{3})+$', value_str):
            return value_str.replace(',', '')
            
        return value_str
    
    def process_value(x):
        if pd.isna(x):  # ì´ë¯¸ NaNì¸ ê²½ìš°
            return x
        
        # ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ì²˜ë¦¬
        x_str = str(x).strip()
        
        # 1. ë¨¼ì € ì‰¼í‘œ ì†Œìˆ˜ì  ë¬¸ì œ í•´ê²°
        x_str = fix_comma_decimal(x_str)
        
        # 2. íœ´ì¼/ë¯¸ë°œí‘œ ë°ì´í„° ì²˜ë¦¬
        if x_str.upper() in ['NOP', 'NO PUBLICATION', 'NO PUB']:
            return np.nan
            
        # 3. TBA (To Be Announced) ê°’ ì²˜ë¦¬ - íŠ¹ë³„ ë§ˆí‚¹í•˜ì—¬ ë‚˜ì¤‘ì— ì „ë‚ ê°’ìœ¼ë¡œ ëŒ€ì²´
        if x_str.upper() in ['TBA', 'TO BE ANNOUNCED']:
            return 'TBA_REPLACE'
            
        # 4. '*' í¬í•¨ëœ ê³„ì‚°ì‹ ì²˜ë¦¬
        if '*' in x_str:
            try:
                # ê³„ì‚°ì‹ ì‹¤í–‰
                return float(eval(x_str.replace(' ', '')))
            except:
                return x
        
        # 5. ìˆ«ìë¡œ ë³€í™˜ ì‹œë„
        try:
            return float(x_str)
        except:
            return x

    # ì‰¼í‘œ ì²˜ë¦¬ í†µê³„ë¥¼ ìœ„í•œ ë³€ìˆ˜
    comma_fixes = 0
    
    # ê° ì—´ì— ëŒ€í•´ ì²˜ë¦¬
    for column in cleaned_data.columns:
        if column != 'Date':  # Date ì—´ ì œì™¸
            # ì²˜ë¦¬ ì „ ì‰¼í‘œê°€ ìˆëŠ” ê°’ë“¤ í™•ì¸
            before_comma_count = cleaned_data[column].astype(str).str.contains(',', na=False).sum()
            
            cleaned_data[column] = cleaned_data[column].apply(process_value)
            
            # ì²˜ë¦¬ í›„ ì‰¼í‘œê°€ ìˆëŠ” ê°’ë“¤ í™•ì¸
            after_comma_count = cleaned_data[column].astype(str).str.contains(',', na=False).sum()
            
            if before_comma_count > after_comma_count:
                fixed_count = before_comma_count - after_comma_count
                comma_fixes += fixed_count
                print(f"ì—´ '{column}': {fixed_count}ê°œì˜ ì‰¼í‘œ ì†Œìˆ˜ì ì„ ìˆ˜ì •í–ˆìŠµë‹ˆë‹¤.")
    
    if comma_fixes > 0:
        print(f"\nì´ {comma_fixes}ê°œì˜ ì‰¼í‘œ ì†Œìˆ˜ì ì„ ì ìœ¼ë¡œ ìˆ˜ì •í–ˆìŠµë‹ˆë‹¤.")
    
    # MOPJ ë³€ìˆ˜ ì²˜ë¦¬ (ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” í–‰ ì œê±°)
    mopj_columns = [col for col in cleaned_data.columns if 'MOPJ' in col or 'Naphtha_Platts_MOPJ' in col]
    if mopj_columns:
        mopj_col = mopj_columns[0]  # ì²« ë²ˆì§¸ MOPJ ê´€ë ¨ ì—´ ì‚¬ìš©
        print(f"\n=== {mopj_col} ë³€ìˆ˜ ì²˜ë¦¬ ì „ ë°ì´í„° í¬ê¸° ===")
        print(f"í–‰ ìˆ˜: {len(cleaned_data)}")
        
        # ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” í–‰ ì œê±°
        cleaned_data = cleaned_data.dropna(subset=[mopj_col])
        
        # ë¬¸ìì—´ ê°’ì´ ìˆëŠ” í–‰ ì œê±°
        try:
            pd.to_numeric(cleaned_data[mopj_col], errors='raise')
        except:
            # ìˆ«ìë¡œ ë³€í™˜í•  ìˆ˜ ì—†ëŠ” í–‰ ì°¾ê¸°
            numeric_mask = pd.to_numeric(cleaned_data[mopj_col], errors='coerce').notna()
            cleaned_data = cleaned_data[numeric_mask]
        
        print(f"\n=== {mopj_col} ë³€ìˆ˜ ì²˜ë¦¬ í›„ ë°ì´í„° í¬ê¸° ===")
        print(f"í–‰ ìˆ˜: {len(cleaned_data)}")
    
    # ğŸ”§ TBA ê°’ì„ ì „ë‚  ê°’ìœ¼ë¡œ ëŒ€ì²´
    tba_replacements = 0
    if 'Date' in cleaned_data.columns:
        # ë‚ ì§œìˆœìœ¼ë¡œ ì •ë ¬ (ì¤‘ìš”: ì „ë‚  ê°’ ì°¸ì¡°ë¥¼ ìœ„í•´)
        cleaned_data = cleaned_data.sort_values('Date').reset_index(drop=True)
        
        for column in cleaned_data.columns:
            if column != 'Date':  # Date ì—´ ì œì™¸
                # TBA_REPLACE ë§ˆí‚¹ëœ ê°’ë“¤ ì°¾ê¸°
                tba_mask = cleaned_data[column] == 'TBA_REPLACE'
                tba_indices = cleaned_data[tba_mask].index.tolist()
                
                if tba_indices:
                    print(f"\n[TBA ì²˜ë¦¬] ì—´ '{column}'ì—ì„œ {len(tba_indices)}ê°œì˜ TBA ê°’ ë°œê²¬")
                    
                    for idx in tba_indices:
                        # ğŸ”§ ê°œì„ : ê°€ì¥ ìµœê·¼ì˜ ìœ íš¨í•œ ê°’ ì°¾ê¸° (ì—°ì† TBA ì²˜ë¦¬)
                        replacement_value = None
                        source_description = ""
                        
                        # ì´ì „ í–‰ë“¤ì„ ê±°ìŠ¬ëŸ¬ ì˜¬ë¼ê°€ë©´ì„œ ìœ íš¨í•œ ê°’ ì°¾ê¸°
                        for prev_idx in range(idx-1, -1, -1):
                            candidate_value = cleaned_data.loc[prev_idx, column]
                            try:
                                if pd.notna(candidate_value) and candidate_value != 'TBA_REPLACE':
                                    replacement_value = float(candidate_value)
                                    days_back = idx - prev_idx
                                    if days_back == 1:
                                        source_description = "ì „ë‚  ê°’"
                                    else:
                                        source_description = f"{days_back}ì¼ ì „ ê°’"
                                    break
                            except (ValueError, TypeError):
                                continue
                        
                        # ê°’ ëŒ€ì²´ ìˆ˜í–‰
                        if replacement_value is not None:
                            cleaned_data.loc[idx, column] = replacement_value
                            tba_replacements += 1
                            print(f"  - í–‰ {idx+1}: TBA â†’ {replacement_value} ({source_description})")
                        else:
                            # ìœ íš¨í•œ ì´ì „ ê°’ì„ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš°
                            cleaned_data.loc[idx, column] = np.nan
                            print(f"  - í–‰ {idx+1}: TBA â†’ NaN (ìœ íš¨í•œ ì´ì „ ê°’ ì—†ìŒ)")
    
    if tba_replacements > 0:
        print(f"\nâœ… ì´ {tba_replacements}ê°œì˜ TBA ê°’ì„ ì „ë‚  ê°’ìœ¼ë¡œ ëŒ€ì²´í–ˆìŠµë‹ˆë‹¤.")
    
    return cleaned_data

def fill_missing_values_advanced(data):
    """ê³ ê¸‰ ê²°ì¸¡ì¹˜ ì±„ìš°ê¸° (forward fill + backward fill)"""
    filled_data = data.copy()
    
    # Date ì—´ ì œì™¸í•œ ëª¨ë“  ìˆ˜ì¹˜í˜• ì—´ì— ëŒ€í•´
    numeric_cols = filled_data.select_dtypes(include=[np.number]).columns
    
    # ì´ì „ ê°’ìœ¼ë¡œ ê²°ì¸¡ì¹˜ ì±„ìš°ê¸° (forward fill)
    filled_data[numeric_cols] = filled_data[numeric_cols].ffill()
    
    # ë‚¨ì€ ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” ê²½ìš° ë‹¤ìŒ ê°’ìœ¼ë¡œ ì±„ìš°ê¸° (backward fill)
    filled_data[numeric_cols] = filled_data[numeric_cols].bfill()
    
    return filled_data

def rename_columns_to_standard(data):
    """ì—´ ì´ë¦„ì„ í‘œì¤€ í˜•íƒœë¡œ ë³€ê²½"""
    column_mapping = {
        'Date': 'Date',
        'Crude Oil_WTI': 'WTI',
        'Crude Oil_Brent': 'Brent',
        'Crude Oil_Dubai': 'Dubai',
        'WS_AG-SG_55': 'WS_55',
        'WS_75.0': 'WS_75',
        'Naphtha_Platts_MOPJ': 'MOPJ',
        'Naphtha_MOPAG': 'MOPAG',
        'Naphtha_MOPS': 'MOPS',
        'Naphtha_Monthly Spread': 'Monthly Spread',
        'LPG_Argus FEI_C3': 'C3_LPG',
        'LPG_C4': 'C4_LPG',
        'Gasoline_FOB SP_92RON': 'Gasoline_92RON',
        'Gasoline_95RON': 'Gasoline_95RON',
        'Ethylene_Platts_CFR NEA': 'EL_CRF NEA',
        'Ethylene_CFR SEA': 'EL_CRF SEA',
        'Propylene_Platts_FOB Korea': 'PL_FOB Korea',
        'Benzene_Platts_FOB Korea': 'BZ_FOB Korea',
        'Benzene_Platts_FOB SEA': 'BZ_FOB SEA',
        'Benzene_Platts_FOB US M1': 'BZ_FOB US M1',
        'Benzene_Platts_FOB US M2': 'BZ_FOB US M2',
        'Benzene_Platts_H2-TIME SPREAD': 'BZ_H2-TIME SPREAD',
        'Toluene_Platts_FOB Korea': 'TL_FOB Korea',
        'Toluene_Platts_FOB US M1': 'TL_FOB US M1',
        'Toluene_Platts_FOB US M2': 'TL_FOB US M2',
        'MX_Platts FE_FOB K': 'MX_FOB Korea',
        'PX_FOB   Korea': 'PX_FOB Korea',
        'SM_FOB   Korea': 'SM_FOB Korea',
        'RPG Value_Calculated_FOB PG': 'RPG Value_FOB PG',
        'FO_Platts_HSFO 180 CST': 'FO_HSFO 180 CST',
        'MTBE_Platts_FOB S\'pore': 'MTBE_FOB Singapore',
        'MTBE_Dow_Jones': 'Dow_Jones',
        'MTBE_Euro': 'Euro',
        'MTBE_Gold': 'Gold',
        'PP (ICIS)_CIF NWE': 'Europe_CIF NWE',
        'PP (ICIS)_M.G.\n10ppm': 'Europe_M.G_10ppm',
        'PP (ICIS)_RBOB (NYMEX)_M1': 'RBOB (NYMEX)_M1',
        'Brent_WTI': 'Brent_WTI',
        'MOPJ_Mopag_Nap': 'MOPJ_MOPAG',
        'MOPJ_MOPS_Nap': 'MOPJ_MOPS',
        'Naphtha_Spread': 'Naphtha_Spread',
        'MG92_E Nap': 'MG92_E Nap',
        'C3_MOPJ': 'C3_MOPJ',
        'C4_MOPJ': 'C4_MOPJ',
        'Nap_Dubai': 'Nap_Dubai',
        'MG92_Nap_mops': 'MG92_Nap_MOPS',
        '95R_92R_Asia': '95R_92R_Asia',
        'M1_M2_RBOB': 'M1_M2_RBOB',
        'RBOB_Brent_m1': 'RBOB_Brent_m1',
        'RBOB_Brent_m2': 'RBOB_Brent_m2',
        'EL': 'EL_MOPJ',
        'PL': 'PL_MOPJ',
        'BZ_MOPJ': 'BZ_MOPJ',
        'TL': 'TL_MOPJ',
        'PX': 'PX_MOPJ',
        'HD': 'HD_EL',
        'LD_EL': 'LD_EL',
        'LLD': 'LLD_EL',
        'PP_PL': 'PP_PL',
        'SM_EL+BZ_Margin': 'SM_EL+BZ',
        'US_FOBK_BZ': 'US_FOBK_BZ',
        'NAP_HSFO_180': 'NAP_HSFO_180',
        'MTBE_MOPJ': 'MTBE_MOPJ',
        'MTBE_PG': 'Freight_55_PG',
        'MTBE_Maili': 'Freight_55_Maili',
        'Freight (55)_Ruwais_Yosu': 'Freight_55_Yosu',
        'Freight (55)_Daes\'': 'Freight_55_Daes',
        'Freight (55)_Chiba': 'Freight_55_Chiba',
        'Freight (55)_PG': 'Freight_75_PG',
        'Freight (55)_Maili': 'Freight_75_Maili',
        'Freight (75)_Ruwais_Yosu': 'Freight_75_Yosu',
        'Freight (75)_Daes\'': 'Freight_75_Daes',
        'Freight (75)_Chiba': 'Freight_75_Chiba',
        'Freight (75)_PG': 'Flat Rate_PG',
        'Freight (75)_Maili': 'Flat Rate_Maili',
        'Flat Rate_Ruwais_Yosu': 'Flat Rate_Yosu',
        'Flat Rate_Daes\'': 'Flat Rate_Daes',
        'Flat Rate_Chiba': 'Flat Rate_Chiba'
    }
    
    # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì—´ë§Œ ë§¤í•‘
    existing_columns = data.columns.tolist()
    final_mapping = {}
    
    for old_name, new_name in column_mapping.items():
        if old_name in existing_columns:
            final_mapping[old_name] = new_name
    
    # ë§¤í•‘ë˜ì§€ ì•Šì€ ì—´ë“¤ í™•ì¸
    unmapped_columns = [col for col in existing_columns if col not in column_mapping.keys()]
    if unmapped_columns:
        print(f"\n=== ë§¤í•‘ë˜ì§€ ì•Šì€ ì—´ë“¤ ===")
        for col in unmapped_columns:
            print(f"- {col}")
    
    # ì—´ ì´ë¦„ ë³€ê²½
    renamed_data = data.rename(columns=final_mapping)
    
    print(f"\n=== ì—´ ì´ë¦„ ë³€ê²½ ì™„ë£Œ ===")
    print(f"ë³€ê²½ëœ ì—´ ê°œìˆ˜: {len(final_mapping)}")
    print(f"ìµœì¢… ë°ì´í„° í˜•íƒœ: {renamed_data.shape}")
    
    return renamed_data

# process_data_250620.pyì˜ ì¶”ê°€ í•¨ìˆ˜ë“¤
def remove_missing_and_analyze(data, threshold=10):
    """
    ì¤‘ê°„ ìˆ˜ì¤€ì˜ ê²°ì¸¡ì¹˜ ë¹„ìœ¨ì„ ê°€ì§„ ì—´ì„ ì œê±°í•˜ê³  ë¶„ì„í•˜ëŠ” í•¨ìˆ˜
    (process_data_250620.pyì—ì„œ ê°€ì ¸ì˜¨ í•¨ìˆ˜)
    """
    # ê²°ì¸¡ì¹˜ ë¹„ìœ¨ ê³„ì‚°
    missing_ratio = (data.isnull().sum() / len(data)) * 100
    
    # threshold% ì´ìƒ ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” ì—´ ì‹ë³„
    high_missing_cols = missing_ratio[missing_ratio >= threshold]
    
    if len(high_missing_cols) > 0:
        logger.info(f"\n=== {threshold}% ì´ìƒ ê²°ì¸¡ì¹˜ê°€ ìˆì–´ ì œê±°ë  ì—´ ëª©ë¡ ===")
        for col, ratio in high_missing_cols.items():
            logger.info(f"- {col}: {ratio:.1f}%")
        
        # ê²°ì¸¡ì¹˜ê°€ threshold% ì´ìƒì¸ ì—´ ì œê±°
        cleaned_data = data.drop(columns=high_missing_cols.index)
        logger.info(f"\nì›ë³¸ ë°ì´í„° í˜•íƒœ: {data.shape}")
        logger.info(f"ì •ì œëœ ë°ì´í„° í˜•íƒœ: {cleaned_data.shape}")
    else:
        cleaned_data = data
        logger.info(f"\nì œê±°í•  {threshold}% ì´ìƒ ê²°ì¸¡ì¹˜ ì—´ ì—†ìŒ: {data.shape}")
    
    return cleaned_data

def find_text_missings(data, text_patterns=['NOP', 'No Publication']):
    """
    ë¬¸ìì—´ í˜•íƒœì˜ ê²°ì¸¡ì¹˜ë¥¼ ì°¾ëŠ” í•¨ìˆ˜
    (process_data_250620.pyì—ì„œ ê°€ì ¸ì˜¨ í•¨ìˆ˜)
    """
    logger.info("\n=== ë¬¸ìì—´ í˜•íƒœì˜ ê²°ì¸¡ì¹˜ ë¶„ì„ ===")
    
    # ê° íŒ¨í„´ë³„ë¡œ ê²€ì‚¬
    for pattern in text_patterns:
        logger.info(f"\n['{pattern}' í¬í•¨ëœ ë°ì´í„° í™•ì¸]")
        
        # ëª¨ë“  ì—´ì— ëŒ€í•´ ê²€ì‚¬
        for column in data.columns:
            # ë¬¸ìì—´ ë°ì´í„°ë§Œ ê²€ì‚¬
            if data[column].dtype == 'object':
                # í•´ë‹¹ íŒ¨í„´ì´ í¬í•¨ëœ ë°ì´í„° ì°¾ê¸°
                mask = data[column].astype(str).str.contains(pattern, na=False, case=False)
                matches = data[mask]
                
                if len(matches) > 0:
                    logger.info(f"\nì—´: {column}")
                    logger.info(f"ë°œê²¬ëœ íšŸìˆ˜: {len(matches)}")

def final_clean_data_improved(data):
    """
    ìµœì¢… ë°ì´í„° ì •ì œ í•¨ìˆ˜ (process_data_250620.pyì—ì„œ ê°€ì ¸ì˜¨ í•¨ìˆ˜)
    M1_M2_RBOB ì»¬ëŸ¼ì˜ ê²°ì¸¡ì¹˜ë‚˜ 'Q' ê°’ì„ RBOB_Brent_m1 - RBOB_Brent_m2ë¡œ ê³„ì‚°í•´ì„œ ì±„ì›€
    """
    # ë°ì´í„° ë³µì‚¬ë³¸ ìƒì„±
    cleaned_data = data.copy()
    
    # MTBE_Dow_Jones ì—´ íŠ¹ë³„ ì²˜ë¦¬
    for col in ['MTBE_Dow_Jones']:
        if col in cleaned_data.columns:
            # ìˆ«ìë¡œ ë³€í™˜ ì‹œë„
            cleaned_data[col] = pd.to_numeric(cleaned_data[col], errors='coerce')
    
    # ğŸ”§ M1_M2_RBOB ì—´ íŠ¹ë³„ ì²˜ë¦¬: ê²°ì¸¡ì¹˜ì™€ 'Q' ê°’ì„ ê³„ì‚°ìœ¼ë¡œ ì±„ìš°ê¸°
    if 'M1_M2_RBOB' in cleaned_data.columns and 'RBOB_Brent_m1' in cleaned_data.columns and 'RBOB_Brent_m2' in cleaned_data.columns:
        logger.info(f"\n=== M1_M2_RBOB ì—´ ì²˜ë¦¬ ì‹œì‘ ===")
        logger.info(f"ì²˜ë¦¬ ì „ ë°ì´í„° íƒ€ì…: {cleaned_data['M1_M2_RBOB'].dtype}")
        logger.info(f"ì²˜ë¦¬ ì „ ê²°ì¸¡ì¹˜ ê°œìˆ˜: {cleaned_data['M1_M2_RBOB'].isnull().sum()}")
        
        # 'Q' ê°’ë“¤ê³¼ ê¸°íƒ€ ë¬¸ìì—´ ê°’ë“¤ì„ NaNìœ¼ë¡œ ë³€í™˜
        original_values = cleaned_data['M1_M2_RBOB'].copy()
        q_count = 0
        other_string_count = 0
        
        # 'Q' ê°’ ê°œìˆ˜ í™•ì¸
        if cleaned_data['M1_M2_RBOB'].dtype == 'object':
            q_mask = cleaned_data['M1_M2_RBOB'].astype(str).str.upper() == 'Q'
            q_count = q_mask.sum()
            
            # ê¸°íƒ€ ë¬¸ìì—´ ê°’ë“¤ í™•ì¸
            numeric_convertible = pd.to_numeric(cleaned_data['M1_M2_RBOB'], errors='coerce')
            string_mask = pd.isna(numeric_convertible) & cleaned_data['M1_M2_RBOB'].notna()
            other_string_count = string_mask.sum() - q_count
            
            if q_count > 0:
                logger.info(f"'Q' ê°’ {q_count}ê°œ ë°œê²¬")
            if other_string_count > 0:
                logger.info(f"ê¸°íƒ€ ë¬¸ìì—´ ê°’ {other_string_count}ê°œ ë°œê²¬")
        
        # 'Q' ê°’ë“¤ê³¼ ê¸°íƒ€ ë¬¸ìì—´ì„ NaNìœ¼ë¡œ ë³€í™˜
        cleaned_data['M1_M2_RBOB'] = cleaned_data['M1_M2_RBOB'].replace('Q', np.nan)
        cleaned_data['M1_M2_RBOB'] = cleaned_data['M1_M2_RBOB'].replace('q', np.nan)
        
        # ë¬¸ìì—´ë¡œ ì €ì¥ëœ ìˆ«ìë“¤ì„ ì‹¤ì œ ìˆ«ìë¡œ ë³€í™˜
        cleaned_data['M1_M2_RBOB'] = pd.to_numeric(cleaned_data['M1_M2_RBOB'], errors='coerce')
        
        # ê²°ì¸¡ì¹˜ì™€ 'Q' ê°’ë“¤ì„ ê³„ì‚°ìœ¼ë¡œ ì±„ìš°ê¸°: M1_M2_RBOB = RBOB_Brent_m1 - RBOB_Brent_m2
        missing_mask = cleaned_data['M1_M2_RBOB'].isnull()
        missing_count_before = missing_mask.sum()
        
        if missing_count_before > 0:
            logger.info(f"ê²°ì¸¡ì¹˜ {missing_count_before}ê°œë¥¼ ê³„ì‚°ìœ¼ë¡œ ì±„ì›ë‹ˆë‹¤: M1_M2_RBOB = RBOB_Brent_m1 - RBOB_Brent_m2")
            
            # ê³„ì‚° ê°€ëŠ¥í•œ í–‰ë“¤ë§Œ ì„ íƒ (m1, m2 ë‘˜ ë‹¤ ìœ íš¨í•œ ê°’ì´ ìˆëŠ” ê²½ìš°)
            can_calculate = (missing_mask & 
                           cleaned_data['RBOB_Brent_m1'].notna() & 
                           cleaned_data['RBOB_Brent_m2'].notna())
            calculated_count = can_calculate.sum()
            
            if calculated_count > 0:
                # ê³„ì‚° ìˆ˜í–‰
                calculated_values = (cleaned_data.loc[can_calculate, 'RBOB_Brent_m1'] - 
                                   cleaned_data.loc[can_calculate, 'RBOB_Brent_m2'])
                
                cleaned_data.loc[can_calculate, 'M1_M2_RBOB'] = calculated_values
                logger.info(f"ì‹¤ì œë¡œ ê³„ì‚°ëœ ê°’: {calculated_count}ê°œ")
                
                # ê³„ì‚° ê²€ì¦ (ì²˜ìŒ 5ê°œ ê°’ ì¶œë ¥)
                logger.info(f"=== ê³„ì‚° ê²€ì¦ (ì²˜ìŒ 5ê°œ ê³„ì‚°ëœ ê°’) ===")
                calculated_rows = cleaned_data[can_calculate].head(5)
                for idx, row in calculated_rows.iterrows():
                    m1_val = row['RBOB_Brent_m1']
                    m2_val = row['RBOB_Brent_m2']
                    calculated_val = row['M1_M2_RBOB']
                    logger.info(f"ì¸ë±ìŠ¤ {idx}: {m1_val:.6f} - {m2_val:.6f} = {calculated_val:.6f}")
                    
            else:
                logger.warning("ê³„ì‚° ê°€ëŠ¥í•œ í–‰ì´ ì—†ìŠµë‹ˆë‹¤ (RBOB_Brent_m1 ë˜ëŠ” RBOB_Brent_m2ì— ê²°ì¸¡ì¹˜ê°€ ìˆìŒ)")
        
        # ì²˜ë¦¬ í›„ ê²°ê³¼ í™•ì¸
        missing_count_after = cleaned_data['M1_M2_RBOB'].isnull().sum()
        valid_count = cleaned_data['M1_M2_RBOB'].notna().sum()
        
        logger.info(f"\n=== M1_M2_RBOB ì—´ ì²˜ë¦¬ í›„ ===")
        logger.info(f"ë°ì´í„° íƒ€ì…: {cleaned_data['M1_M2_RBOB'].dtype}")
        logger.info(f"ê²°ì¸¡ì¹˜ ê°œìˆ˜: {missing_count_after}")
        logger.info(f"ìœ íš¨ ë°ì´í„° ê°œìˆ˜: {valid_count}")
        logger.info(f"ì²˜ë¦¬ëœ ê²°ì¸¡ì¹˜ ê°œìˆ˜: {missing_count_before - missing_count_after}")
        
        if valid_count > 0:
            logger.info(f"ìµœì†Œê°’: {cleaned_data['M1_M2_RBOB'].min():.6f}")
            logger.info(f"ìµœëŒ€ê°’: {cleaned_data['M1_M2_RBOB'].max():.6f}")
            logger.info(f"í‰ê· ê°’: {cleaned_data['M1_M2_RBOB'].mean():.6f}")
    
    else:
        # í•„ìš”í•œ ì»¬ëŸ¼ì´ ì—†ëŠ” ê²½ìš°
        missing_cols = []
        for col in ['M1_M2_RBOB', 'RBOB_Brent_m1', 'RBOB_Brent_m2']:
            if col not in cleaned_data.columns:
                missing_cols.append(col)
        
        if missing_cols:
            logger.warning(f"M1_M2_RBOB ê³„ì‚°ì— í•„ìš”í•œ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {missing_cols}")
    
    return cleaned_data

def clean_and_trim_data(data, start_date='2013-02-06'):
    """
    ë°ì´í„° ì •ì œ ë° ë‚ ì§œ ë²”ìœ„ ì¡°ì • í•¨ìˆ˜
    (process_data_250620.pyì—ì„œ ê°€ì ¸ì˜¨ í•¨ìˆ˜)
    """
    # ì‹œì‘ ë‚ ì§œ ì´í›„ì˜ ë°ì´í„°ë§Œ ì„ íƒ
    cleaned_data = data[data['Date'] >= pd.to_datetime(start_date)].copy()
    
    # ê¸°ë³¸ ì •ë³´ ì¶œë ¥
    logger.info(f"=== ë°ì´í„° ì²˜ë¦¬ ê²°ê³¼ ===")
    logger.info(f"ì›ë³¸ ë°ì´í„° ê¸°ê°„: {data['Date'].min()} ~ {data['Date'].max()}")
    logger.info(f"ì²˜ë¦¬ëœ ë°ì´í„° ê¸°ê°„: {cleaned_data['Date'].min()} ~ {cleaned_data['Date'].max()}")
    logger.info(f"ì›ë³¸ ë°ì´í„° í–‰ ìˆ˜: {len(data)}")
    logger.info(f"ì²˜ë¦¬ëœ ë°ì´í„° í–‰ ìˆ˜: {len(cleaned_data)}")
    
    return cleaned_data

def load_and_process_data_improved(file_path, sheet_name, start_date):
    """
    ê°œì„ ëœ ë°ì´í„° ë¡œë“œ ë° ì²˜ë¦¬ í•¨ìˆ˜
    (process_data_250620.pyì—ì„œ ê°€ì ¸ì˜¨ í•¨ìˆ˜)
    """
    # ì—´ ì´ë¦„ ìƒì„±
    column_names = create_proper_column_names(file_path, sheet_name)
    
    # ì‹¤ì œ ë°ì´í„° ì½ê¸°
    data = pd.read_excel(file_path, sheet_name=sheet_name, header=None, skiprows=3)
    data.columns = column_names
    
    # Date ì—´ ë³€í™˜
    data['Date'] = pd.to_datetime(data['Date'], errors='coerce')
    
    # ì‹œì‘ ë‚ ì§œ ì´í›„ ë°ì´í„°ë§Œ í•„í„°ë§
    data = data[data['Date'] >= start_date]
    
    # ë¶ˆí•„ìš”í•œ ì—´ ì œê±°
    data = data.loc[:, ~data.columns.str.startswith('Unnamed')]
    
    return data

def process_excel_data_complete(file_path, sheet_name='29 Nov 2010 till todate', start_date='2013-01-04'):
    """
    Excel ë°ì´í„°ë¥¼ ì™„ì „íˆ ì²˜ë¦¬í•˜ëŠ” í†µí•© í•¨ìˆ˜
    (process_data_250620.pyì˜ ë©”ì¸ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì„ í•¨ìˆ˜í™”)
    """
    try:
        logger.info("=== Excel ë°ì´í„° ì™„ì „ ì²˜ë¦¬ ì‹œì‘ === ğŸ“Š")
        
        # 1. ë°ì´í„° ë¡œë“œ ë° ê¸°ë³¸ ì²˜ë¦¬
        cleaned_data = load_and_process_data_improved(file_path, sheet_name, pd.Timestamp(start_date))
        logger.info(f"ì´ˆê¸° ë°ì´í„° í˜•íƒœ: {cleaned_data.shape}")
        
        # 2. 70% ì´ìƒ ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” ì—´ ì œê±°
        final_data = remove_high_missing_columns(cleaned_data, threshold=70)
        
        # 3. 10% ì´ìƒ ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” ì—´ ì œê±°  
        final_cleaned_data = remove_missing_and_analyze(final_data, threshold=10)
        
        # 4. í…ìŠ¤íŠ¸ í˜•íƒœì˜ ê²°ì¸¡ì¹˜ ì²˜ë¦¬
        text_patterns = ['NOP', 'No Publication', 'N/A', 'na', 'NA', 'none', 'None', '-']
        find_text_missings(final_cleaned_data, text_patterns)
        
        # 5. í…ìŠ¤íŠ¸ ê°’ë“¤ ì •ì œ
        final_cleaned_data_v2 = clean_text_values_advanced(final_cleaned_data)
        
        # 6. ìµœì¢… ì •ì œ
        final_data_clean = final_clean_data_improved(final_cleaned_data_v2)
        
        # 7. ê²°ì¸¡ì¹˜ ì±„ìš°ê¸°
        filled_final_data = fill_missing_values_advanced(final_data_clean)
        
        # 8. ë‚ ì§œ ë²”ìœ„ ì¡°ì •
        trimmed_data = clean_and_trim_data(filled_final_data, start_date='2013-02-06')
        
        # 9. ì—´ ì´ë¦„ì„ ìµœì¢… í˜•íƒœë¡œ ë³€ê²½
        final_renamed_data = rename_columns_to_standard(trimmed_data)
        
        logger.info(f"\n=== ìµœì¢… ê²°ê³¼ ===")
        logger.info(f"ìµœì¢… ë°ì´í„° í˜•íƒœ: {final_renamed_data.shape}")
        logger.info(f"ìµœì¢… ì—´ ì´ë¦„ë“¤: {len(final_renamed_data.columns)}ê°œ")
        
        return final_renamed_data
        
    except Exception as e:
        logger.error(f"Excel ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        logger.error(traceback.format_exc())
        return None

# xlwings ëŒ€ì•ˆ ë¡œë” (ë³´ì•ˆí”„ë¡œê·¸ë¨ì´ íŒŒì¼ì„ ì ê·¸ëŠ” ê²½ìš° ì‚¬ìš©)
try:
    import xlwings as xw
    XLWINGS_AVAILABLE = True
    logger.info("âœ… xlwings library available - Excel security bypass enabled")
except ImportError:
    XLWINGS_AVAILABLE = False
    logger.warning("âš ï¸ xlwings not available - falling back to pandas only")

def load_data_with_xlwings(file_path, model_type=None):
    """
    xlwingsë¥¼ ì‚¬ìš©í•˜ì—¬ ë³´ì•ˆí”„ë¡œê·¸ë¨ì´ íŒŒì¼ì„ ì ê·¸ëŠ” ìƒí™©ì—ì„œë„ ì•ˆì •ì ìœ¼ë¡œ Excel íŒŒì¼ì„ ì½ëŠ” í•¨ìˆ˜
    
    Args:
        file_path (str): Excel íŒŒì¼ ê²½ë¡œ
        model_type (str): ëª¨ë¸ íƒ€ì… ('lstm', 'varmax', None)
    
    Returns:
        pd.DataFrame: ì „ì²˜ë¦¬ëœ ë°ì´í„°í”„ë ˆì„
    """
    if not XLWINGS_AVAILABLE:
        raise ImportError("xlwings is not available. Please install it with: pip install xlwings")
    
    logger.info(f"ğŸ”“ [XLWINGS] Loading Excel file with security bypass: {os.path.basename(file_path)}")
    
    app = None
    wb = None
    
    try:
        # Excel ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹œì‘
        app = xw.App(visible=False, add_book=False)
        app.display_alerts = False  # ê²½ê³ ì°½ ë¹„í™œì„±í™”
        app.screen_updating = False  # í™”ë©´ ì—…ë°ì´íŠ¸ ë¹„í™œì„±í™” (ì„±ëŠ¥ í–¥ìƒ)
        
        logger.info(f"ğŸ“± [XLWINGS] Excel app started (PID: {app.pid})")
        
        # Excel íŒŒì¼ ì—´ê¸°
        wb = app.books.open(file_path, read_only=True, update_links=False)
        logger.info(f"ğŸ“– [XLWINGS] Workbook opened: {wb.name}")
        
        # ì ì ˆí•œ ì‹œíŠ¸ ì°¾ê¸°
        sheet_names = [sheet.name for sheet in wb.sheets]
        logger.info(f"ğŸ“‹ [XLWINGS] Available sheets: {sheet_names}")
        
        # ê¸°ë³¸ ì‹œíŠ¸ëª… ë˜ëŠ” ì²« ë²ˆì§¸ ì‹œíŠ¸ ì‚¬ìš©
        target_sheet_name = '29 Nov 2010 till todate'
        if target_sheet_name in sheet_names:
            sheet = wb.sheets[target_sheet_name]
            logger.info(f"ğŸ¯ [XLWINGS] Using target sheet: {target_sheet_name}")
        else:
            sheet = wb.sheets[0]  # ì²« ë²ˆì§¸ ì‹œíŠ¸ ì‚¬ìš©
            logger.info(f"ğŸ¯ [XLWINGS] Using first sheet: {sheet.name}")
        
        # ì‚¬ìš©ëœ ë²”ìœ„ í™•ì¸
        used_range = sheet.used_range
        if used_range is None:
            raise ValueError("Sheet appears to be empty")
        
        logger.info(f"ğŸ“ [XLWINGS] Used range: {used_range.address}")
        
        # ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ì½ê¸° (í—¤ë” í¬í•¨)
        # xlwingsì˜ expand='table' ì˜µì…˜ìœ¼ë¡œ ìë™ìœ¼ë¡œ ì „ì²´ ë°ì´í„° ë²”ìœ„ ê°ì§€
        df = sheet['A1'].options(pd.DataFrame, index=False, expand='table').value
        
        logger.info(f"ğŸ“Š [XLWINGS] Raw data loaded: {df.shape}")
        logger.info(f"ğŸ“‹ [XLWINGS] Columns: {list(df.columns)}")
        
        # ë°ì´í„° ê²€ì¦
        if df is None or df.empty:
            raise ValueError("No data found in the Excel file")
        
        # Date ì»¬ëŸ¼ í™•ì¸ ë° ì²˜ë¦¬
        if 'Date' not in df.columns:
            # ì²« ë²ˆì§¸ ì»¬ëŸ¼ì´ ë‚ ì§œì¼ ê°€ëŠ¥ì„± í™•ì¸
            first_col = df.columns[0]
            if 'date' in first_col.lower() or df[first_col].dtype == 'datetime64[ns]':
                df = df.rename(columns={first_col: 'Date'})
                logger.info(f"ğŸ”„ [XLWINGS] Renamed '{first_col}' to 'Date'")
            else:
                raise ValueError("Date column not found in the data")
        
        # Date ì»¬ëŸ¼ì„ datetimeìœ¼ë¡œ ë³€í™˜
        df['Date'] = pd.to_datetime(df['Date'])
        df.set_index('Date', inplace=True)
        
        logger.info(f"ğŸ“… [XLWINGS] Date range: {df.index.min()} to {df.index.max()}")
        
        # ëª¨ë¸ íƒ€ì…ë³„ ë°ì´í„° í•„í„°ë§ (ê¸°ì¡´ load_dataì™€ ë™ì¼)
        if model_type == 'lstm':
            cutoff_date = pd.to_datetime('2022-01-01')
            original_shape = df.shape
            df = df[df.index >= cutoff_date]
            logger.info(f"ğŸ” [XLWINGS] LSTM filter: {original_shape[0]} -> {df.shape[0]} records")
            
            if df.empty:
                raise ValueError("No data available after 2022-01-01 filter for LSTM model")
        
        # ê¸°ë³¸ ë°ì´í„° ì •ì œ
        df = df.replace([np.inf, -np.inf], np.nan)
        df = df.fillna(method='ffill').fillna(method='bfill')
        
        logger.info(f"âœ… [XLWINGS] Data loaded successfully: {df.shape}")
        return df
        
    except Exception as e:
        logger.error(f"âŒ [XLWINGS] Error loading file: {str(e)}")
        raise e
        
    finally:
        # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
        try:
            if wb is not None:
                wb.close()
                logger.info("ğŸ“– [XLWINGS] Workbook closed")
        except:
            pass
        
        try:
            if app is not None:
                app.quit()
                logger.info("ğŸ“± [XLWINGS] Excel app closed")
        except:
            pass

def load_csv_with_xlwings(csv_path):
    """
    xlwingsë¥¼ ì‚¬ìš©í•˜ì—¬ CSV íŒŒì¼ì„ ì½ëŠ” í•¨ìˆ˜ - ë³´ì•ˆí”„ë¡œê·¸ë¨ ìš°íšŒ
    
    Args:
        csv_path (str): CSV íŒŒì¼ ê²½ë¡œ
    
    Returns:
        pd.DataFrame: CSV ë°ì´í„°í”„ë ˆì„
    """
    if not XLWINGS_AVAILABLE:
        raise ImportError("xlwings is not available for CSV loading")
    
    logger.info(f"ğŸ”“ [XLWINGS_CSV] Loading CSV file with security bypass: {os.path.basename(csv_path)}")
    
    app = None
    wb = None
    
    try:
        # Excel ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹œì‘
        app = xw.App(visible=False, add_book=False)
        app.display_alerts = False
        app.screen_updating = False
        
        logger.info(f"ğŸ“± [XLWINGS_CSV] Excel app started for CSV")
        
        # CSV íŒŒì¼ì„ Excelë¡œ ì—´ê¸° (CSVëŠ” ìë™ìœ¼ë¡œ íŒŒì‹±ë¨)
        wb = app.books.open(csv_path, read_only=True, update_links=False)
        logger.info(f"ğŸ“– [XLWINGS_CSV] CSV workbook opened: {wb.name}")
        
        # ì²« ë²ˆì§¸ ì‹œíŠ¸ ì‚¬ìš© (CSVëŠ” í•­ìƒ í•˜ë‚˜ì˜ ì‹œíŠ¸ë§Œ ê°€ì§)
        sheet = wb.sheets[0]
        
        # ì‚¬ìš©ëœ ë²”ìœ„ í™•ì¸
        used_range = sheet.used_range
        if used_range is None:
            raise ValueError("CSV file appears to be empty")
        
        logger.info(f"ğŸ“ [XLWINGS_CSV] Used range: {used_range.address}")
        
        # ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ì½ê¸° (í—¤ë” í¬í•¨)
        df = sheet['A1'].options(pd.DataFrame, index=False, expand='table').value
        
        logger.info(f"ğŸ“Š [XLWINGS_CSV] CSV data loaded: {df.shape}")
        logger.info(f"ğŸ“‹ [XLWINGS_CSV] Columns: {list(df.columns)}")
        
        # ë°ì´í„° ê²€ì¦
        if df is None or df.empty:
            raise ValueError("No data found in the CSV file")
        
        logger.info(f"âœ… [XLWINGS_CSV] CSV loaded successfully: {df.shape}")
        return df
        
    except Exception as e:
        logger.error(f"âŒ [XLWINGS_CSV] Error loading CSV file: {str(e)}")
        raise e
        
    finally:
        # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
        try:
            if wb is not None:
                wb.close()
                logger.info("ğŸ“– [XLWINGS_CSV] CSV workbook closed")
        except:
            pass
        
        try:
            if app is not None:
                app.quit()
                logger.info("ğŸ“± [XLWINGS_CSV] Excel app closed")
        except:
            pass

def load_data_safe_holidays(file_path):
    """
    íœ´ì¼ íŒŒì¼ ì „ìš© xlwings ë¡œë”© í•¨ìˆ˜ - ë³´ì•ˆí”„ë¡œê·¸ë¨ ìš°íšŒ
    
    Args:
        file_path (str): íœ´ì¼ Excel íŒŒì¼ ê²½ë¡œ
    
    Returns:
        pd.DataFrame: íœ´ì¼ ë°ì´í„°í”„ë ˆì„ (date, description ì»¬ëŸ¼)
    """
    if not XLWINGS_AVAILABLE:
        raise ImportError("xlwings is not available for holiday file loading")
    
    logger.info(f"ğŸ”“ [HOLIDAYS_XLWINGS] Loading holiday file with security bypass: {os.path.basename(file_path)}")
    
    app = None
    wb = None
    
    try:
        # Excel ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹œì‘
        app = xw.App(visible=False, add_book=False)
        app.display_alerts = False
        app.screen_updating = False
        
        logger.info(f"ğŸ“± [HOLIDAYS_XLWINGS] Excel app started for holidays")
        
        # Excel íŒŒì¼ ì—´ê¸°
        wb = app.books.open(file_path, read_only=True, update_links=False)
        logger.info(f"ğŸ“– [HOLIDAYS_XLWINGS] Holiday workbook opened: {wb.name}")
        
        # ì²« ë²ˆì§¸ ì‹œíŠ¸ ì‚¬ìš© (íœ´ì¼ íŒŒì¼ì€ ë³´í†µ ë‹¨ìˆœ êµ¬ì¡°)
        sheet = wb.sheets[0]
        logger.info(f"ğŸ¯ [HOLIDAYS_XLWINGS] Using sheet: {sheet.name}")
        
        # ì‚¬ìš©ëœ ë²”ìœ„ í™•ì¸
        used_range = sheet.used_range
        if used_range is None:
            raise ValueError("Holiday sheet appears to be empty")
        
        logger.info(f"ğŸ“ [HOLIDAYS_XLWINGS] Used range: {used_range.address}")
        
        # ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ì½ê¸° (í—¤ë” í¬í•¨)
        df = sheet['A1'].options(pd.DataFrame, index=False, expand='table').value
        
        logger.info(f"ğŸ“Š [HOLIDAYS_XLWINGS] Holiday data loaded: {df.shape}")
        logger.info(f"ğŸ“‹ [HOLIDAYS_XLWINGS] Columns: {list(df.columns)}")
        
        # ë°ì´í„° ê²€ì¦
        if df is None or df.empty:
            raise ValueError("No holiday data found in the Excel file")
        
        # ì»¬ëŸ¼ëª… ì •ê·œí™” (case-insensitive)
        df.columns = df.columns.str.lower()
        
        # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
        if 'date' not in df.columns:
            # ì²« ë²ˆì§¸ ì»¬ëŸ¼ì„ ë‚ ì§œë¡œ ê°€ì •
            first_col = df.columns[0]
            df = df.rename(columns={first_col: 'date'})
            logger.info(f"ğŸ”„ [HOLIDAYS_XLWINGS] Renamed '{first_col}' to 'date'")
        
        # description ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ì¶”ê°€
        if 'description' not in df.columns:
            df['description'] = 'Holiday'
            logger.info(f"â• [HOLIDAYS_XLWINGS] Added default 'description' column")
        
        logger.info(f"âœ… [HOLIDAYS_XLWINGS] Holiday data loaded successfully: {len(df)} holidays")
        return df
        
    except Exception as e:
        logger.error(f"âŒ [HOLIDAYS_XLWINGS] Error loading holiday file: {str(e)}")
        raise e
        
    finally:
        # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
        try:
            if wb is not None:
                wb.close()
                logger.info("ğŸ“– [HOLIDAYS_XLWINGS] Holiday workbook closed")
        except:
            pass
        
        try:
            if app is not None:
                app.quit()
                logger.info("ğŸ“± [HOLIDAYS_XLWINGS] Excel app closed")
        except:
            pass

def load_data_safe(file_path, model_type=None, use_cache=True, use_xlwings_fallback=True):
    """
    ì•ˆì „í•œ ë°ì´í„° ë¡œë”© í•¨ìˆ˜ - ë³´ì•ˆ ë¬¸ì œ ì‹œ xlwingsë¡œ ìë™ ì „í™˜
    
    Args:
        file_path (str): ë°ì´í„° íŒŒì¼ ê²½ë¡œ
        model_type (str): ëª¨ë¸ íƒ€ì… ('lstm', 'varmax', None)
        use_cache (bool): ë©”ëª¨ë¦¬ ìºì‹œ ì‚¬ìš© ì—¬ë¶€
        use_xlwings_fallback (bool): ì‹¤íŒ¨ ì‹œ xlwings ì‚¬ìš© ì—¬ë¶€
    
    Returns:
        pd.DataFrame: ì „ì²˜ë¦¬ëœ ë°ì´í„°í”„ë ˆì„
    """
    try:
        # ë¨¼ì € ê¸°ë³¸ load_data í•¨ìˆ˜ ì‹œë„
        return load_data(file_path, model_type, use_cache)
        
    except (PermissionError, OSError, pd.errors.ExcelFileError) as e:
        # íŒŒì¼ ì ‘ê·¼ ì˜¤ë¥˜ ì‹œ xlwingsë¡œ ëŒ€ì²´ ì‹œë„
        if use_xlwings_fallback and XLWINGS_AVAILABLE and file_path.endswith(('.xlsx', '.xls')):
            logger.warning(f"âš ï¸ [SECURITY_BYPASS] Standard loading failed: {str(e)}")
            logger.info("ğŸ”“ [SECURITY_BYPASS] Attempting xlwings bypass...")
            
            try:
                return load_data_with_xlwings(file_path, model_type)
            except Exception as xlwings_error:
                logger.error(f"âŒ [SECURITY_BYPASS] xlwings also failed: {str(xlwings_error)}")
                raise e  # ì›ë˜ ì˜¤ë¥˜ë¥¼ ë‹¤ì‹œ ë°œìƒ
        else:
            raise e

# ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬ í•¨ìˆ˜
def load_data(file_path, model_type=None, use_cache=True):
    """
    ë°ì´í„° ë¡œë“œ ë° ê¸°ë³¸ ì „ì²˜ë¦¬
    
    Args:
        file_path (str): ë°ì´í„° íŒŒì¼ ê²½ë¡œ
        model_type (str): ëª¨ë¸ íƒ€ì… ('lstm', 'varmax', None)
                         - 'lstm': ë‹¨ì¼/ëˆ„ì  ì˜ˆì¸¡ìš©, 2022ë…„ ì´ì „ ë°ì´í„° ì œê±°
                         - 'varmax': ì¥ê¸°ì˜ˆì¸¡ìš©, ëª¨ë“  ë°ì´í„° ìœ ì§€
                         - None: ê¸°ë³¸ ë™ì‘ (ëª¨ë“  ë°ì´í„° ìœ ì§€)
        use_cache (bool): ë©”ëª¨ë¦¬ ìºì‹œ ì‚¬ìš© ì—¬ë¶€ (default: True)
    
    Returns:
        pd.DataFrame: ì „ì²˜ë¦¬ëœ ë°ì´í„°í”„ë ˆì„
    """
    # ğŸ”§ ë©”ëª¨ë¦¬ ìºì‹œ í™•ì¸ (ì¤‘ë³µ ë¡œë”© ë°©ì§€)
    cache_key = f"{file_path}|{model_type}|{os.path.getmtime(file_path)}"
    current_time = time.time()
    
    if use_cache and cache_key in _dataframe_cache:
        cached_data, cache_time = _dataframe_cache[cache_key]
        if (current_time - cache_time) < _cache_expiry_seconds:
            logger.info(f"ğŸš€ [CACHE_HIT] Using cached DataFrame for {os.path.basename(file_path)} (saved {current_time - cache_time:.1f}s ago)")
            return cached_data.copy()  # ë³µì‚¬ë³¸ ë°˜í™˜ìœ¼ë¡œ ì›ë³¸ ë³´í˜¸
        else:
            # ë§Œë£Œëœ ìºì‹œ ì œê±°
            del _dataframe_cache[cache_key]
            logger.info(f"ğŸ—‘ï¸ [CACHE_EXPIRED] Removed expired cache for {os.path.basename(file_path)}")
    
    logger.info(f"ğŸ“ [LOAD_DATA] Loading data with model_type: {model_type} from {os.path.basename(file_path)}")

    
    # íŒŒì¼ í™•ì¥ìì— ë”°ë¼ ë‹¤ë¥¸ ë¡œë“œ ë°©ë²• ì‚¬ìš©
    if file_path.endswith('.csv'):
        logger.info("Loading CSV file with xlwings fallback support")
        # CSV íŒŒì¼ë„ xlwings ìš°ì„  ì‹œë„
        try:
            if XLWINGS_AVAILABLE:
                logger.info(f"ğŸ”“ [XLWINGS_CSV] Attempting to load CSV with xlwings: {file_path}")
                df = load_csv_with_xlwings(file_path)
            else:
                df = pd.read_csv(file_path)
        except Exception as e:
            logger.warning(f"âš ï¸ [XLWINGS_CSV] xlwings failed, falling back to pandas: {str(e)}")
            df = pd.read_csv(file_path)
        
        df['Date'] = pd.to_datetime(df['Date'])
        df.set_index('Date', inplace=True)
        
        # ê¸°ë³¸ì ì¸ ê²°ì¸¡ì¹˜ ì²˜ë¦¬
        df = df.replace([np.inf, -np.inf], np.nan)
        df = df.fillna(method='ffill').fillna(method='bfill')
        
    elif file_path.endswith(('.xlsx', '.xls')):
        logger.info("Loading Excel file with advanced processing pipeline")
        
        # process_data_250620.pyì˜ ì™„ì „í•œ Excel ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‚¬ìš©
        try:
            # 1. Excel íŒŒì¼ì˜ ì ì ˆí•œ ì‹œíŠ¸ ì´ë¦„ ì°¾ê¸°
            sheet_name = '29 Nov 2010 till todate'  # ê¸°ë³¸ ì‹œíŠ¸ëª…
            try:
                # íŒŒì¼ì˜ ì‹œíŠ¸ ëª©ë¡ í™•ì¸
                excel_file = pd.ExcelFile(file_path)
                available_sheets = excel_file.sheet_names
                logger.info(f"Available sheets: {available_sheets}")
                
                # ì ì ˆí•œ ì‹œíŠ¸ ì°¾ê¸°
                if sheet_name not in available_sheets:
                    sheet_name = available_sheets[0]  # ì²« ë²ˆì§¸ ì‹œíŠ¸ ì‚¬ìš©
                    logger.info(f"Default sheet not found, using '{sheet_name}' sheet")
            except:
                sheet_name = 0  # ì¸ë±ìŠ¤ë¡œ ì²« ë²ˆì§¸ ì‹œíŠ¸ ì‚¬ìš©
            
            # 2. process_data_250620.pyì˜ ì™„ì „í•œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
            df = process_excel_data_complete(file_path, sheet_name, start_date='2013-01-04')
            
            if df is None:
                logger.error("Excel ë°ì´í„° ì²˜ë¦¬ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ê¸°ë³¸ ë°©ì‹ìœ¼ë¡œ ë‹¤ì‹œ ì‹œë„í•©ë‹ˆë‹¤.")
                df = pd.read_excel(file_path, sheet_name=sheet_name)
                df['Date'] = pd.to_datetime(df['Date'])
            else:
                logger.info(f"Excel file processed successfully with advanced pipeline: {df.shape}")
                
            # Dateë¥¼ ì¸ë±ìŠ¤ë¡œ ì„¤ì •
            if 'Date' in df.columns:
                df.set_index('Date', inplace=True)
                
        except Exception as e:
            logger.error(f"Advanced Excel processing failed: {e}")
            logger.info("Falling back to standard Excel loading method")
            
            # ì‹¤íŒ¨ì‹œ ê¸°ë³¸ ë°©ì‹ìœ¼ë¡œ ë¡œë“œ
            df = pd.read_excel(file_path)
            df['Date'] = pd.to_datetime(df['Date'])
            df.set_index('Date', inplace=True)
    
    else:
        raise ValueError(f"Unsupported file format: {file_path}")
    
    logger.info(f"Original data shape: {df.shape} (from {df.index.min()} to {df.index.max()})")
    
    # ğŸ”‘ ëª¨ë¸ íƒ€ì…ë³„ ë°ì´í„° í•„í„°ë§
    if model_type == 'lstm':
        # LSTM ëª¨ë¸ìš©: 2022ë…„ ì´ì „ ë°ì´í„° ì œê±°
        cutoff_date = pd.to_datetime('2022-01-01')
        original_shape = df.shape
        df = df[df.index >= cutoff_date]
        
        logger.info(f"ğŸ“Š LSTM model: Filtered data from 2022-01-01")
        logger.info(f"  Original: {original_shape[0]} records")
        logger.info(f"  Filtered: {df.shape[0]} records (removed {original_shape[0] - df.shape[0]} records)")
        logger.info(f"  Date range: {df.index.min()} to {df.index.max()}")
        
        if df.empty:
            raise ValueError("No data available after 2022-01-01 filter for LSTM model")
            
    elif model_type == 'varmax':
        # VARMAX ëª¨ë¸ìš©: ëª¨ë“  ë°ì´í„° ì‚¬ìš©
        logger.info(f"ğŸ“Š VARMAX model: Using all available data")
        logger.info(f"  Full date range: {df.index.min()} to {df.index.max()}")
        
    else:
        # ê¸°ë³¸ ë™ì‘: ëª¨ë“  ë°ì´í„° ì‚¬ìš©
        logger.info(f"ğŸ“Š Default mode: Using all available data")
        logger.info(f"  Full date range: {df.index.min()} to {df.index.max()}")
    
    # ëª¨ë“  inf ê°’ì„ NaNìœ¼ë¡œ ë³€í™˜
    df = df.replace([np.inf, -np.inf], np.nan)
    
    # ê²°ì¸¡ì¹˜ ì²˜ë¦¬ - ëª¨ë“  ì»¬ëŸ¼ì— ë™ì¼í•˜ê²Œ ì ìš©
    df = df.fillna(method='ffill').fillna(method='bfill')
    
    # ì²˜ë¦¬ í›„ ë‚¨ì•„ìˆëŠ” infë‚˜ nan í™•ì¸
    # ìˆ«ì ì»¬ëŸ¼ë§Œ ì„ íƒí•´ì„œ isinf ê²€ì‚¬
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    has_nan = df.isnull().any().any()
    has_inf = False
    if len(numeric_cols) > 0:
        has_inf = np.isinf(df[numeric_cols].values).any()
    
    if has_nan or has_inf:
        logger.warning("Dataset still contains NaN or inf values after preprocessing")
        
        # ğŸ“Š ìƒì„¸í•œ ì»¬ëŸ¼ ë¶„ì„ ë° ë¬¸ì œ ì§„ë‹¨
        logger.warning("=" * 60)
        logger.warning("ğŸ“Š DATA QUALITY ANALYSIS")
        logger.warning("=" * 60)
        
        # 1. ë°ì´í„° íƒ€ì… ì •ë³´
        logger.warning(f"ğŸ“‹ Total columns: {len(df.columns)}")
        logger.warning(f"ğŸ”¢ Numeric columns: {len(numeric_cols)} - {list(numeric_cols)}")
        non_numeric_cols = [col for col in df.columns if col not in numeric_cols]
        logger.warning(f"ğŸ”¤ Non-numeric columns: {len(non_numeric_cols)} - {list(non_numeric_cols)}")
        
        # 2. NaN ê°’ ë¶„ì„
        problematic_cols_nan = df.columns[df.isnull().any()]
        if len(problematic_cols_nan) > 0:
            logger.warning(f"âš ï¸ Columns with NaN values: {len(problematic_cols_nan)}")
            for col in problematic_cols_nan:
                nan_count = df[col].isnull().sum()
                total_count = len(df[col])
                percentage = (nan_count / total_count) * 100
                logger.warning(f"   â€¢ {col}: {nan_count}/{total_count} ({percentage:.1f}%) NaN")
        
        # 3. inf ê°’ ë¶„ì„ (ìˆ«ì ì»¬ëŸ¼ë§Œ)
        problematic_cols_inf = []
        if len(numeric_cols) > 0:
            for col in numeric_cols:
                if np.isinf(df[col]).any():
                    problematic_cols_inf.append(col)
                    inf_count = np.isinf(df[col]).sum()
                    total_count = len(df[col])
                    percentage = (inf_count / total_count) * 100
                    logger.warning(f"   â€¢ {col}: {inf_count}/{total_count} ({percentage:.1f}%) inf values")
        
        if len(problematic_cols_inf) > 0:
            logger.warning(f"âš ï¸ Columns with inf values: {len(problematic_cols_inf)} - {problematic_cols_inf}")
        
        # 4. ê° ì»¬ëŸ¼ì˜ ë°ì´í„° íƒ€ì…ê³¼ ìƒ˜í”Œ ê°’
        logger.warning("ğŸ“ Column details:")
        for col in df.columns:
            dtype = str(df[col].dtype)
            non_null_count = df[col].count()
            sample_values = df[col].dropna().head(3).tolist()
            logger.warning(f"   â€¢ {col}: {dtype} ({non_null_count} non-null) - Sample: {sample_values}")
        
        problematic_cols = list(set(list(problematic_cols_nan) + problematic_cols_inf))
        logger.warning("=" * 60)
        logger.warning(f"ğŸ¯ SUMMARY: {len(problematic_cols)} problematic columns found: {problematic_cols}")
        logger.warning("=" * 60)
        
        # ì¶”ê°€ì ì¸ ì „ì²˜ë¦¬: ë‚¨ì€ inf/nan ê°’ì„ í•´ë‹¹ ì»¬ëŸ¼ì˜ í‰ê· ê°’ìœ¼ë¡œ ëŒ€ì²´ (ìˆ«ì ì»¬ëŸ¼ë§Œ)
        for col in problematic_cols:
            if col in numeric_cols:
                # ìˆ«ì ì»¬ëŸ¼ì— ëŒ€í•´ì„œë§Œ inf ì²˜ë¦¬
                col_mean = df[col].replace([np.inf, -np.inf], np.nan).mean()
                df[col] = df[col].replace([np.inf, -np.inf], np.nan).fillna(col_mean)
            else:
                # ë¹„ìˆ«ì ì»¬ëŸ¼ì— ëŒ€í•´ì„œëŠ” NaNë§Œ ì²˜ë¦¬
                df[col] = df[col].fillna(method='ffill').fillna(method='bfill')
    
    logger.info(f"Final shape after preprocessing: {df.shape}")
    
    # ğŸ”§ ë©”ëª¨ë¦¬ ìºì‹œì— ì €ì¥ (ì„±ê³µì ìœ¼ë¡œ ë¡œë”©ëœ ê²½ìš°)
    if use_cache:
        _dataframe_cache[cache_key] = (df.copy(), current_time)
        logger.info(f"ğŸ’¾ [CACHE_SAVE] Saved DataFrame to cache for {os.path.basename(file_path)} (expires in {_cache_expiry_seconds}s)")
        
        # ë©”ëª¨ë¦¬ ê´€ë¦¬: ì˜¤ë˜ëœ ìºì‹œ ì •ë¦¬
        expired_keys = []
        for key, (cached_df, cache_time) in _dataframe_cache.items():
            if (current_time - cache_time) >= _cache_expiry_seconds:
                expired_keys.append(key)
        
        for key in expired_keys:
            del _dataframe_cache[key]
        
        if expired_keys:
            logger.info(f"ğŸ—‘ï¸ [CACHE_CLEANUP] Removed {len(expired_keys)} expired cache entries")
    
    return df

# ë³€ìˆ˜ ê·¸ë£¹ ì •ì˜
variable_groups = {
    'crude_oil': ['WTI', 'Brent', 'Dubai'],
    'gasoline': ['Gasoline_92RON', 'Gasoline_95RON', 'Europe_M.G_10ppm', 'RBOB (NYMEX)_M1'],
    'naphtha': ['MOPAG', 'MOPS', 'Europe_CIF NWE'],
    'lpg': ['C3_LPG', 'C4_LPG'],
    'product': ['EL_CRF NEA', 'EL_CRF SEA', 'PL_FOB Korea', 'BZ_FOB Korea', 'BZ_FOB SEA', 'BZ_FOB US M1', 'BZ_FOB US M2', 'TL_FOB Korea', 'TL_FOB US M1', 'TL_FOB US M2',
    'MX_FOB Korea', 'PX_FOB Korea', 'SM_FOB Korea', 'RPG Value_FOB PG', 'FO_HSFO 180 CST', 'MTBE_FOB Singapore'],
    'spread': ['biweekly Spread','BZ_H2-TIME SPREAD', 'Brent_WTI', 'MOPJ_MOPAG', 'MOPJ_MOPS', 'Naphtha_Spread', 'MG92_E Nap', 'C3_MOPJ', 'C4_MOPJ', 'Nap_Dubai',
    'MG92_Nap_MOPS', '95R_92R_Asia', 'M1_M2_RBOB', 'RBOB_Brent_m1', 'RBOB_Brent_m2', 'EL_MOPJ', 'PL_MOPJ', 'BZ_MOPJ', 'TL_MOPJ', 'PX_MOPJ', 'HD_EL', 'LD_EL', 'LLD_EL', 'PP_PL',
    'SM_EL+BZ', 'US_FOBK_BZ', 'NAP_HSFO_180', 'MTBE_MOPJ'],
    'economics': ['Dow_Jones', 'Euro', 'Gold'],
    'freight': ['Freight_55_PG', 'Freight_55_Maili', 'Freight_55_Yosu', 'Freight_55_Daes', 'Freight_55_Chiba',
    'Freight_75_PG', 'Freight_75_Maili', 'Freight_75_Yosu', 'Freight_75_Daes', 'Freight_75_Chiba', 'Flat Rate_PG', 'Flat Rate_Maili', 'Flat Rate_Yosu', 'Flat Rate_Daes',
    'Flat Rate_Chiba'],
    'ETF': ['DIG', 'DUG', 'IYE', 'VDE', 'XLE']
}

def load_holidays_from_file(filepath=None):
    """
    CSV ë˜ëŠ” Excel íŒŒì¼ì—ì„œ íœ´ì¼ ëª©ë¡ì„ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜
    
    Args:
        filepath (str): íœ´ì¼ ëª©ë¡ íŒŒì¼ ê²½ë¡œ, Noneì´ë©´ ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©
    
    Returns:
        set: íœ´ì¼ ë‚ ì§œ ì§‘í•© (YYYY-MM-DD í˜•ì‹)
    """
    # ê¸°ë³¸ íŒŒì¼ ê²½ë¡œ - holidays í´ë”ë¡œ ë³€ê²½
    if filepath is None:
        holidays_dir = Path('holidays')
        holidays_dir.mkdir(exist_ok=True)
        filepath = str(holidays_dir / 'holidays.csv')
    
    # íŒŒì¼ í™•ì¥ì í™•ì¸
    _, ext = os.path.splitext(filepath)
    
    # íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ íœ´ì¼ ëª©ë¡ ìƒì„±
    if not os.path.exists(filepath):
        logger.warning(f"Holiday file {filepath} not found. Creating default holiday file.")
        
        # ê¸°ë³¸ 2025ë…„ ì‹±ê°€í´ ê³µíœ´ì¼
        default_holidays = [
            "2025-01-01", "2025-01-29", "2025-01-30", "2025-03-31", "2025-04-18", 
            "2025-05-01", "2025-05-12", "2025-06-07", "2025-08-09", "2025-10-20", 
            "2025-12-25", "2026-01-01"
        ]
        
        # ê¸°ë³¸ íŒŒì¼ ìƒì„±
        df = pd.DataFrame({'date': default_holidays, 'description': ['Singapore Holiday']*len(default_holidays)})
        
        if ext.lower() == '.xlsx':
            df.to_excel(filepath, index=False)
        else:
            df.to_csv(filepath, index=False)
        
        logger.info(f"Created default holiday file at {filepath}")
        return set(default_holidays)
    
    try:
        # íŒŒì¼ ë¡œë“œ - ë³´ì•ˆ ë¬¸ì œë¥¼ ê³ ë ¤í•œ ì•ˆì „í•œ ë¡œë”© ì‚¬ìš©
        if ext.lower() == '.xlsx':
            # Excel íŒŒì¼ì˜ ê²½ìš° xlwings ë³´ì•ˆ ìš°íšŒ ê¸°ëŠ¥ ì‚¬ìš©
            try:
                df = load_data_safe_holidays(filepath)
            except Exception as e:
                logger.warning(f"âš ï¸ [HOLIDAYS] xlwings loading failed, using pandas: {str(e)}")
                df = pd.read_excel(filepath)
        else:
            df = pd.read_csv(filepath)
        
        # 'date' ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸
        if 'date' not in df.columns:
            logger.error(f"Holiday file {filepath} does not have 'date' column")
            return set()
        
        # ë‚ ì§œ í˜•ì‹ í‘œì¤€í™”
        holidays = set()
        for date_str in df['date']:
            try:
                date = pd.to_datetime(date_str)
                holidays.add(date.strftime('%Y-%m-%d'))
            except:
                logger.warning(f"Invalid date format: {date_str}")
        
        logger.info(f"Loaded {len(holidays)} holidays from {filepath}")
        return holidays
        
    except Exception as e:
        logger.error(f"Error loading holiday file: {str(e)}")
        logger.error(traceback.format_exc())
        return set()

# ì „ì—­ ë³€ìˆ˜ë¡œ íœ´ì¼ ì§‘í•© ê´€ë¦¬
holidays = load_holidays_from_file()

def is_holiday(date):
    """ì£¼ì–´ì§„ ë‚ ì§œê°€ íœ´ì¼ì¸ì§€ í™•ì¸í•˜ëŠ” í•¨ìˆ˜"""
    date_str = format_date(date, '%Y-%m-%d')
    return date_str in holidays

# ë°ì´í„°ì—ì„œ í‰ì¼ ë¹ˆ ë‚ ì§œë¥¼ íœ´ì¼ë¡œ ê°ì§€í•˜ëŠ” í•¨ìˆ˜
def detect_missing_weekdays_as_holidays(df, date_column='Date'):
    """
    ë°ì´í„°í”„ë ˆì„ì—ì„œ í‰ì¼(ì›”~ê¸ˆ)ì¸ë° ë°ì´í„°ê°€ ì—†ëŠ” ë‚ ì§œë“¤ì„ íœ´ì¼ë¡œ ê°ì§€í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        df (pd.DataFrame): ë°ì´í„°í”„ë ˆì„
        date_column (str): ë‚ ì§œ ì»¬ëŸ¼ëª…
    
    Returns:
        set: ê°ì§€ëœ íœ´ì¼ ë‚ ì§œ ì§‘í•© (YYYY-MM-DD í˜•ì‹)
    """
    if df.empty or date_column not in df.columns:
        return set()
    
    try:
        # ë‚ ì§œ ì»¬ëŸ¼ì„ datetimeìœ¼ë¡œ ë³€í™˜
        df_dates = pd.to_datetime(df[date_column]).dt.date
        date_set = set(df_dates)
        
        # ë°ì´í„° ë²”ìœ„ì˜ ì²« ë‚ ê³¼ ë§ˆì§€ë§‰ ë‚ 
        start_date = min(df_dates)
        end_date = max(df_dates)
        
        # ì „ì²´ ê¸°ê°„ì˜ ëª¨ë“  í‰ì¼ ìƒì„±
        current_date = start_date
        missing_weekdays = set()
        
        while current_date <= end_date:
            # í‰ì¼ì¸ì§€ í™•ì¸ (ì›”ìš”ì¼=0, ì¼ìš”ì¼=6)
            if current_date.weekday() < 5:  # ì›”~ê¸ˆ
                if current_date not in date_set:
                    missing_weekdays.add(current_date.strftime('%Y-%m-%d'))
            current_date += pd.Timedelta(days=1)
        
        logger.info(f"Detected {len(missing_weekdays)} missing weekdays as potential holidays")
        if missing_weekdays:
            logger.info(f"Missing weekdays sample: {list(missing_weekdays)[:10]}")
        
        return missing_weekdays
        
    except Exception as e:
        logger.error(f"Error detecting missing weekdays: {str(e)}")
        return set()

# íœ´ì¼ ì •ë³´ì™€ ë°ì´í„° ë¹ˆ ë‚ ì§œë¥¼ ê²°í•©í•˜ëŠ” í•¨ìˆ˜
def get_combined_holidays(df=None, filepath=None):
    """
    íœ´ì¼ íŒŒì¼ì˜ íœ´ì¼ê³¼ ë°ì´í„°ì—ì„œ ê°ì§€ëœ íœ´ì¼ì„ ê²°í•©í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        df (pd.DataFrame): ë°ì´í„°í”„ë ˆì„ (ë¹ˆ ë‚ ì§œ ê°ì§€ìš©)
        filepath (str): íœ´ì¼ íŒŒì¼ ê²½ë¡œ
    
    Returns:
        set: ê²°í•©ëœ íœ´ì¼ ë‚ ì§œ ì§‘í•©
    """
    # íœ´ì¼ íŒŒì¼ì—ì„œ íœ´ì¼ ë¡œë“œ
    file_holidays = load_holidays_from_file(filepath)
    
    # ë°ì´í„°ì—ì„œ ë¹ˆ í‰ì¼ ê°ì§€
    data_holidays = set()
    if df is not None:
        data_holidays = detect_missing_weekdays_as_holidays(df)
    
    # ë‘ ì„¸íŠ¸ ê²°í•©
    combined_holidays = file_holidays.union(data_holidays)
    
    logger.info(f"Combined holidays: {len(file_holidays)} from file + {len(data_holidays)} from data = {len(combined_holidays)} total")
    
    return combined_holidays

# íœ´ì¼ ì •ë³´ ì—…ë°ì´íŠ¸ í•¨ìˆ˜
def update_holidays(filepath=None, df=None):
    """íœ´ì¼ ì •ë³´ë¥¼ ì¬ë¡œë“œí•˜ëŠ” í•¨ìˆ˜ (ë°ì´í„° ë¹ˆ ë‚ ì§œ í¬í•¨)"""
    global holidays
    holidays = get_combined_holidays(df, filepath)
    return holidays

def update_holidays_safe(filepath=None, df=None):
    """
    ì•ˆì „í•œ íœ´ì¼ ì •ë³´ ì—…ë°ì´íŠ¸ í•¨ìˆ˜ - xlwings ë³´ì•ˆ ìš°íšŒ ê¸°ëŠ¥ í¬í•¨
    
    Args:
        filepath (str): íœ´ì¼ íŒŒì¼ ê²½ë¡œ
        df (pd.DataFrame): ë°ì´í„° ë¶„ì„ìš© ë°ì´í„°í”„ë ˆì„
    
    Returns:
        set: ì—…ë°ì´íŠ¸ëœ íœ´ì¼ ë‚ ì§œ ì§‘í•©
    """
    global holidays
    
    try:
        # ê¸°ë³¸ ë°©ì‹ìœ¼ë¡œ íœ´ì¼ ë¡œë“œ ì‹œë„
        holidays = get_combined_holidays(df, filepath)
        logger.info(f"âœ… [HOLIDAY_SAFE] Standard holiday loading successful: {len(holidays)} holidays")
        return holidays
        
    except (PermissionError, OSError, pd.errors.ExcelFileError) as e:
        # íŒŒì¼ ì ‘ê·¼ ì˜¤ë¥˜ ì‹œ xlwingsë¡œ ëŒ€ì²´ ì‹œë„ (Excel íŒŒì¼ë§Œ)
        if filepath and filepath.endswith(('.xlsx', '.xls')) and XLWINGS_AVAILABLE:
            logger.warning(f"âš ï¸ [HOLIDAY_BYPASS] Standard holiday loading failed: {str(e)}")
            logger.info("ğŸ”“ [HOLIDAY_BYPASS] Attempting xlwings bypass for holiday file...")
            
            try:
                # xlwingsë¡œ íœ´ì¼ íŒŒì¼ ë¡œë“œ
                file_holidays = load_holidays_from_file_safe(filepath)
                
                # ë°ì´í„°ì—ì„œ ë¹ˆ í‰ì¼ ê°ì§€ (ê¸°ì¡´ ë°©ì‹)
                data_holidays = set()
                if df is not None:
                    data_holidays = detect_missing_weekdays_as_holidays(df)
                
                # ë‘ ì„¸íŠ¸ ê²°í•©
                holidays = file_holidays.union(data_holidays)
                
                logger.info(f"âœ… [HOLIDAY_BYPASS] xlwings holiday loading successful: {len(file_holidays)} from file + {len(data_holidays)} from data = {len(holidays)} total")
                return holidays
                
            except Exception as xlwings_error:
                logger.error(f"âŒ [HOLIDAY_BYPASS] xlwings holiday loading also failed: {str(xlwings_error)}")
                # ê¸°ë³¸ íœ´ì¼ë¡œ í´ë°±
                logger.info("ğŸ”„ [HOLIDAY_FALLBACK] Using default holidays")
                holidays = load_holidays_from_file()  # ê¸°ë³¸ íŒŒì¼ì—ì„œ ë¡œë“œ
                return holidays
        else:
            # xlwingsë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìœ¼ë©´ ê¸°ë³¸ íœ´ì¼ë¡œ í´ë°±
            logger.warning(f"âš ï¸ [HOLIDAY_FALLBACK] Cannot use xlwings, using default holidays: {str(e)}")
            holidays = load_holidays_from_file()  # ê¸°ë³¸ íŒŒì¼ì—ì„œ ë¡œë“œ
            return holidays

def load_holidays_from_file_safe(filepath):
    """
    xlwingsë¥¼ ì‚¬ìš©í•œ ì•ˆì „í•œ íœ´ì¼ íŒŒì¼ ë¡œë”©
    
    Args:
        filepath (str): íœ´ì¼ íŒŒì¼ ê²½ë¡œ
    
    Returns:
        set: íœ´ì¼ ë‚ ì§œ ì§‘í•©
    """
    try:
        # xlwingsë¡œ íœ´ì¼ íŒŒì¼ ë¡œë“œ
        df = load_data_safe_holidays(filepath)
        
        # ë‚ ì§œ í˜•ì‹ í‘œì¤€í™”
        holidays_set = set()
        for date_str in df['date']:
            try:
                date = pd.to_datetime(date_str)
                holidays_set.add(date.strftime('%Y-%m-%d'))
            except:
                logger.warning(f"Invalid date format in xlwings holiday data: {date_str}")
        
        logger.info(f"ğŸ”“ [HOLIDAY_XLWINGS] Loaded {len(holidays_set)} holidays with xlwings")
        return holidays_set
        
    except Exception as e:
        logger.error(f"âŒ [HOLIDAY_XLWINGS] xlwings holiday loading failed: {str(e)}")
        raise e

# TimeSeriesDataset ë° í‰ê°€ ë©”íŠ¸ë¦­ìŠ¤
class TimeSeriesDataset(Dataset):
    def __init__(self, X, y, device, prev_values=None):
        if isinstance(X, torch.Tensor):
            self.X = X
            self.y = y
        else:
            self.X = torch.FloatTensor(X).to(device)
            self.y = torch.FloatTensor(y).to(device)
        
        if prev_values is not None:
            if isinstance(prev_values, torch.Tensor):
                self.prev_values = prev_values
            else:
                self.prev_values = torch.FloatTensor(prev_values).to(device)
        else:
            self.prev_values = None
    
    def __len__(self):
        return len(self.X)
    
    def __getitem__(self, idx):
        if self.prev_values is not None:
            return self.X[idx], self.y[idx], self.prev_values[idx]
        return self.X[idx], self.y[idx]

# ë³µí•© ì†ì‹¤ í•¨ìˆ˜
class DirectionalLoss(nn.Module):
    def __init__(self, alpha=0.7, beta=0.2):
        super().__init__()
        self.alpha = alpha
        self.beta = beta
        self.mse = nn.MSELoss()
    
    def forward(self, pred, target, prev_value=None):
        # ì°¨ì› ë§ì¶”ê¸°
        if len(target.shape) == 1:
            target = target.view(-1, 1)
        if len(pred.shape) == 1:
            pred = pred.view(-1, 1)
        
        # MSE Loss
        mse_loss = self.mse(pred, target)
        
        # Directional Loss (ì°¨ì› í™•ì¸)
        if pred.shape[1] > 1:
            pred_diff = pred[:, 1:] - pred[:, :-1]
            target_diff = target[:, 1:] - target[:, :-1]
            directional_loss = -torch.mean(torch.sign(pred_diff) * torch.sign(target_diff))
        else:
            directional_loss = torch.tensor(0.0).to(pred.device)
        
        # Continuity Loss
        continuity_loss = 0
        if prev_value is not None:
            if len(prev_value.shape) == 1:
                prev_value = prev_value.view(-1, 1)
            continuity_loss = self.mse(pred[:, 0:1], prev_value)
        
        return self.alpha * mse_loss + (1 - self.alpha) * directional_loss + self.beta * continuity_loss

# í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ í´ë˜ìŠ¤ (Purchase_decision_5days.py ë°©ì‹)

class WarmupScheduler:
    def __init__(self, optimizer, warmup_steps, max_lr):
        self.optimizer = optimizer
        self.warmup_steps = warmup_steps
        self.max_lr = max_lr
        self.current_step = 0

    def step(self):
        self.current_step += 1
        # warmup ë‹¨ê³„ ë™ì•ˆ ì„ í˜• ì¦ê°€
        lr = self.max_lr * self.current_step / self.warmup_steps
        # warmup ë‹¨ê³„ë¥¼ ì´ˆê³¼í•˜ë©´ max_lrë¡œ ê³ ì •
        if self.current_step > self.warmup_steps:
            lr = self.max_lr
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = lr

# ê°œì„ ëœ LSTM ì˜ˆì¸¡ ëª¨ë¸
class ImprovedLSTMPredictor(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, dropout=0.2, output_size=23):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        # hidden_sizeë¥¼ 8ì˜ ë°°ìˆ˜ë¡œ ì¡°ì •
        self.adjusted_hidden = (hidden_size // 8) * 8
        if self.adjusted_hidden < 32:
            self.adjusted_hidden = 32
        
        # LSTM dropout ì„¤ì •
        self.lstm_dropout = 0.0 if num_layers == 1 else dropout
        
        # ê³„ì¸µì  LSTM êµ¬ì¡°
        self.lstm_layers = nn.ModuleList([
            nn.LSTM(
                input_size=input_size if i == 0 else self.adjusted_hidden,
                hidden_size=self.adjusted_hidden,
                num_layers=1,
                batch_first=True
            ) for i in range(num_layers)
        ])
        
        # ë“€ì–¼ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜
        self.temporal_attention = nn.MultiheadAttention(
            self.adjusted_hidden,
            num_heads=8,
            dropout=dropout,
            batch_first=True
        )
        
        self.feature_attention = nn.MultiheadAttention(
            self.adjusted_hidden,
            num_heads=4,
            dropout=dropout,
            batch_first=True
        )
        
        # Layer Normalization
        self.layer_norms = nn.ModuleList([
            nn.LayerNorm(self.adjusted_hidden) for _ in range(num_layers)
        ])
        
        self.final_layer_norm = nn.LayerNorm(self.adjusted_hidden)
        
        # Dropout ë ˆì´ì–´
        self.dropout_layer = nn.Dropout(dropout)
        
        # ì´ì „ ê°’ ì •ë³´ë¥¼ ê²°í•©í•˜ê¸° ìœ„í•œ ë ˆì´ì–´
        self.prev_value_encoder = nn.Sequential(
            nn.Linear(1, self.adjusted_hidden // 4),
            nn.ReLU(),
            nn.Linear(self.adjusted_hidden // 4, self.adjusted_hidden)
        )
        
        # ì‹œê³„ì—´ íŠ¹ì„± ì¶”ì¶œì„ ìœ„í•œ ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´
        self.conv_layers = nn.Sequential(
            nn.Conv1d(self.adjusted_hidden, self.adjusted_hidden, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv1d(self.adjusted_hidden, self.adjusted_hidden, kernel_size=3, padding=1),
            nn.ReLU()
        )
        
        # ì¶œë ¥ ë ˆì´ì–´ - ê³„ì¸µì  êµ¬ì¡°
        self.output_layers = nn.ModuleList([
            nn.Linear(self.adjusted_hidden, self.adjusted_hidden // 2),
            nn.Linear(self.adjusted_hidden // 2, self.adjusted_hidden // 4),
            nn.Linear(self.adjusted_hidden // 4, output_size)
        ])
        
        # ì”ì°¨ ì—°ê²°ì„ ìœ„í•œ í”„ë¡œì ì…˜ ë ˆì´ì–´
        self.residual_proj = nn.Linear(self.adjusted_hidden, output_size)
        
    def forward(self, x, prev_value=None, return_attention=False):
        batch_size = x.size(0)
        
        # ê³„ì¸µì  LSTM ì²˜ë¦¬
        lstm_out = x
        skip_connections = []
        
        for i, (lstm, layer_norm) in enumerate(zip(self.lstm_layers, self.layer_norms)):
            lstm_out, _ = lstm(lstm_out)
            lstm_out = layer_norm(lstm_out)
            lstm_out = self.dropout_layer(lstm_out)
            skip_connections.append(lstm_out)
        
        # ì‹œê°„ì  ì–´í…ì…˜
        temporal_context, temporal_weights = self.temporal_attention(lstm_out, lstm_out, lstm_out)
        temporal_context = self.dropout_layer(temporal_context)
        
        # íŠ¹ì§• ì–´í…ì…˜
        # íŠ¹ì§• ì°¨ì›ìœ¼ë¡œ ë³€í™˜ (B, L, H) -> (B, H, L)
        feature_input = lstm_out.transpose(1, 2)
        feature_input = self.conv_layers(feature_input)
        feature_input = feature_input.transpose(1, 2)
        
        feature_context, feature_weights = self.feature_attention(feature_input, feature_input, feature_input)
        feature_context = self.dropout_layer(feature_context)
        
        # ì»¨í…ìŠ¤íŠ¸ ê²°í•©
        combined_context = temporal_context + feature_context
        for skip in skip_connections:
            combined_context = combined_context + skip
        
        combined_context = self.final_layer_norm(combined_context)
        
        # ì´ì „ ê°’ ì •ë³´ ì²˜ë¦¬
        if prev_value is not None:
            prev_value = prev_value.unsqueeze(1) if len(prev_value.shape) == 1 else prev_value
            prev_encoded = self.prev_value_encoder(prev_value)
            combined_context = combined_context + prev_encoded.unsqueeze(1)
        
        # ìµœì¢… íŠ¹ì§• ì¶”ì¶œ (ë§ˆì§€ë§‰ ì‹œí€€ìŠ¤)
        final_features = combined_context[:, -1, :]
        
        # ê³„ì¸µì  ì¶œë ¥ ì²˜ë¦¬
        out = final_features
        residual = self.residual_proj(final_features)
        
        for i, layer in enumerate(self.output_layers):
            out = layer(out)
            if i < len(self.output_layers) - 1:
                out = F.relu(out)
                out = self.dropout_layer(out)
        
        # ì”ì°¨ ì—°ê²° ì¶”ê°€
        out = out + residual
        
        if return_attention:
            attention_weights = {
                'temporal_weights': temporal_weights,
                'feature_weights': feature_weights
            }
            return out, attention_weights
        
        return out
        
    def get_attention_maps(self, x, prev_value=None):
        """ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ë§µì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜"""
        with torch.no_grad():
            # forward ë©”ì„œë“œì— return_attention=True ì „ë‹¬
            _, attention_weights = self.forward(x, prev_value, return_attention=True)
            
            # ì–´í…ì…˜ ê°€ì¤‘ì¹˜ í‰ê·  ê³„ì‚° (multi-head -> single map)
            temporal_weights = attention_weights['temporal_weights'].mean(dim=1)  # í—¤ë“œ í‰ê· 
            feature_weights = attention_weights['feature_weights'].mean(dim=1)    # í—¤ë“œ í‰ê· 
            
            return {
                'temporal_weights': temporal_weights.cpu().numpy(),
                'feature_weights': feature_weights.cpu().numpy()
            }

# VolatileLoss í´ë˜ìŠ¤ ì œê±°ë¨ - ë‹¨ìˆœí™”ë¥¼ ìœ„í•´ DirectionalLossë§Œ ì‚¬ìš©

# VolatileAwareLSTMPredictor í´ë˜ìŠ¤ ì œê±°ë¨ - ë‹¨ìˆœí™”ë¥¼ ìœ„í•´ ImprovedLSTMPredictorë§Œ ì‚¬ìš©

#######################################################################
# ë°˜ì›” ê¸°ê°„ ê´€ë ¨ í•¨ìˆ˜
#######################################################################

# 1. ë°˜ì›” ê¸°ê°„ ê³„ì‚° í•¨ìˆ˜
def get_semimonthly_period(date):
    """
    ë‚ ì§œë¥¼ ë°˜ì›” ê¸°ê°„ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
    - 1ì¼~15ì¼: "YYYY-MM-SM1"
    - 16ì¼~ë§ì¼: "YYYY-MM-SM2"
    """
    year = date.year
    month = date.month
    day = date.day
    
    if day <= 15:
        semimonthly = f"{year}-{month:02d}-SM1"
    else:
        semimonthly = f"{year}-{month:02d}-SM2"
    
    return semimonthly

# 2. íŠ¹ì • ë‚ ì§œ ì´í›„ì˜ ë‹¤ìŒ ë°˜ì›” ê¸°ê°„ ê³„ì‚° í•¨ìˆ˜
def get_next_semimonthly_period(date):
    """
    ì£¼ì–´ì§„ ë‚ ì§œ ì´í›„ì˜ ë‹¤ìŒ ë°˜ì›” ê¸°ê°„ì„ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜
    """
    year = date.year
    month = date.month
    day = date.day
    
    if day <= 15:
        # í˜„ì¬ ìƒë°˜ì›”ì´ë©´ ê°™ì€ ë‹¬ì˜ í•˜ë°˜ì›”
        semimonthly = f"{year}-{month:02d}-SM2"
    else:
        # í˜„ì¬ í•˜ë°˜ì›”ì´ë©´ ë‹¤ìŒ ë‹¬ì˜ ìƒë°˜ì›”
        if month == 12:
            # 12ì›” í•˜ë°˜ì›”ì´ë©´ ë‹¤ìŒ í•´ 1ì›” ìƒë°˜ì›”
            semimonthly = f"{year+1}-01-SM1"
        else:
            semimonthly = f"{year}-{(month+1):02d}-SM1"
    
    return semimonthly

# 3. ë°˜ì›” ê¸°ê°„ì˜ ì‹œì‘ì¼ê³¼ ì¢…ë£Œì¼ ê³„ì‚° í•¨ìˆ˜
def get_semimonthly_date_range(semimonthly_period):
    """
    ë°˜ì›” ê¸°ê°„ ë¬¸ìì—´ì„ ë°›ì•„ ì‹œì‘ì¼ê³¼ ì¢…ë£Œì¼ì„ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜
    
    Parameters:
    -----------
    semimonthly_period : str
        "YYYY-MM-SM1" ë˜ëŠ” "YYYY-MM-SM2" í˜•ì‹ì˜ ë°˜ì›” ê¸°ê°„
    
    Returns:
    --------
    tuple
        (ì‹œì‘ì¼, ì¢…ë£Œì¼) - datetime ê°ì²´
    """
    parts = semimonthly_period.split('-')
    year = int(parts[0])
    month = int(parts[1])
    half = parts[2]
    
    if half == "SM1":
        # ìƒë°˜ì›” (1ì¼~15ì¼)
        start_date = pd.Timestamp(year=year, month=month, day=1)
        end_date = pd.Timestamp(year=year, month=month, day=15)
    else:
        # í•˜ë°˜ì›” (16ì¼~ë§ì¼)
        start_date = pd.Timestamp(year=year, month=month, day=16)
        _, last_day = calendar.monthrange(year, month)
        end_date = pd.Timestamp(year=year, month=month, day=last_day)
    
    return start_date, end_date

# 4. ë‹¤ìŒ ë°˜ì›”ì˜ ëª¨ë“  ë‚ ì§œ ëª©ë¡ ìƒì„± í•¨ìˆ˜
def get_next_semimonthly_dates(reference_date, original_df):
    """
    ì°¸ì¡° ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ë‹¤ìŒ ë°˜ì›” ê¸°ê°„ì— ì†í•˜ëŠ” ëª¨ë“  ì˜ì—…ì¼ ëª©ë¡ì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
    """
    # ë‹¤ìŒ ë°˜ì›” ê¸°ê°„ ê³„ì‚°
    next_period = get_next_semimonthly_period(reference_date)
    
    logger.info(f"Calculating next semimonthly dates from reference: {format_date(reference_date)} â†’ target period: {next_period}")
    
    # ë°˜ì›” ê¸°ê°„ì˜ ì‹œì‘ì¼ê³¼ ì¢…ë£Œì¼ ê³„ì‚°
    start_date, end_date = get_semimonthly_date_range(next_period)
    
    logger.info(f"Target period date range: {format_date(start_date)} ~ {format_date(end_date)}")
    
    # ì´ ê¸°ê°„ì— ì†í•˜ëŠ” ì˜ì—…ì¼(ì›”~ê¸ˆ, íœ´ì¼ ì œì™¸) ì„ íƒ
    business_days = []
    
    # ì›ë³¸ ë°ì´í„°ì—ì„œ ì°¾ê¸°
    future_dates = original_df.index[original_df.index > reference_date]
    for date in future_dates:
        if start_date <= date <= end_date and date.weekday() < 5 and not is_holiday(date):
            business_days.append(date)
    
    # ì›ë³¸ ë°ì´í„°ì— ì—†ëŠ” ê²½ìš°, ë‚ ì§œ ë²”ìœ„ì—ì„œ ì§ì ‘ ìƒì„±
    if len(business_days) == 0:
        logger.info(f"No business days found in original data for period {next_period}. Generating from date range.")
        current_date = start_date
        while current_date <= end_date:
            if current_date.weekday() < 5 and not is_holiday(current_date):
                business_days.append(current_date)
            current_date += pd.Timedelta(days=1)
    
    # ë‚ ì§œê°€ ì—†ê±°ë‚˜ ë¶€ì¡±í•˜ë©´ ì¶”ê°€ ë¡œì§
    min_required_days = 5
    if len(business_days) < min_required_days:
        logger.warning(f"Only {len(business_days)} business days found in period {next_period}. Creating synthetic dates.")
        
        if business_days:
            synthetic_date = business_days[-1] + pd.Timedelta(days=1)
        else:
            synthetic_date = max(reference_date, start_date) + pd.Timedelta(days=1)
        
        while len(business_days) < 15 and synthetic_date <= end_date:
            if synthetic_date.weekday() < 5 and not is_holiday(synthetic_date):
                business_days.append(synthetic_date)
            synthetic_date += pd.Timedelta(days=1)
        
        logger.info(f"Created synthetic dates. Total business days: {len(business_days)} for period {next_period}")
    
    business_days.sort()
    logger.info(f"Final business days for purchase interval: {len(business_days)} days in {next_period}")
    
    return business_days, next_period

# 5. ë‹¤ìŒ N ì˜ì—…ì¼ ê³„ì‚° í•¨ìˆ˜
def get_next_n_business_days(current_date, original_df, n_days=23):
    """
    í˜„ì¬ ë‚ ì§œ ì´í›„ì˜ n_days ì˜ì—…ì¼ì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ - ì›ë³¸ ë°ì´í„°ì— ì—†ëŠ” ë¯¸ë˜ ë‚ ì§œë„ ìƒì„±
    íœ´ì¼(ì£¼ë§ ë° ê³µíœ´ì¼)ì€ ì œì™¸
    """
    # í˜„ì¬ ë‚ ì§œ ì´í›„ì˜ ë°ì´í„°í”„ë ˆì„ì—ì„œ ì˜ì—…ì¼ ì°¾ê¸°
    future_df = original_df[original_df.index > current_date]
    
    # í•„ìš”í•œ ìˆ˜ì˜ ì˜ì—…ì¼ ì„ íƒ
    business_days = []
    
    # ë¨¼ì € ë°ì´í„°í”„ë ˆì„ì— ìˆëŠ” ì˜ì—…ì¼ ì¶”ê°€
    for date in future_df.index:
        if date.weekday() < 5 and not is_holiday(date):  # ì›”~ê¸ˆì´ê³  íœ´ì¼ì´ ì•„ë‹Œ ê²½ìš°ë§Œ ì„ íƒ
            business_days.append(date)
        
        if len(business_days) >= n_days:
            break
    
    # ë°ì´í„°í”„ë ˆì„ì—ì„œ ì¶©ë¶„í•œ ë‚ ì§œë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš° í•©ì„± ë‚ ì§œ ìƒì„±
    if len(business_days) < n_days:
        # ë§ˆì§€ë§‰ ë‚ ì§œ ë˜ëŠ” í˜„ì¬ ë‚ ì§œì—ì„œ ì‹œì‘
        last_date = business_days[-1] if business_days else current_date
        
        # í•„ìš”í•œ ë§Œí¼ ì¶”ê°€ ë‚ ì§œ ìƒì„±
        current = last_date + pd.Timedelta(days=1)
        while len(business_days) < n_days:
            if current.weekday() < 5 and not is_holiday(current):  # ì›”~ê¸ˆì´ê³  íœ´ì¼ì´ ì•„ë‹Œ ê²½ìš°ë§Œ í¬í•¨
                business_days.append(current)
            current += pd.Timedelta(days=1)
    
    logger.info(f"Generated {len(business_days)} business days, excluding holidays")
    return business_days

# 6. êµ¬ê°„ë³„ í‰ê·  ê°€ê²© ê³„ì‚° ë° ì ìˆ˜ ë¶€ì—¬ í•¨ìˆ˜
def calculate_interval_averages_and_scores(predictions, business_days, min_window_size=5):
    """
    ë‹¤ìŒ ë°˜ì›” ê¸°ê°„ì— ëŒ€í•´ ë‹¤ì–‘í•œ í¬ê¸°ì˜ êµ¬ê°„ë³„ í‰ê·  ê°€ê²©ì„ ê³„ì‚°í•˜ê³  ì ìˆ˜ë¥¼ ë¶€ì—¬í•˜ëŠ” í•¨ìˆ˜
    - ë°˜ì›” ì „ì²´ ì˜ì—…ì¼ ìˆ˜ì— ë§ì¶° ìœˆë„ìš° í¬ê¸° ë²”ìœ„ ì¡°ì •
    - global_rank ë°©ì‹: ëª¨ë“  êµ¬ê°„ì„ ë¹„êµí•´ ì „ì—­ì ìœ¼ë¡œ ê°€ì¥ ì €ë ´í•œ êµ¬ê°„ì— ì ìˆ˜ ë¶€ì—¬
    
    Parameters:
    -----------
    predictions : list
        ë‚ ì§œë³„ ì˜ˆì¸¡ ê°€ê²© ì •ë³´ (ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸)
    business_days : list
        ë‹¤ìŒ ë°˜ì›”ì˜ ì˜ì—…ì¼ ëª©ë¡
    min_window_size : int
        ìµœì†Œ ê³ ë ¤í•  ìœˆë„ìš° í¬ê¸° (ê¸°ë³¸ê°’: 3)
    
    Returns:
    -----------
    tuple
        (êµ¬ê°„ë³„ í‰ê·  ê°€ê²© ì •ë³´, êµ¬ê°„ë³„ ì ìˆ˜ ì •ë³´, ë¶„ì„ ì¶”ê°€ ì •ë³´)
    """
    import numpy as np
    
    # ì˜ˆì¸¡ ë°ì´í„°ë¥¼ ë‚ ì§œë³„ë¡œ ì •ë¦¬
    predictions_dict = {pred['Date']: pred['Prediction'] for pred in predictions if pred['Date'] in business_days}
    
    # ë‚ ì§œ ìˆœìœ¼ë¡œ ì •ë ¬ëœ ì˜ì—…ì¼ ëª©ë¡
    sorted_days = sorted(business_days)
    
    # ë‹¤ìŒ ë°˜ì›” ì´ ì˜ì—…ì¼ ìˆ˜ ê³„ì‚°
    total_days = len(sorted_days)
    
    # ìµœì†Œ ìœˆë„ìš° í¬ê¸°ì™€ ìµœëŒ€ ìœˆë„ìš° í¬ê¸° ì„¤ì • (ìµœëŒ€ëŠ” ë°˜ì›” ì „ì²´ ì¼ìˆ˜)
    max_window_size = total_days
    
    # ê³ ë ¤í•  ëª¨ë“  ìœˆë„ìš° í¬ê¸° ë²”ìœ„ ìƒì„±
    window_sizes = range(min_window_size, max_window_size + 1)
    
    print(f"ë‹¤ìŒ ë°˜ì›” ì˜ì—…ì¼: {total_days}ì¼, ê³ ë ¤í•  ìœˆë„ìš° í¬ê¸°: {list(window_sizes)}")
    
    # ê° ìœˆë„ìš° í¬ê¸°ë³„ ê²°ê³¼ ì €ì¥
    interval_averages = {}
    
    # ëª¨ë“  êµ¬ê°„ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
    all_intervals = []
    
    # ê° ìœˆë„ìš° í¬ê¸°ì— ëŒ€í•´ ëª¨ë“  ê°€ëŠ¥í•œ êµ¬ê°„ ê³„ì‚°
    for window_size in window_sizes:
        window_results = []
        
        # ê°€ëŠ¥í•œ ëª¨ë“  ì‹œì‘ì ì— ëŒ€í•´ ìœˆë„ìš° í‰ê·  ê³„ì‚°
        for i in range(len(sorted_days) - window_size + 1):
            interval_days = sorted_days[i:i+window_size]
            
            # ëª¨ë“  ë‚ ì§œì— ì˜ˆì¸¡ ê°€ê²©ì´ ìˆëŠ”ì§€ í™•ì¸
            if all(day in predictions_dict for day in interval_days):
                avg_price = np.mean([predictions_dict[day] for day in interval_days])
                
                interval_info = {
                    'start_date': interval_days[0],
                    'end_date': interval_days[-1],
                    'days': window_size,
                    'avg_price': avg_price,
                    'dates': interval_days.copy()
                }
                
                window_results.append(interval_info)
                all_intervals.append(interval_info)  # ëª¨ë“  êµ¬ê°„ ëª©ë¡ì—ë„ ì¶”ê°€
        
        # í•´ë‹¹ ìœˆë„ìš° í¬ê¸°ì— ëŒ€í•œ ê²°ê³¼ ì €ì¥ (ì°¸ê³ ìš©)
        if window_results:
            # í‰ê·  ê°€ê²© ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
            window_results.sort(key=lambda x: x['avg_price'])
            interval_averages[window_size] = window_results
    
    # êµ¬ê°„ ì ìˆ˜ ê³„ì‚°ì„ ìœ„í•œ ë”•ì…”ë„ˆë¦¬
    interval_scores = {}
    
    # Global Rank ì „ëµ: ëª¨ë“  êµ¬ê°„ì„ í†µí•©í•˜ì—¬ ê°€ê²© ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
    all_intervals.sort(key=lambda x: x['avg_price'])
    
    # ìƒìœ„ 3ê°œ êµ¬ê°„ì—ë§Œ ì ìˆ˜ ë¶€ì—¬ (ì „ì²´ ì¤‘ì—ì„œ)
    for i, interval in enumerate(all_intervals[:min(3, len(all_intervals))]):
        score = 3 - i  # 1ë“±: 3ì , 2ë“±: 2ì , 3ë“±: 1ì 
        
        # êµ¬ê°„ ì‹ë³„ì„ ìœ„í•œ í‚¤ ìƒì„± (ë¬¸ìì—´ í‚¤ë¡œ ë³€ê²½)
        interval_key = f"{format_date(interval['start_date'])}-{format_date(interval['end_date'])}"
        
        # ì ìˆ˜ ì •ë³´ ì €ì¥
        interval_scores[interval_key] = {
            'start_date': format_date(interval['start_date']),  # í˜•ì‹ ì ìš©
            'end_date': format_date(interval['end_date']),      # í˜•ì‹ ì ìš©
            'days': interval['days'],
            'avg_price': interval['avg_price'],
            'dates': [format_date(d) for d in interval['dates']],  # ë‚ ì§œ ëª©ë¡ë„ í˜•ì‹ ì ìš©
            'score': score,
            'rank': i + 1
        }
    
    # ë¶„ì„ ì •ë³´ ì¶”ê°€
    analysis_info = {
        'total_days': total_days,
        'window_sizes': list(window_sizes),
        'total_intervals': len(all_intervals),
        'min_avg_price': min([interval['avg_price'] for interval in all_intervals]) if all_intervals else None,
        'max_avg_price': max([interval['avg_price'] for interval in all_intervals]) if all_intervals else None
    }
    
    # ê²°ê³¼ ì¶œë ¥ (ì°¸ê³ ìš©)
    if interval_scores:
        top_interval = max(interval_scores.values(), key=lambda x: x['score'])
        print(f"\nìµœê³  ì ìˆ˜ êµ¬ê°„: {top_interval['days']}ì¼ êµ¬ê°„ ({format_date(top_interval['start_date'])} ~ {format_date(top_interval['end_date'])})")
        print(f"ì ìˆ˜: {top_interval['score']}, ìˆœìœ„: {top_interval['rank']}, í‰ê· ê°€: {top_interval['avg_price']:.2f}")
    
    return interval_averages, interval_scores, analysis_info

# 7. ë‘ êµ¬ë§¤ ë°©ë²•ì˜ ê²°ê³¼ ë¹„êµ í•¨ìˆ˜
def decide_purchase_interval(interval_scores):
    """
    ì ìˆ˜ê°€ ë¶€ì—¬ëœ êµ¬ê°„ë“¤ ì¤‘ì—ì„œ ìµœì¢… êµ¬ë§¤ êµ¬ê°„ì„ ê²°ì •í•˜ëŠ” í•¨ìˆ˜
    - ì ìˆ˜ê°€ ê°€ì¥ ë†’ì€ êµ¬ê°„ ì„ íƒ
    - ë™ì ì¸ ê²½ìš° í‰ê·  ê°€ê²©ì´ ë” ë‚®ì€ êµ¬ê°„ ì„ íƒ
    
    Parameters:
    -----------
    interval_scores : dict
        êµ¬ê°„ë³„ ì ìˆ˜ ì •ë³´
    
    Returns:
    -----------
    dict
        ìµœì¢… ì„ íƒëœ êµ¬ë§¤ êµ¬ê°„ ì •ë³´
    """
    if not interval_scores:
        return None
    
    # ì ìˆ˜ê°€ ê°€ì¥ ë†’ì€ êµ¬ê°„ ì„ íƒ
    max_score = max(interval['score'] for interval in interval_scores.values())
    
    # ìµœê³  ì ìˆ˜ë¥¼ ê°€ì§„ ëª¨ë“  êµ¬ê°„ ì°¾ê¸°
    top_intervals = [interval for interval in interval_scores.values() 
                    if interval['score'] == max_score]
    
    # ë™ì ì´ ìˆëŠ” ê²½ìš°, í‰ê·  ê°€ê²©ì´ ë” ë‚®ì€ êµ¬ê°„ ì„ íƒ
    if len(top_intervals) > 1:
        best_interval = min(top_intervals, key=lambda x: x['avg_price'])
        best_interval['selection_reason'] = "ìµœê³  ì ìˆ˜ ì¤‘ ìµœì € í‰ê· ê°€ êµ¬ê°„"
    else:
        best_interval = top_intervals[0]
        best_interval['selection_reason'] = "ìµœê³  ì ìˆ˜ êµ¬ê°„"
    
    return best_interval

#######################################################################
# íŠ¹ì„± ì„ íƒ í•¨ìˆ˜
#######################################################################

def calculate_group_vif(df, variables):
    """ê·¸ë£¹ ë‚´ ë³€ìˆ˜ë“¤ì˜ VIF ê³„ì‚°"""
    # ë³€ìˆ˜ê°€ í•œ ê°œ ì´í•˜ë©´ VIF ê³„ì‚° ë¶ˆê°€
    if len(variables) <= 1:
        return pd.DataFrame({
            "Feature": variables,
            "VIF": [1.0] * len(variables)
        })
    
    # ëª¨ë“  ë³€ìˆ˜ê°€ ë°ì´í„°í”„ë ˆì„ì— ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    available_vars = [var for var in variables if var in df.columns]
    if len(available_vars) <= 1:
        return pd.DataFrame({
            "Feature": available_vars,
            "VIF": [1.0] * len(available_vars)
        })
    
    try:
        from statsmodels.stats.outliers_influence import variance_inflation_factor
        
        vif_data = pd.DataFrame()
        vif_data["Feature"] = available_vars
        vif_data["VIF"] = [variance_inflation_factor(df[available_vars].values, i) 
                          for i in range(len(available_vars))]
        return vif_data.sort_values('VIF', ascending=False)
    except Exception as e:
        logger.error(f"Error calculating VIF: {str(e)}")
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
        return pd.DataFrame({
            "Feature": available_vars,
            "VIF": [float('nan')] * len(available_vars)
        })

def analyze_group_correlations(df, variable_groups, target_col='MOPJ'):
    """ê·¸ë£¹ë³„ ìƒê´€ê´€ê³„ ë¶„ì„"""
    logger.info("Analyzing correlations for each group:")
    group_correlations = {}
    
    for group_name, variables in variable_groups.items():
        # ê° ê·¸ë£¹ì˜ ë³€ìˆ˜ë“¤ê³¼ íƒ€ê²Ÿ ë³€ìˆ˜ì˜ ìƒê´€ê´€ê³„ ê³„ì‚°
        # í•´ë‹¹ ê·¸ë£¹ì˜ ë³€ìˆ˜ë“¤ì´ ë°ì´í„°í”„ë ˆì„ì— ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        available_vars = [var for var in variables if var in df.columns]
        if not available_vars:
            logger.warning(f"Warning: No variables from {group_name} group found in dataframe")
            continue
            
        if target_col not in df.columns:
            logger.warning(f"Warning: Target column {target_col} not found in dataframe")
            continue
            
        correlations = df[available_vars].corrwith(df[target_col]).abs().sort_values(ascending=False)
        group_correlations[group_name] = correlations
        
        logger.info(f"\n{group_name} group correlations with {target_col}:")
        logger.info(str(correlations))
    
    return group_correlations

def select_features_from_groups(df, variable_groups, target_col='MOPJ', vif_threshold=50.0, corr_threshold=0.8):
    """ê° ê·¸ë£¹ì—ì„œ ëŒ€í‘œ ë³€ìˆ˜ ì„ íƒ"""
    selected_features = []
    selection_process = {}
    
    logger.info(f"\nCorrelation threshold: {corr_threshold}")
    
    for group_name, variables in variable_groups.items():
        logger.info(f"\nProcessing {group_name} group:")
        
        # í•´ë‹¹ ê·¸ë£¹ì˜ ë³€ìˆ˜ë“¤ì´ dfì— ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        available_vars = [var for var in variables if var in df.columns]
        if not available_vars:
            logger.warning(f"Warning: No variables from {group_name} group found in dataframe")
            continue
            
        if target_col not in df.columns:
            logger.warning(f"Warning: Target column {target_col} not found in dataframe")
            continue
        
        # ê·¸ë£¹ ë‚´ ìƒê´€ê´€ê³„ ê³„ì‚°
        correlations = df[available_vars].corrwith(df[target_col]).abs().sort_values(ascending=False)
        logger.info(f"\nCorrelations with {target_col}:")
        logger.info(str(correlations))
        
        # ìƒê´€ê´€ê³„ê°€ ì„ê³„ê°’ ì´ìƒì¸ ë³€ìˆ˜ë§Œ í•„í„°ë§
        high_corr_vars = correlations[correlations >= corr_threshold].index.tolist()
        
        if not high_corr_vars:
            logger.warning(f"Warning: No variables in {group_name} group meet the correlation threshold of {corr_threshold}")
            continue
        
        # ìƒê´€ê´€ê³„ ì„ê³„ê°’ì„ ë§Œì¡±í•˜ëŠ” ë³€ìˆ˜ë“¤ì— ëŒ€í•´ VIF ê³„ì‚°
        if len(high_corr_vars) > 1:
            vif_data = calculate_group_vif(df[high_corr_vars], high_corr_vars)
            logger.info(f"\nVIF values for {group_name} group (high correlation vars only):")
            logger.info(str(vif_data))
            
            # VIF ê¸°ì¤€ ì ìš©í•˜ì—¬ ë‹¤ì¤‘ê³µì„ ì„± ë‚®ì€ ë³€ìˆ˜ ì„ íƒ
            low_vif_vars = vif_data[vif_data['VIF'] < vif_threshold]['Feature'].tolist()
            
            if low_vif_vars:
                # ë‚®ì€ VIF ë³€ìˆ˜ë“¤ ì¤‘ ìƒê´€ê´€ê³„ê°€ ê°€ì¥ ë†’ì€ ë³€ìˆ˜ ì„ íƒ
                for var in correlations.index:
                    if var in low_vif_vars:
                        selected_var = var
                        break
                else:
                    selected_var = high_corr_vars[0]
            else:
                selected_var = high_corr_vars[0]
        else:
            selected_var = high_corr_vars[0]
            vif_data = pd.DataFrame({"Feature": [selected_var], "VIF": [1.0]})
        
        # ì„ íƒëœ ë³€ìˆ˜ê°€ ìƒê´€ê´€ê³„ ì„ê³„ê°’ì„ ë§Œì¡±í•˜ëŠ”ì§€ í™•ì¸ (ì•ˆì „ì¥ì¹˜)
        if correlations[selected_var] >= corr_threshold:
            selected_features.append(selected_var)
            
            selection_process[group_name] = {
                'selected_variable': selected_var,
                'correlation': correlations[selected_var],
                'all_correlations': correlations.to_dict(),
                'vif_data': vif_data.to_dict() if not vif_data.empty else {},
                'high_corr_vars': high_corr_vars
            }
            
            logger.info(f"\nSelected variable from {group_name}: {selected_var} (corr: {correlations[selected_var]:.4f})")
        else:
            logger.info(f"\nNo variable selected from {group_name}: correlation threshold not met")
    
    # ìƒê´€ê´€ê³„ ê¸°ì¤€ ì¬í™•ì¸ (ìµœì¢… ì•ˆì „ì¥ì¹˜)
    final_features = []
    for feature in selected_features:
        corr = abs(df[feature].corr(df[target_col]))
        if corr >= corr_threshold:
            final_features.append(feature)
            logger.info(f"Final selection: {feature} (corr: {corr:.4f})")
        else:
            logger.info(f"Excluded: {feature} (corr: {corr:.4f}) - below threshold")
    
    # íƒ€ê²Ÿ ì»¬ëŸ¼ì´ í¬í•¨ë˜ì–´ ìˆì§€ ì•Šìœ¼ë©´ ì¶”ê°€
    if target_col not in final_features:
        final_features.append(target_col)
        logger.info(f"Added target column: {target_col}")
    
    # ìµœì†Œ íŠ¹ì„± ìˆ˜ í™•ì¸
    if len(final_features) < 3:
        logger.warning(f"Selected features ({len(final_features)}) < 3, lowering threshold to 0.5")
        return select_features_from_groups(df, variable_groups, target_col, vif_threshold, 0.5)
    
    return final_features, selection_process

def find_compatible_hyperparameters(current_file_path, current_period):
    """
    í˜„ì¬ íŒŒì¼ì´ ê¸°ì¡´ íŒŒì¼ì˜ í™•ì¥ì¸ ê²½ìš°, ê¸°ì¡´ íŒŒì¼ì˜ í˜¸í™˜ ê°€ëŠ¥í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ëŠ” í•¨ìˆ˜
    
    Parameters:
    -----------
    current_file_path : str
        í˜„ì¬ íŒŒì¼ ê²½ë¡œ
    current_period : str
        í˜„ì¬ ì˜ˆì¸¡ ê¸°ê°„
        
    Returns:
    --------
    dict or None: {
        'hyperparams': dict,
        'source_file': str,
        'extension_info': dict
    } ë˜ëŠ” None (í˜¸í™˜ ê°€ëŠ¥í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ê°€ ì—†ì„ ê²½ìš°)
    """
    try:
        # uploads í´ë”ì˜ ë‹¤ë¥¸ íŒŒì¼ë“¤ì„ í™•ì¸ (ğŸ”§ ìˆ˜ì •: xlsx íŒŒì¼ë„ í¬í•¨)
        upload_dir = Path(UPLOAD_FOLDER)
        existing_files = [f for f in upload_dir.glob('*.xlsx') if str(f) != current_file_path]
        logger.info(f"ğŸ” [HYPERPARAMS_SEARCH] íƒìƒ‰í•  ê¸°ì¡´ íŒŒì¼ ìˆ˜: {len(existing_files)}")
        for i, file in enumerate(existing_files):
            logger.info(f"    {i+1}. {file.name}")
        
        for existing_file in existing_files:
            try:
                # ğŸ”§ ìˆ˜ì •: í™•ì¥ ê´€ê³„ í™•ì¸ + ë‹¨ìˆœ íŒŒì¼ëª… ìœ ì‚¬ì„± í™•ì¸
                extension_result = check_data_extension(str(existing_file), current_file_path)
                is_extension = extension_result.get('is_extension', False)
                
                # ğŸ“ í™•ì¥ ê´€ê³„ê°€ ì¸ì‹ë˜ì§€ ì•ŠëŠ” ê²½ìš° íŒŒì¼ëª… ìœ ì‚¬ì„±ìœ¼ë¡œ ëŒ€ì²´ í™•ì¸
                if not is_extension:
                    existing_name = existing_file.stem.lower()
                    current_name = Path(current_file_path).stem.lower()
                    # ê¸°ë³¸ ì´ë¦„ì´ ê°™ê±°ë‚˜ í•˜ë‚˜ê°€ ë‹¤ë¥¸ í•˜ë‚˜ë¥¼ í¬í•¨í•˜ëŠ” ê²½ìš°
                    if (existing_name in current_name or current_name in existing_name or 
                        existing_name.replace('_', '') == current_name.replace('_', '')):
                        is_extension = True
                        logger.info(f"ğŸ” [HYPERPARAMS_SEARCH] íŒŒì¼ëª… ìœ ì‚¬ì„±ìœ¼ë¡œ í™•ì¥ ê´€ê³„ ì¸ì •: {existing_file.name} -> {Path(current_file_path).name}")
                
                if is_extension:
                    if extension_result.get('is_extension', False):
                        logger.info(f"ğŸ” [HYPERPARAMS_SEARCH] í™•ì¥ ê´€ê³„ ë°œê²¬: {existing_file.name} -> {Path(current_file_path).name}")
                        logger.info(f"    ğŸ“ˆ Extension type: {extension_result.get('validation_details', {}).get('extension_type', 'Unknown')}")
                        logger.info(f"    â• New rows: {extension_result.get('new_rows_count', 0)}")
                    else:
                        logger.info(f"ğŸ” [HYPERPARAMS_SEARCH] íŒŒì¼ëª… ìœ ì‚¬ì„± ê¸°ë°˜ í˜¸í™˜ì„± ì¸ì •: {existing_file.name} -> {Path(current_file_path).name}")
                    
                    # ê¸°ì¡´ íŒŒì¼ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìºì‹œ í™•ì¸
                    existing_cache_dirs = get_file_cache_dirs(str(existing_file))
                    existing_models_dir = existing_cache_dirs['models']
                    
                    if os.path.exists(existing_models_dir):
                        # í•´ë‹¹ ê¸°ê°„ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŒŒì¼ ì°¾ê¸°
                        hyperparams_pattern = f"hyperparams_kfold_{current_period.replace('-', '_')}.json"
                        hyperparams_file = os.path.join(existing_models_dir, hyperparams_pattern)
                        
                        if os.path.exists(hyperparams_file):
                            try:
                                with open(hyperparams_file, 'r') as f:
                                    hyperparams = json.load(f)
                                
                                logger.info(f"âœ… [HYPERPARAMS_SEARCH] ê¸°ì¡´ íŒŒì¼ì—ì„œ í˜¸í™˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë°œê²¬!")
                                logger.info(f"    ğŸ“ Source file: {existing_file.name}")
                                logger.info(f"    ğŸ“Š Hyperparams file: {hyperparams_pattern}")
                                
                                return {
                                    'hyperparams': hyperparams,
                                    'source_file': str(existing_file),
                                    'extension_info': extension_result,
                                    'period': current_period
                                }
                                
                            except Exception as e:
                                logger.warning(f"ê¸°ì¡´ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ ({existing_file.name}): {str(e)}")
                        else:
                            # âŒ ì‚­ì œëœ ë¶€ë¶„: ë‹¤ë¥¸ ê¸°ê°„ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ëŒ€ì²´ë¡œ ì‚¬ìš©í•˜ëŠ” ë¡œì§ ì œê±°
                            logger.info(f"ğŸ” [HYPERPARAMS_SEARCH] {current_period} ê¸°ê°„ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œìš´ ìµœì í™”ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
                    
            except Exception as e:
                logger.warning(f"íŒŒì¼ í™•ì¥ ê´€ê³„ í™•ì¸ ì‹¤íŒ¨ ({existing_file.name}): {str(e)}")
                continue
        
        logger.info(f"âŒ [HYPERPARAMS_SEARCH] í˜¸í™˜ ê°€ëŠ¥í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return None
        
    except Exception as e:
        logger.error(f"í•˜ì´í¼íŒŒë¼ë¯¸í„° í˜¸í™˜ì„± íƒìƒ‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None

def optimize_hyperparameters_semimonthly_kfold(train_data, input_size, target_col_idx, device, current_period, file_path=None, n_trials=30, k_folds=10, use_cache=True):
    """
    ì‹œê³„ì—´ K-fold êµì°¨ ê²€ì¦ì„ ì‚¬ìš©í•˜ì—¬ ë°˜ì›”ë³„ ë°ì´í„°ì— ëŒ€í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” (Purchase_decision_5days.py ë°©ì‹)
    """
    # ì¼ê´€ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”ë¥¼ ìœ„í•œ ì‹œë“œ ê³ ì •
    set_seed()
    
    logger.info(f"\n===== {current_period} í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œì‘ (ì‹œê³„ì—´ {k_folds}-fold êµì°¨ ê²€ì¦) =====")
    
    # ğŸ”§ í™•ì¥ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìºì‹œ ë¡œì§ - ê¸°ì¡´ íŒŒì¼ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë„ íƒìƒ‰
    file_cache_dir = get_file_cache_dirs(file_path)['models']
    cache_file = os.path.join(file_cache_dir, f"hyperparams_kfold_{current_period.replace('-', '_')}.json")
    logger.info(f"ğŸ“ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìºì‹œ íŒŒì¼: {cache_file}")
    
    # models ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(file_cache_dir, exist_ok=True)
    
    # ğŸ” 1ë‹¨ê³„: í•˜ì´í¼íŒŒë¼ë¯¸í„° ìºì‹œ í™•ì¸
    if use_cache:
        if os.path.exists(cache_file):
            try:
                with open(cache_file, 'r') as f:
                    cached_params = json.load(f)
                logger.info(f"âœ… [{current_period}] í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¡œë“œ ì™„ë£Œ")
                return cached_params
            except Exception as e:
                logger.error(f"ìºì‹œ íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜: {str(e)}")
    
    # ğŸ” 2ë‹¨ê³„: ë°ì´í„° í™•ì¥ ì‹œ ê¸°ì¡´ íŒŒì¼ì˜ ë™ì¼ ê¸°ê°„ í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰
    if use_cache:
        logger.info(f"ğŸ” [{current_period}] í˜„ì¬ íŒŒì¼ì— ìºì‹œê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ì¡´ íŒŒì¼ì—ì„œ ë™ì¼ ê¸°ê°„ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ íƒìƒ‰í•©ë‹ˆë‹¤...")
        compatible_hyperparams = find_compatible_hyperparameters(file_path, current_period)
        if compatible_hyperparams:
            logger.info(f"âœ… [{current_period}] ë™ì¼ ê¸°ê°„ì˜ í˜¸í™˜ ê°€ëŠ¥í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤!")
            logger.info(f"    ğŸ“ Source: {compatible_hyperparams['source_file']}")
            logger.info(f"    ğŸ“Š Extension info: {compatible_hyperparams['extension_info']}")
            
            # ğŸ”§ ìˆ˜ì •: ìºì‹œ ì €ì¥ì— ì‹¤íŒ¨í•´ë„ ê¸°ì¡´ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë°˜í™˜
            try:
                with open(cache_file, 'w') as f:
                    json.dump(compatible_hyperparams['hyperparams'], f, indent=2)
                logger.info(f"ğŸ’¾ [{current_period}] í˜¸í™˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ í˜„ì¬ íŒŒì¼ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
            except Exception as e:
                logger.warning(f"âš ï¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì €ì¥ ì‹¤íŒ¨, í•˜ì§€ë§Œ ê¸°ì¡´ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤: {str(e)}")
            
            # ğŸ”‘ í•µì‹¬: ì €ì¥ ì„±ê³µ/ì‹¤íŒ¨ ì—¬ë¶€ì™€ ê´€ê³„ì—†ì´ ê¸°ì¡´ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë°˜í™˜
            logger.info(f"ğŸš€ [{current_period}] ê¸°ì¡´ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì í™”ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return compatible_hyperparams['hyperparams']
                
        logger.info(f"ğŸ†• [{current_period}] ë™ì¼ ê¸°ê°„ì˜ ê¸°ì¡´ í•˜ì´í¼íŒŒë¼ë¯¸í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œìš´ ìµœì í™”ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.")
    
            # ê¸°ë³¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì •ì˜ (ìµœì í™” ì‹¤íŒ¨ ì‹œ ì‚¬ìš©)
    default_params = {
        'sequence_length': 46,
        'hidden_size': 224,
        'num_layers': 6,
        'dropout': 0.318369281841675,
        'batch_size': 49,
        'learning_rate': 0.0009452489017042499,
        'num_epochs': 72,
        'loss_alpha': 0.7,  # DirectionalLoss alpha
        'loss_beta': 0.2,   # DirectionalLoss beta
        'patience': 14
    }
    
    # ë°ì´í„° ê¸¸ì´ í™•ì¸ - ì¶©ë¶„í•˜ì§€ ì•Šìœ¼ë©´ ë°”ë¡œ ê¸°ë³¸ê°’ ë°˜í™˜
    MIN_DATA_SIZE = 100
    if len(train_data) < MIN_DATA_SIZE:
        logger.warning(f"í›ˆë ¨ ë°ì´í„°ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤ ({len(train_data)} ë°ì´í„° í¬ì¸íŠ¸ < {MIN_DATA_SIZE}). ê¸°ë³¸ íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return default_params
    
    # K-fold ë¶„í•  ë¡œì§
    predict_window = 23  # ì˜ˆì¸¡ ìœˆë„ìš° í¬ê¸°
    min_fold_size = 20 + predict_window + 5  # ìµœì†Œ ì‹œí€€ìŠ¤ ê¸¸ì´ + ì˜ˆì¸¡ ìœˆë„ìš° + ì—¬ìœ 
    max_possible_folds = len(train_data) // min_fold_size
    
    if max_possible_folds < 2:
        logger.warning(f"ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•Šì•„ k-foldë¥¼ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (ê°€ëŠ¥í•œ fold: {max_possible_folds} < 2). ê¸°ë³¸ íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return default_params
    
    # ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ fold ìˆ˜ ì¡°ì •
    k_folds = min(k_folds, max_possible_folds)
    fold_size = len(train_data) // (k_folds + 1)  # +1ì€ ì˜ˆì¸¡ ìœˆë„ìš°ë¥¼ ìœ„í•œ ì¶”ê°€ ë¶€ë¶„

    logger.info(f"ë°ì´í„° í¬ê¸°: {len(train_data)}, Fold ìˆ˜: {k_folds}, ê° Fold í¬ê¸°: {fold_size}")

    # fold ë¶„í• ì„ ìœ„í•œ ì¸ë±ìŠ¤ ìƒì„±
    folds = []
    for i in range(k_folds):
        test_start = i * fold_size
        test_end = (i + 1) * fold_size
        
        train_indices = list(range(0, test_start)) + list(range(test_end, len(train_data)))
        test_indices = list(range(test_start, test_end))
        
        folds.append((train_indices, test_indices))
    
    # Optuna ëª©ì  í•¨ìˆ˜ ì •ì˜
    def objective(trial):
        # ì¼ê´€ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”ë¥¼ ìœ„í•œ ì‹œë“œ ê³ ì •
        set_seed(SEED + trial.number)  # trialë§ˆë‹¤ ë‹¤ë¥¸ ì‹œë“œë¡œ ë‹¤ì–‘ì„± ë³´ì¥í•˜ë©´ì„œë„ ì¬í˜„ ê°€ëŠ¥
        
        # í•˜ì´í¼íŒŒë¼ë¯¸í„° ë²”ìœ„ ìˆ˜ì • - ì‹œí€€ìŠ¤ ê¸¸ì´ ìµœëŒ€ê°’ ì œí•œ
        max_seq_length = min(fold_size - predict_window - 5, 60)
        
        # ìµœì†Œ ì‹œí€€ìŠ¤ ê¸¸ì´ë„ ì œí•œ
        min_seq_length = min(10, max_seq_length)
        
        if max_seq_length <= min_seq_length:
            logger.warning(f"ì‹œí€€ìŠ¤ ê¸¸ì´ ë²”ìœ„ê°€ ë„ˆë¬´ ì œí•œì ì…ë‹ˆë‹¤ (min={min_seq_length}, max={max_seq_length}). í•´ë‹¹ trial ê±´ë„ˆë›°ê¸°.")
            return float('inf')
        
        params = {
            'sequence_length': trial.suggest_int('sequence_length', min_seq_length, max_seq_length),
            'hidden_size': trial.suggest_int('hidden_size', 32, 256, step=8),
            'num_layers': trial.suggest_int('num_layers', 2, 6),
            'dropout': trial.suggest_float('dropout', 0.1, 0.5),
            'batch_size': trial.suggest_int('batch_size', 16, min(128, fold_size)),
            'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True),
            'num_epochs': trial.suggest_int('num_epochs', 50, 200),
            'patience': trial.suggest_int('patience', 10, 30),
            'loss_alpha': trial.suggest_float('loss_alpha', 0.5, 0.9),
            'loss_beta': trial.suggest_float('loss_beta', 0.1, 0.3)
        }
        
        # loss_gamma ì œê±°ë¨ - ë‹¨ìˆœí™”ë¥¼ ìœ„í•´ DirectionalLossë§Œ ì‚¬ìš©
        
        # K-fold êµì°¨ ê²€ì¦
        fold_losses = []
        valid_fold_count = 0
        
        for fold_idx, (train_indices, test_indices) in enumerate(folds):
            try:
                # ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ fold í¬ê¸°ë³´ë‹¤ í¬ë©´ ê±´ë„ˆë›°ê¸°
                if params['sequence_length'] >= len(test_indices):
                    logger.warning(f"Fold {fold_idx+1}: ì‹œí€€ìŠ¤ ê¸¸ì´({params['sequence_length']})ê°€ í…ŒìŠ¤íŠ¸ ë°ì´í„°({len(test_indices)})ë³´ë‹¤ í½ë‹ˆë‹¤.")
                    continue
                
                # foldë³„ í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
                fold_train_data = train_data[train_indices]
                fold_test_data = train_data[test_indices]
                
                # ë°ì´í„° ì¤€ë¹„
                X_train, y_train, prev_train, X_val, y_val, prev_val = prepare_data(
                    fold_train_data, fold_test_data, params['sequence_length'],
                    predict_window, target_col_idx, augment=False
                )
                
                # ë°ì´í„°ê°€ ì¶©ë¶„í•œì§€ í™•ì¸
                if len(X_train) < params['batch_size'] or len(X_val) < 1:
                    logger.warning(f"Fold {fold_idx+1}: ë°ì´í„° ë¶ˆì¶©ë¶„ (í›ˆë ¨: {len(X_train)}, ê²€ì¦: {len(X_val)})")
                    continue
                
                # ë°ì´í„°ì…‹ ë° ë¡œë” ìƒì„± (CPUì—ì„œ ìƒì„±, í•™ìŠµ ì‹œ GPUë¡œ ì´ë™)
                train_dataset = TimeSeriesDataset(X_train, y_train, torch.device('cpu'), prev_train)
                batch_size = min(params['batch_size'], len(X_train))
                train_loader = DataLoader(
                    train_dataset,
                    batch_size=batch_size,
                    shuffle=True,
                    worker_init_fn=seed_worker,
                    generator=g
                )
                
                val_dataset = TimeSeriesDataset(X_val, y_val, torch.device('cpu'), prev_val)
                val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)
                
                # ëª¨ë¸ ìƒì„±
                model = ImprovedLSTMPredictor(
                    input_size=input_size,
                    hidden_size=params['hidden_size'],
                    num_layers=params['num_layers'],
                    dropout=params['dropout'],
                    output_size=predict_window
                ).to(device)

                # ì†ì‹¤ í•¨ìˆ˜ ìƒì„±
                criterion = DirectionalLoss(
                    alpha=params['loss_alpha'],
                    beta=params['loss_beta']
                )

                optimizer = torch.optim.Adam(model.parameters(), lr=params['learning_rate'])
                
                # ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
                scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
                    optimizer, mode='min', factor=0.5,
                    patience=params['patience']//2
                )

                # best_val_loss ë³€ìˆ˜ ëª…ì‹œì  ì •ì˜
                best_val_loss = float('inf')
                patience_counter = 0
                
                for epoch in range(params['num_epochs']):
                    # í•™ìŠµ
                    model.train()
                    train_loss = 0
                    for X_batch, y_batch, prev_batch in train_loader:
                        optimizer.zero_grad()
                        
                        # ëª¨ë¸ê³¼ ê°™ì€ ë””ë°”ì´ìŠ¤ë¡œ ë°ì´í„° ì´ë™
                        X_batch = X_batch.to(device, non_blocking=device.type=='cuda')
                        y_batch = y_batch.to(device, non_blocking=device.type=='cuda')
                        prev_batch = prev_batch.to(device, non_blocking=device.type=='cuda')
                        
                        # ëª¨ë¸ ì˜ˆì¸¡ ë° ì†ì‹¤ ê³„ì‚°
                        y_pred = model(X_batch, prev_batch)
                        loss = criterion(y_pred, y_batch, prev_batch)
                        
                        loss.backward()
                        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                        optimizer.step()
                        train_loss += loss.item()
                    
                    # ê²€ì¦
                    model.eval()
                    val_loss = 0
                    
                    with torch.no_grad():
                        for X_batch, y_batch, prev_batch in val_loader:
                            # ëª¨ë¸ê³¼ ê°™ì€ ë””ë°”ì´ìŠ¤ë¡œ ë°ì´í„° ì´ë™
                            X_batch = X_batch.to(device, non_blocking=device.type=='cuda')
                            y_batch = y_batch.to(device, non_blocking=device.type=='cuda')
                            prev_batch = prev_batch.to(device, non_blocking=device.type=='cuda')
                            
                            # ëª¨ë¸ ì˜ˆì¸¡ ë° ì†ì‹¤ ê³„ì‚°
                            y_pred = model(X_batch, prev_batch)
                            loss = criterion(y_pred, y_batch, prev_batch)
                            
                            val_loss += loss.item()
                        
                        val_loss /= len(val_loader)
                        
                        # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
                        scheduler.step(val_loss)
                        
                        if val_loss < best_val_loss:
                            best_val_loss = val_loss
                            patience_counter = 0
                        else:
                            patience_counter += 1
                        
                        if patience_counter >= params['patience']:
                            break
                
                valid_fold_count += 1
                fold_losses.append(best_val_loss)
                
            except Exception as e:
                logger.error(f"Error in fold {fold_idx+1}: {str(e)}")
                continue
        
        # ëª¨ë“  foldê°€ ì‹¤íŒ¨í•œ ê²½ìš° ë§¤ìš° í° ì†ì‹¤ê°’ ë°˜í™˜
        if not fold_losses:
            logger.warning("ëª¨ë“  foldê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì´ íŒŒë¼ë¯¸í„° ì¡°í•©ì€ ê±´ë„ˆëœë‹ˆë‹¤.")
            return float('inf')
        
        # ì„±ê³µí•œ foldì˜ í‰ê·  ì†ì‹¤ê°’ ë°˜í™˜
        return sum(fold_losses) / len(fold_losses)
    
    # Optuna ìµœì í™” ì‹œë„
    try:
        import optuna
        study = optuna.create_study(
            direction='minimize',
            sampler=optuna.samplers.TPESampler(seed=42)
        )
        
        study.optimize(objective, n_trials=n_trials)
        
        # ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°
        if study.best_trial.value == float('inf'):
            logger.warning(f"ëª¨ë“  trialì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ê¸°ë³¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return default_params
            
        best_params = study.best_params
        logger.info(f"\n{current_period} ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° (K-fold):")
        for key, value in best_params.items():
            logger.info(f"  {key}: {value}")
        
        # ëª¨ë“  í•„ìˆ˜ í‚¤ê°€ ìˆëŠ”ì§€ í™•ì¸
        required_keys = ['sequence_length', 'hidden_size', 'num_layers', 'dropout', 
                        'batch_size', 'learning_rate', 'num_epochs', 'patience',
                        'warmup_steps', 'lr_factor', 'lr_patience', 'min_lr',
                        'loss_alpha', 'loss_beta', 'loss_gamma', 'loss_delta']
        
        for key in required_keys:
            if key not in best_params:
                # ëˆ„ë½ëœ í‚¤ê°€ ìˆìœ¼ë©´ ê¸°ë³¸ê°’ í• ë‹¹
                if key == 'warmup_steps':
                    best_params[key] = 382
                elif key == 'lr_factor':
                    best_params[key] = 0.49
                elif key == 'lr_patience':
                    best_params[key] = 8
                elif key == 'min_lr':
                    best_params[key] = 1e-7
                elif key == 'loss_gamma':
                    best_params[key] = 0.07
                elif key == 'loss_delta':
                    best_params[key] = 0.07
                else:
                    best_params[key] = default_params[key]
        
        # ìºì‹œì— ì €ì¥
        with open(cache_file, 'w') as f:
            json.dump(best_params, f, indent=2)
        
        logger.info(f"í•˜ì´í¼íŒŒë¼ë¯¸í„°ê°€ {cache_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        return best_params
        
    except Exception as e:
        logger.error(f"í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì˜¤ë¥˜: {str(e)}")
        traceback.print_exc()
        
        # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë°˜í™˜
        return default_params

#######################################################################
# ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥/ë¡œë“œ í•¨ìˆ˜ë“¤
#######################################################################

def save_prediction_simple(prediction_results: dict, prediction_date):
    """ë¦¬ìŠ¤íŠ¸Â·ë”•íŠ¸ ì–´ë–¤ êµ¬ì¡°ë“  ì €ì¥ ê°€ëŠ¥í•œ ì•ˆì „ ë²„ì „ - íŒŒì¼ëª… í†µì¼"""
    try:
        preds_root = prediction_results.get("predictions")

        # â”€â”€ ì²« ì˜ˆì¸¡ ë ˆì½”ë“œ ì¶”ì¶œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if isinstance(preds_root, dict) and preds_root:
            preds_seq = preds_root.get("future") or []
        else:                                   # list í˜¹ì€ None
            preds_seq = preds_root or prediction_results.get("predictions_flat", [])

        if not preds_seq:
            raise ValueError("prediction_results ì•ˆì— ì˜ˆì¸¡ ë°ì´í„°ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")

        first_rec = preds_seq[0]
        first_date = pd.to_datetime(first_rec.get("date") or first_rec.get("Date"))
        if pd.isna(first_date):
            raise ValueError("ì²« ì˜ˆì¸¡ ë ˆì½”ë“œì— ë‚ ì§œ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")

        # ğŸ¯ íŒŒì¼ë³„ ìºì‹œ ë””ë ‰í† ë¦¬ ì‚¬ìš©
        cache_dirs = get_file_cache_dirs()
        file_predictions_dir = cache_dirs['predictions']
        
        # âœ… íŒŒì¼ ê²½ë¡œ ì„¤ì • (íŒŒì¼ë³„ ë””ë ‰í† ë¦¬ ë‚´)
        json_path = file_predictions_dir / f"prediction_start_{first_date:%Y%m%d}.json"
        csv_path = file_predictions_dir / f"prediction_start_{first_date:%Y%m%d}.csv"
        meta_path = file_predictions_dir / f"prediction_start_{first_date:%Y%m%d}_meta.json"
        attention_path = file_predictions_dir / f"prediction_start_{first_date:%Y%m%d}_attention.json"
        
        logger.info(f"ğŸ“ Using file cache directory: {cache_dirs['root'].name}")
        logger.info(f"  ğŸ“„ Predictions: {file_predictions_dir.name}")
        logger.info(f"  ğŸ“„ CSV: {csv_path.name}")
        logger.info(f"  ğŸ“„ Meta: {meta_path.name}")

        # â”€â”€ validation ê°œìˆ˜ ê³„ì‚° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if isinstance(preds_root, dict):
            validation_cnt = len(preds_root.get("validation", []))
        else:
            validation_cnt = 0

        # â”€â”€ ë©”íƒ€ + ë³¸ë¬¸ êµ¬ì„± (íŒŒì¼ ìºì‹œ ì •ë³´ í¬í•¨) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        current_file_path = prediction_state.get('current_file', None)
        file_content_hash = get_data_content_hash(current_file_path) if current_file_path else None
        
        meta = {
            "prediction_start_date": first_date.strftime("%Y-%m-%d"),
            "data_end_date": str(prediction_date)[:10],
            "created_at": datetime.now().isoformat(),
            "semimonthly_period": prediction_results.get("semimonthly_period"),
            "next_semimonthly_period": prediction_results.get("next_semimonthly_period"),
            "selected_features": prediction_results.get("selected_features", []),
            "total_predictions": len(prediction_results.get("predictions_flat", preds_seq)),
            "validation_points": validation_cnt,
            "is_pure_future_prediction": prediction_results.get("summary", {}).get(
                "is_pure_future_prediction", validation_cnt == 0
            ),
            "metrics": prediction_results.get("metrics"),
            "interval_scores": prediction_results.get("interval_scores", {}),
            # ğŸ”‘ ìºì‹œ ì—°ë™ì„ ìœ„í•œ íŒŒì¼ ì •ë³´
            "file_path": current_file_path,
            "file_content_hash": file_content_hash,
            "model_type": prediction_results.get("model_type", "ImprovedLSTMPredictor"),
            "loss_function": prediction_results.get("loss_function", "DirectionalLoss"),
            "prediction_mode": "ì¼ë°˜ ëª¨ë“œ"
        }

        # âœ… CSV íŒŒì¼ ì €ì¥
        predictions_data = clean_predictions_data(
            prediction_results.get("predictions_flat", preds_seq)
        )
        
        if predictions_data:
            pred_df = pd.DataFrame(predictions_data)
            pred_df.to_csv(csv_path, index=False)
            logger.info(f"âœ… CSV saved: {csv_path}")

        # âœ… ë©”íƒ€ë°ì´í„° ì €ì¥
        with open(meta_path, "w", encoding="utf-8") as fp:
            json.dump(meta, fp, ensure_ascii=False, indent=2)
        logger.info(f"âœ… Metadata saved: {meta_path}")

        # âœ… Attention ë°ì´í„° ì €ì¥ (ìˆëŠ” ê²½ìš°)
        attention_data = prediction_results.get("attention_data")
        if attention_data:
            attention_save_data = {
                "image_base64": attention_data.get("image", ""),
                "feature_importance": attention_data.get("feature_importance", {}),
                "temporal_importance": attention_data.get("temporal_importance", {})
            }
            
            with open(attention_path, "w", encoding="utf-8") as fp:
                json.dump(attention_save_data, fp, ensure_ascii=False, indent=2)
            logger.info(f"âœ… Attention saved: {attention_path}")

        # âœ… ì´ë™í‰ê·  ë°ì´í„° ì €ì¥ (ìˆëŠ” ê²½ìš°)
        ma_results = prediction_results.get("ma_results")
        ma_file = None
        if ma_results:
            ma_path = file_predictions_dir / f"prediction_start_{first_date:%Y%m%d}_ma.json"
            try:
                with open(ma_path, "w", encoding="utf-8") as fp:
                    json.dump(ma_results, fp, ensure_ascii=False, indent=2, default=str)
                logger.info(f"âœ… MA results saved: {ma_path}")
                ma_file = str(ma_path)
            except Exception as e:
                logger.warning(f"âš ï¸ Failed to save MA results: {str(e)}")

        # âœ… ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸
        update_predictions_index_simple(meta)
        
        logger.info(f"âœ… Complete prediction save â†’ start date: {meta['prediction_start_date']}")
        return {
            "success": True, 
            "csv_file": str(csv_path),
            "meta_file": str(meta_path),
            "attention_file": str(attention_path) if attention_data else None,
            "ma_file": ma_file,
            "prediction_start_date": meta["prediction_start_date"]
        }

    except Exception as e:
        logger.error(f"âŒ save_prediction_simple ì˜¤ë¥˜: {e}")
        logger.error(traceback.format_exc())
        return {"success": False, "error": str(e)}

# 2. Attention ë°ì´í„°ë¥¼ í¬í•¨í•œ ë¡œë“œ í•¨ìˆ˜
def load_prediction_simple(prediction_start_date):
    """
    ë‹¨ìˆœí™”ëœ ì˜ˆì¸¡ ê²°ê³¼ ë¡œë“œ í•¨ìˆ˜
    """
    try:
        predictions_dir = Path(PREDICTIONS_DIR)
        
        if isinstance(prediction_start_date, str):
            start_date = pd.to_datetime(prediction_start_date)
        else:
            start_date = prediction_start_date
        
        date_str = start_date.strftime('%Y%m%d')
        
        csv_filepath = predictions_dir / f"prediction_{date_str}.csv"
        meta_filepath = predictions_dir / f"prediction_{date_str}_meta.json"
        
        if not csv_filepath.exists() or not meta_filepath.exists():
            return {'success': False, 'error': f'Prediction files not found for {start_date.strftime("%Y-%m-%d")}'}
        
        # CSV ë¡œë“œ - xlwings ìš°ì„  ì‹œë„
        try:
            if XLWINGS_AVAILABLE:
                logger.info(f"ğŸ”“ [XLWINGS_CSV] Attempting to load CSV with xlwings: {csv_filepath}")
                predictions_df = load_csv_with_xlwings(csv_filepath)
            else:
                predictions_df = pd.read_csv(csv_filepath)
        except Exception as e:
            logger.warning(f"âš ï¸ [XLWINGS_CSV] xlwings failed, falling back to pandas: {str(e)}")
            predictions_df = pd.read_csv(csv_filepath)
        predictions_df['Date'] = pd.to_datetime(predictions_df['Date'])
        if 'Prediction_From' in predictions_df.columns:
            predictions_df['Prediction_From'] = pd.to_datetime(predictions_df['Prediction_From'])
        
        predictions = predictions_df.to_dict('records')
        
        # ë©”íƒ€ë°ì´í„° ë¡œë“œ
        with open(meta_filepath, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        
        logger.info(f"Simple prediction load completed: {len(predictions)} predictions")
        
        return {
            'success': True,
            'predictions': predictions,
            'metadata': metadata,
            'attention_data': {
                'image': metadata.get('attention_map', {}).get('image_base64', ''),
                'feature_importance': metadata.get('attention_map', {}).get('feature_importance', {}),
                'temporal_importance': metadata.get('attention_map', {}).get('temporal_importance', {})
            }
        }
        
    except Exception as e:
        logger.error(f"Error loading prediction: {str(e)}")
        return {'success': False, 'error': str(e)}

def update_predictions_index_simple(metadata):
    """ë‹¨ìˆœí™”ëœ ì˜ˆì¸¡ ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸ - íŒŒì¼ë³„ ìºì‹œ ë””ë ‰í† ë¦¬ ì‚¬ìš©"""
    try:
        # ğŸ”§ metadataê°€ Noneì¸ ê²½ìš° ì²˜ë¦¬
        if metadata is None:
            logger.warning("âš ï¸ [INDEX] metadataê°€ Noneì…ë‹ˆë‹¤. ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return False
            
        # ğŸ¯ íŒŒì¼ë³„ ìºì‹œ ë””ë ‰í† ë¦¬ ì‚¬ìš©
        cache_dirs = get_file_cache_dirs()
        predictions_index_file = cache_dirs['predictions'] / 'predictions_index.csv'
        
        # ê¸°ì¡´ ì¸ë±ìŠ¤ ì½ê¸°
        index_data = []
        if predictions_index_file.exists():
            with open(predictions_index_file, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                index_data = list(reader)
        
        # ì¤‘ë³µ ì œê±°
        prediction_start_date = metadata.get('prediction_start_date')
        if not prediction_start_date:
            logger.warning("âš ï¸ [INDEX] metadataì— prediction_start_dateê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
            
        index_data = [row for row in index_data 
                     if row.get('prediction_start_date') != prediction_start_date]
        
        # metricsê°€ Noneì¼ ìˆ˜ë„ ìˆìœ¼ë¯€ë¡œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
        metrics = metadata.get('metrics') or {}
        
        # ìƒˆ ë°ì´í„° ì¶”ê°€ (ğŸ”§ í•„ë“œëª… ìˆ˜ì •)
        new_row = {
            'prediction_start_date': metadata.get('prediction_start_date', ''),
            'data_end_date': metadata.get('data_end_date', ''),
            'created_at': metadata.get('created_at', ''),
            'semimonthly_period': metadata.get('semimonthly_period', ''),
            'next_semimonthly_period': metadata.get('next_semimonthly_period', ''),
            'prediction_count': metadata.get('total_predictions', metadata.get('prediction_count', 0)),  # ğŸ”§ ìˆ˜ì •
            'f1_score': metrics.get('f1', 0) if isinstance(metrics, dict) else 0,
            'accuracy': metrics.get('accuracy', 0) if isinstance(metrics, dict) else 0,
            'mape': metrics.get('mape', 0) if isinstance(metrics, dict) else 0,
            'weighted_score': metrics.get('weighted_score', 0) if isinstance(metrics, dict) else 0
        }
        index_data.append(new_row)
        
        # ë‚ ì§œìˆœ ì •ë ¬ í›„ ì €ì¥
        index_data.sort(key=lambda x: x.get('data_end_date', ''), reverse=True)
        
        if index_data:
            fieldnames = new_row.keys()  # ğŸ”§ ì¼ê´€ëœ í•„ë“œëª… ì‚¬ìš©
            with open(predictions_index_file, 'w', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerows(index_data)
            
            logger.info(f"âœ… Predictions index updated successfully: {len(index_data)} entries")
            logger.info(f"ğŸ“„ Index file: {predictions_index_file}")
            return True
        else:
            logger.warning("âš ï¸ No data to write to index file")
            return False
        
    except Exception as e:
        logger.error(f"âŒ Error updating simple predictions index: {str(e)}")
        logger.error(traceback.format_exc())
        return False

def rebuild_predictions_index_from_existing_files():
    """
    ê¸°ì¡´ ì˜ˆì¸¡ íŒŒì¼ë“¤ë¡œë¶€í„° predictions_index.csvë¥¼ ì¬ìƒì„±í•˜ëŠ” í•¨ìˆ˜
    ğŸ”§ ëˆ„ì  ì˜ˆì¸¡ì´ ê¸°ì¡´ ë‹¨ì¼ ì˜ˆì¸¡ ìºì‹œë¥¼ ì¸ì‹í•  ìˆ˜ ìˆë„ë¡ í•¨
    """
    try:
        current_file = prediction_state.get('current_file')
        if not current_file:
            logger.warning("âš ï¸ No current file set, cannot rebuild index")
            return False
        
        cache_dirs = get_file_cache_dirs(current_file)
        predictions_dir = cache_dirs['predictions']
        predictions_index_file = predictions_dir / 'predictions_index.csv'
        
        logger.info(f"ğŸ”„ Rebuilding predictions index from existing files in: {predictions_dir}")
        
        # ê¸°ì¡´ ë©”íƒ€ íŒŒì¼ë“¤ ì°¾ê¸°
        meta_files = list(predictions_dir.glob("*_meta.json"))
        logger.info(f"ğŸ“‹ Found {len(meta_files)} meta files")
        
        if not meta_files:
            logger.warning("âš ï¸ No meta files found to rebuild index")
            return False
        
        index_data = []
        
        for meta_file in meta_files:
            try:
                with open(meta_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                
                # ì¸ë±ìŠ¤ ë ˆì½”ë“œ ìƒì„± (ë™ì¼í•œ í•„ë“œëª… ì‚¬ìš©)
                new_row = {
                    'prediction_start_date': metadata.get('prediction_start_date', ''),
                    'data_end_date': metadata.get('data_end_date', ''),
                    'created_at': metadata.get('created_at', ''),
                    'semimonthly_period': metadata.get('semimonthly_period', ''),
                    'next_semimonthly_period': metadata.get('next_semimonthly_period', ''),
                    'prediction_count': metadata.get('total_predictions', metadata.get('prediction_count', 0)),
                    'f1_score': metadata.get('metrics', {}).get('f1', 0),
                    'accuracy': metadata.get('metrics', {}).get('accuracy', 0),
                    'mape': metadata.get('metrics', {}).get('mape', 0),
                    'weighted_score': metadata.get('metrics', {}).get('weighted_score', 0)
                }
                
                index_data.append(new_row)
                logger.info(f"  âœ… {meta_file.name}: {new_row['prediction_start_date']}")
                
            except Exception as e:
                logger.warning(f"  âš ï¸  Error reading {meta_file.name}: {str(e)}")
                continue
        
        if not index_data:
            logger.error("âŒ No valid metadata found")
            return False
        
        # ë‚ ì§œìˆœ ì •ë ¬
        index_data.sort(key=lambda x: x.get('data_end_date', ''), reverse=True)
        
        # CSV íŒŒì¼ ìƒì„±
        fieldnames = index_data[0].keys()
        
        with open(predictions_index_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(index_data)
        
        logger.info(f"âœ… Successfully rebuilt predictions_index.csv with {len(index_data)} entries")
        logger.info(f"ğŸ“„ Index file: {predictions_index_file}")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ Error rebuilding predictions index: {str(e)}")
        logger.error(traceback.format_exc())
        return False

def update_cached_prediction_actual_values(prediction_start_date, update_latest_only=True):
    """
    ìºì‹œëœ ì˜ˆì¸¡ì˜ ì‹¤ì œê°’ë§Œ ì„ íƒì ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•˜ëŠ” ìµœì í™”ëœ í•¨ìˆ˜
    
    Args:
        prediction_start_date: ì˜ˆì¸¡ ì‹œì‘ ë‚ ì§œ
        update_latest_only: Trueë©´ ìµœì‹  ë°ì´í„°ë§Œ ì²´í¬í•˜ì—¬ ì„±ëŠ¥ ìµœì í™”
    
    Returns:
        dict: ì—…ë°ì´íŠ¸ ê²°ê³¼
    """
    try:
        current_file = prediction_state.get('current_file')
        if not current_file:
            return {'success': False, 'error': 'No current file context available'}
        
        # ìºì‹œëœ ì˜ˆì¸¡ ë¡œë“œ (ì‹¤ì œê°’ ì—…ë°ì´íŠ¸ ì—†ì´)
        cached_result = load_prediction_with_attention_from_csv(prediction_start_date)
        if not cached_result['success']:
            return cached_result
        
        predictions = cached_result['predictions']
        
        # ë°ì´í„° ë¡œë“œ (ìºì‹œ í™œìš©)
        logger.info(f"ğŸ”„ [ACTUAL_UPDATE] Loading data for actual value update...")
        df = load_data(current_file, use_cache=True)
        
        if df is None or df.empty:
            logger.warning(f"âš ï¸ [ACTUAL_UPDATE] Could not load data file")
            return {'success': False, 'error': 'Could not load data file'}
        
        last_data_date = df.index.max()
        updated_count = 0
        
        # ê° ì˜ˆì¸¡ì— ëŒ€í•´ ì‹¤ì œê°’ í™•ì¸ ë° ì„¤ì •
        for pred in predictions:
            pred_date = pd.to_datetime(pred['Date'])
            
            # ìµœì‹  ë°ì´í„°ë§Œ ì²´í¬í•˜ëŠ” ê²½ìš° ì„±ëŠ¥ ìµœì í™”
            if update_latest_only and pred_date < last_data_date - pd.Timedelta(days=30):
                continue
            
            # ì‹¤ì œ ë°ì´í„°ê°€ ì¡´ì¬í•˜ëŠ” ë‚ ì§œë©´ ì‹¤ì œê°’ ì„¤ì •
            if (pred_date in df.index and 
                pd.notna(df.loc[pred_date, 'MOPJ']) and 
                pred_date <= last_data_date):
                actual_val = float(df.loc[pred_date, 'MOPJ'])
                pred['Actual'] = actual_val
                updated_count += 1
                logger.debug(f"  ğŸ“Š Updated actual value for {pred_date.strftime('%Y-%m-%d')}: {actual_val:.2f}")
            elif 'Actual' not in pred or pred['Actual'] is None:
                pred['Actual'] = None
        
        logger.info(f"âœ… [ACTUAL_UPDATE] Updated {updated_count} actual values")
        
        # ì—…ë°ì´íŠ¸ëœ ê²°ê³¼ ë°˜í™˜
        cached_result['predictions'] = predictions
        cached_result['actual_values_updated'] = True
        cached_result['updated_count'] = updated_count
        
        return cached_result
        
    except Exception as e:
        logger.error(f"âŒ [ACTUAL_UPDATE] Error updating actual values: {str(e)}")
        return {'success': False, 'error': str(e)}

def load_prediction_from_csv(prediction_start_date_or_data_end_date):
    """
    í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ í•¨ìˆ˜ - ìë™ìœ¼ë¡œ ìƒˆë¡œìš´ í•¨ìˆ˜ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸
    """
    logger.info("Using compatibility wrapper - redirecting to new smart cache function")
    return load_prediction_with_attention_from_csv(prediction_start_date_or_data_end_date)

def load_prediction_with_attention_from_csv_in_dir(prediction_start_date, file_predictions_dir):
    """
    íŒŒì¼ë³„ ë””ë ‰í† ë¦¬ì—ì„œ ì €ì¥ëœ ì˜ˆì¸¡ ê²°ê³¼ì™€ attention ë°ì´í„°ë¥¼ í•¨ê»˜ ë¶ˆëŸ¬ì˜¤ëŠ” í•¨ìˆ˜
    """
    try:
        if isinstance(prediction_start_date, str):
            start_date = pd.to_datetime(prediction_start_date)
        else:
            start_date = prediction_start_date
        
        date_str = start_date.strftime('%Y%m%d')
        
        # íŒŒì¼ë³„ ë””ë ‰í† ë¦¬ì—ì„œ íŒŒì¼ ê²½ë¡œ ì„¤ì •
        csv_filepath = file_predictions_dir / f"prediction_start_{date_str}.csv"
        meta_filepath = file_predictions_dir / f"prediction_start_{date_str}_meta.json"
        attention_filepath = file_predictions_dir / f"prediction_start_{date_str}_attention.json"
        ma_filepath = file_predictions_dir / f"prediction_start_{date_str}_ma.json"
        
        logger.info(f"ğŸ“‚ Loading from file directory: {file_predictions_dir.name}")
        logger.info(f"  ğŸ“„ CSV: {csv_filepath.name}")
        
        if not csv_filepath.exists() or not meta_filepath.exists():
            logger.warning(f"  âŒ Required files missing in {file_predictions_dir.name}")
            return {'success': False, 'error': f'Prediction files not found for {start_date.strftime("%Y-%m-%d")}'}
        
        # CSV ë¡œë“œ - xlwings ìš°ì„  ì‹œë„
        try:
            if XLWINGS_AVAILABLE:
                logger.info(f"ğŸ”“ [XLWINGS_CSV] Attempting to load CSV with xlwings: {csv_filepath}")
                predictions_df = load_csv_with_xlwings(csv_filepath)
            else:
                predictions_df = pd.read_csv(csv_filepath)
        except Exception as e:
            logger.warning(f"âš ï¸ [XLWINGS_CSV] xlwings failed, falling back to pandas: {str(e)}")
            predictions_df = pd.read_csv(csv_filepath)
        
        # ğŸ”§ ì»¬ëŸ¼ëª… í˜¸í™˜ì„± ì²˜ë¦¬: ì†Œë¬¸ìë¡œ ì €ì¥ëœ ì»¬ëŸ¼ì„ ëŒ€ë¬¸ìë¡œ ë³€í™˜
        if 'date' in predictions_df.columns:
            predictions_df['Date'] = pd.to_datetime(predictions_df['date'])
        elif 'Date' in predictions_df.columns:
            predictions_df['Date'] = pd.to_datetime(predictions_df['Date'])
        
        if 'prediction' in predictions_df.columns:
            predictions_df['Prediction'] = predictions_df['prediction']
        
        if 'prediction_from' in predictions_df.columns:
            predictions_df['Prediction_From'] = pd.to_datetime(predictions_df['prediction_from'])
        elif 'Prediction_From' in predictions_df.columns:
            predictions_df['Prediction_From'] = pd.to_datetime(predictions_df['Prediction_From'])
        
        predictions = predictions_df.to_dict('records')
        
        # âœ… JSON ì§ë ¬í™”ë¥¼ ìœ„í•´ Timestamp ê°ì²´ë“¤ì„ ë¬¸ìì—´ë¡œ ì•ˆì „í•˜ê²Œ ë³€í™˜
        for pred in predictions:
            for key, value in list(pred.items()):
                if pd.isna(value):
                    pred[key] = None
                elif isinstance(value, pd.Timestamp):
                    pred[key] = value.strftime('%Y-%m-%d')
                elif isinstance(value, (np.int64, np.float64)):
                    # ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì€ ëª¨ë‘ floatë¡œ ìœ ì§€
                    pred[key] = float(value)
                elif hasattr(value, 'item'):  # numpy scalars
                    pred[key] = value.item()
        
        # âœ… ìºì‹œì—ì„œ ë¡œë“œí•  ë•Œ ì‹¤ì œê°’ ë‹¤ì‹œ ì„¤ì • (ì„ íƒì  - ì„±ëŠ¥ ìµœì í™”)
        # ğŸ’¡ ìºì‹œëœ ì˜ˆì¸¡ì„ ë¹ ë¥´ê²Œ ë¶ˆëŸ¬ì˜¤ê¸° ìœ„í•´ ì‹¤ì œê°’ ì—…ë°ì´íŠ¸ë¥¼ ìŠ¤í‚µ
        # í•„ìš”ì‹œì—ë§Œ ë³„ë„ APIë¡œ ì‹¤ì œê°’ ì—…ë°ì´íŠ¸ ìˆ˜í–‰
        logger.info(f"ğŸ“¦ [CACHE_FAST] Skipping actual value update for faster cache loading")
        logger.info(f"ğŸ’¡ [CACHE_FAST] Use separate API endpoint if actual value update is needed")
        
        # ë©”íƒ€ë°ì´í„° ë¡œë“œ
        with open(meta_filepath, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        
        # Attention ë°ì´í„° ë¡œë“œ
        attention_data = {}
        if attention_filepath.exists():
            try:
                with open(attention_filepath, 'r', encoding='utf-8') as f:
                    attention_raw = json.load(f)
                attention_data = {
                    'image': attention_raw.get('image_base64', ''),
                    'feature_importance': attention_raw.get('feature_importance', {}),
                    'temporal_importance': attention_raw.get('temporal_importance', {})
                }
                logger.info(f"  ğŸ§  Attention data loaded successfully")
                logger.info(f"  ğŸ§  Image data length: {len(attention_data['image']) if attention_data['image'] else 0}")
                logger.info(f"  ğŸ§  Feature importance keys: {len(attention_data['feature_importance'])}")
                logger.info(f"  ğŸ§  Temporal importance keys: {len(attention_data['temporal_importance'])}")
            except Exception as e:
                logger.warning(f"  âš ï¸  Failed to load attention data: {str(e)}")
                attention_data = {}
        
        # ì´ë™í‰ê·  ë°ì´í„° ë¡œë“œ
        ma_results = {}
        if ma_filepath.exists():
            try:
                with open(ma_filepath, 'r', encoding='utf-8') as f:
                    ma_results = json.load(f)
                logger.info(f"  ğŸ“Š MA results loaded successfully")
            except Exception as e:
                logger.warning(f"  âš ï¸  Failed to load MA results: {str(e)}")
        
        logger.info(f"âœ… File directory cache load completed: {len(predictions)} predictions")
        
        return {
            'success': True,
            'predictions': predictions,
            'metadata': metadata,
            'attention_data': attention_data,
            'ma_results': ma_results
        }
        
    except Exception as e:
        logger.error(f"âŒ Error loading prediction from file directory: {str(e)}")
        return {'success': False, 'error': str(e)}

def load_prediction_with_attention_from_csv(prediction_start_date):
    """
    ì €ì¥ëœ ì˜ˆì¸¡ ê²°ê³¼ì™€ attention ë°ì´í„°ë¥¼ í•¨ê»˜ ë¶ˆëŸ¬ì˜¤ëŠ” í•¨ìˆ˜ - íŒŒì¼ë³„ ìºì‹œ ì‹œìŠ¤í…œ ì‚¬ìš©
    """
    try:
        # ğŸ¯ íŒŒì¼ë³„ ìºì‹œ ë””ë ‰í† ë¦¬ ì‚¬ìš©
        current_file = prediction_state.get('current_file')
        if not current_file:
            logger.error("âŒ No current file set in prediction_state")
            return {'success': False, 'error': 'No current file context available'}
            
        cache_dirs = get_file_cache_dirs(current_file)
        predictions_dir = cache_dirs['predictions']
        
        if isinstance(prediction_start_date, str):
            start_date = pd.to_datetime(prediction_start_date)
        else:
            start_date = prediction_start_date
        
        date_str = start_date.strftime('%Y%m%d')
        
        # íŒŒì¼ ê²½ë¡œë“¤
        csv_filepath = predictions_dir / f"prediction_start_{date_str}.csv"
        meta_filepath = predictions_dir / f"prediction_start_{date_str}_meta.json"
        attention_filepath = predictions_dir / f"prediction_start_{date_str}_attention.json"
        
        # í•„ìˆ˜ íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not csv_filepath.exists() or not meta_filepath.exists():
            return {
                'success': False,
                'error': f'Prediction files not found for start date {start_date.strftime("%Y-%m-%d")}'
            }
        
        # CSV íŒŒì¼ ì½ê¸° - xlwings ìš°ì„  ì‹œë„
        try:
            if XLWINGS_AVAILABLE:
                logger.info(f"ğŸ”“ [XLWINGS_CSV] Attempting to load CSV with xlwings: {csv_filepath}")
                predictions_df = load_csv_with_xlwings(csv_filepath)
            else:
                predictions_df = pd.read_csv(csv_filepath)
        except Exception as e:
            logger.warning(f"âš ï¸ [XLWINGS_CSV] xlwings failed, falling back to pandas: {str(e)}")
            predictions_df = pd.read_csv(csv_filepath)
        
        # ğŸ”§ ì»¬ëŸ¼ëª… í˜¸í™˜ì„± ì²˜ë¦¬: ì†Œë¬¸ìë¡œ ì €ì¥ëœ ì»¬ëŸ¼ì„ ëŒ€ë¬¸ìë¡œ ë³€í™˜
        if 'date' in predictions_df.columns:
            predictions_df['Date'] = pd.to_datetime(predictions_df['date'])
        elif 'Date' in predictions_df.columns:
            predictions_df['Date'] = pd.to_datetime(predictions_df['Date'])
        
        if 'prediction' in predictions_df.columns:
            predictions_df['Prediction'] = predictions_df['prediction']
        
        if 'prediction_from' in predictions_df.columns:
            predictions_df['Prediction_From'] = pd.to_datetime(predictions_df['prediction_from'])
        elif 'Prediction_From' in predictions_df.columns:
            predictions_df['Prediction_From'] = pd.to_datetime(predictions_df['Prediction_From'])
        
        predictions = predictions_df.to_dict('records')
        
        # âœ… JSON ì§ë ¬í™”ë¥¼ ìœ„í•´ Timestamp ê°ì²´ë“¤ì„ ë¬¸ìì—´ë¡œ ì•ˆì „í•˜ê²Œ ë³€í™˜
        for pred in predictions:
            for key, value in list(pred.items()):
                if pd.isna(value):
                    pred[key] = None
                elif isinstance(value, pd.Timestamp):
                    pred[key] = value.strftime('%Y-%m-%d')
                elif isinstance(value, (np.int64, np.float64)):
                    # ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì€ ëª¨ë‘ floatë¡œ ìœ ì§€
                    pred[key] = float(value)
                elif hasattr(value, 'item'):  # numpy scalars
                    pred[key] = value.item()
        
        # âœ… ìºì‹œì—ì„œ ë¡œë“œí•  ë•Œ ì‹¤ì œê°’ ë‹¤ì‹œ ì„¤ì • (ì„ íƒì  - ì„±ëŠ¥ ìµœì í™”)
        # ğŸ’¡ ìºì‹œëœ ì˜ˆì¸¡ì„ ë¹ ë¥´ê²Œ ë¶ˆëŸ¬ì˜¤ê¸° ìœ„í•´ ì‹¤ì œê°’ ì—…ë°ì´íŠ¸ë¥¼ ìŠ¤í‚µ
        # í•„ìš”ì‹œì—ë§Œ ë³„ë„ APIë¡œ ì‹¤ì œê°’ ì—…ë°ì´íŠ¸ ìˆ˜í–‰
        logger.info(f"ğŸ“¦ [CACHE_FAST] Skipping actual value update for faster cache loading")
        logger.info(f"ğŸ’¡ [CACHE_FAST] Use separate API endpoint if actual value update is needed")
        
        # ë©”íƒ€ë°ì´í„° ì½ê¸°
        with open(meta_filepath, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        
        # Attention ë°ì´í„° ì½ê¸° (ìˆëŠ” ê²½ìš°)
        attention_data = None
        if attention_filepath.exists():
            try:
                with open(attention_filepath, 'r', encoding='utf-8') as f:
                    stored_attention = json.load(f)
                
                attention_data = {
                    'image': stored_attention.get('image_base64', ''),
                    'file_path': None,  # ì´ë¯¸ì§€ëŠ” base64ë¡œ ì €ì¥ë¨
                    'feature_importance': stored_attention.get('feature_importance', {}),
                    'temporal_importance': stored_attention.get('temporal_importance', {})
                }
                logger.info(f"Attention data loaded from: {attention_filepath}")
            except Exception as e:
                logger.warning(f"Failed to load attention data: {str(e)}")
                attention_data = None

        # ğŸ”„ ì´ë™í‰ê·  ë°ì´í„° ì½ê¸° (ìˆëŠ” ê²½ìš°)
        ma_filepath = predictions_dir / f"prediction_start_{date_str}_ma.json"
        ma_results = None
        if ma_filepath.exists():
            try:
                with open(ma_filepath, 'r', encoding='utf-8') as f:
                    ma_results = json.load(f)
                logger.info(f"MA results loaded from: {ma_filepath} ({len(ma_results)} windows)")
            except Exception as e:
                logger.warning(f"Failed to load MA results: {str(e)}")
                ma_results = None
        
        logger.info(f"Complete prediction data loaded: {csv_filepath} ({len(predictions)} predictions)")
        
        return {
            'success': True,
            'predictions': predictions,
            'metadata': metadata,
            'attention_data': attention_data,
            'ma_results': ma_results,  # ğŸ”‘ ì´ë™í‰ê·  ë°ì´í„° ì¶”ê°€
            'prediction_start_date': start_date.strftime('%Y-%m-%d'),
            'data_end_date': metadata.get('data_end_date'),
            'semimonthly_period': metadata['semimonthly_period'],
            'next_semimonthly_period': metadata['next_semimonthly_period'],
            'metrics': metadata['metrics'],
            'interval_scores': metadata['interval_scores'],
            'selected_features': metadata['selected_features'],
            'has_cached_attention': attention_data is not None,
            'has_cached_ma': ma_results is not None  # ğŸ”‘ MA ìºì‹œ ì—¬ë¶€ ì¶”ê°€
        }
        
    except Exception as e:
        logger.error(f"Error loading prediction with attention: {str(e)}")
        logger.error(traceback.format_exc())
        return {
            'success': False,
            'error': str(e)
        }

def get_saved_predictions_list_for_file(file_path, limit=100):
    """
    íŠ¹ì • íŒŒì¼ì˜ ìºì‹œ ë””ë ‰í† ë¦¬ì—ì„œ ì €ì¥ëœ ì˜ˆì¸¡ ê²°ê³¼ ëª©ë¡ì„ ì¡°íšŒí•˜ëŠ” í•¨ìˆ˜
    
    Parameters:
    -----------
    file_path : str
        í˜„ì¬ íŒŒì¼ ê²½ë¡œ
    limit : int
        ë°˜í™˜í•  ìµœëŒ€ ê°œìˆ˜
    
    Returns:
    --------
    list : ì €ì¥ëœ ì˜ˆì¸¡ ëª©ë¡
    """
    try:
        predictions_list = []
        
        # íŒŒì¼ë³„ ìºì‹œ ë””ë ‰í† ë¦¬ ê²½ë¡œ êµ¬ì„±
        cache_dirs = get_file_cache_dirs(file_path)
        predictions_dir = Path(cache_dirs['predictions'])
        predictions_index_file = predictions_dir / 'predictions_index.csv'
        
        logger.info(f"ğŸ” [CACHE] Searching predictions in: {predictions_dir}")
        
        if predictions_index_file.exists():
            with open(predictions_index_file, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    if len(predictions_list) >= limit:
                        break
                    
                    prediction_start_date = row.get('prediction_start_date', row.get('first_prediction_date'))
                    data_end_date = row.get('data_end_date', row.get('prediction_base_date', row.get('prediction_date')))
                    
                    if prediction_start_date and data_end_date:
                        pred_info = {
                            'prediction_start_date': prediction_start_date,
                            'data_end_date': data_end_date,
                            'prediction_date': data_end_date,
                            'first_prediction_date': prediction_start_date,
                            'created_at': row.get('created_at'),
                            'semimonthly_period': row.get('semimonthly_period'),
                            'next_semimonthly_period': row.get('next_semimonthly_period'),
                            'prediction_count': row.get('prediction_count'),
                            'actual_business_days': row.get('actual_business_days'),
                            'csv_file': row.get('csv_file'),
                            'meta_file': row.get('meta_file'),
                            'f1_score': float(row.get('f1_score', 0)),
                            'accuracy': float(row.get('accuracy', 0)),
                            'mape': float(row.get('mape', 0)),
                            'weighted_score': float(row.get('weighted_score', 0)),
                            'naming_scheme': row.get('naming_scheme', 'file_based'),
                            'source_file': os.path.basename(file_path),
                            'cache_system': 'file_based'
                        }
                        predictions_list.append(pred_info)
            
            logger.info(f"ğŸ¯ [CACHE] Found {len(predictions_list)} predictions in file-specific cache")
        else:
            logger.info(f"ğŸ“‚ [CACHE] No predictions index found in {predictions_index_file}")
        
        # ë‚ ì§œìˆœìœ¼ë¡œ ì •ë ¬ (ìµœì‹  ìˆœ)
        predictions_list.sort(key=lambda x: x['data_end_date'], reverse=True)
        
        return predictions_list
        
    except Exception as e:
        logger.error(f"Error reading file-specific predictions list: {str(e)}")
        return []

def get_saved_predictions_list(limit=100):
    """
    ì €ì¥ëœ ì˜ˆì¸¡ ê²°ê³¼ ëª©ë¡ì„ ì¡°íšŒí•˜ëŠ” í•¨ìˆ˜ (ìƒˆë¡œìš´ íŒŒì¼ ì²´ê³„ í˜¸í™˜)
    
    Parameters:
    -----------
    limit : int
        ë°˜í™˜í•  ìµœëŒ€ ê°œìˆ˜
    
    Returns:
    --------
    list : ì €ì¥ëœ ì˜ˆì¸¡ ëª©ë¡
    """
    try:
        predictions_list = []
        
        # 1. íŒŒì¼ë³„ ìºì‹œ ì‹œìŠ¤í…œì—ì„œ ì˜ˆì¸¡ ê²€ìƒ‰
        cache_root = Path(CACHE_ROOT_DIR)
        if cache_root.exists():
            for file_dir in cache_root.iterdir():
                if not file_dir.is_dir():
                    continue
                
                predictions_dir = file_dir / 'predictions'
                predictions_index_file = predictions_dir / 'predictions_index.csv'
                
                if predictions_index_file.exists():
                    with open(predictions_index_file, 'r', encoding='utf-8') as f:
                        reader = csv.DictReader(f)
                        for row in reader:
                            if len(predictions_list) >= limit:
                                break
                            
                            prediction_start_date = row.get('prediction_start_date', row.get('first_prediction_date'))
                            data_end_date = row.get('data_end_date', row.get('prediction_base_date', row.get('prediction_date')))
                            
                            if prediction_start_date and data_end_date:
                                pred_info = {
                                    'prediction_start_date': prediction_start_date,
                                    'data_end_date': data_end_date,
                                    'prediction_date': data_end_date,
                                    'first_prediction_date': prediction_start_date,
                                    'created_at': row.get('created_at'),
                                    'semimonthly_period': row.get('semimonthly_period'),
                                    'next_semimonthly_period': row.get('next_semimonthly_period'),
                                    'prediction_count': row.get('prediction_count'),
                                    'actual_business_days': row.get('actual_business_days'),
                                    'csv_file': row.get('csv_file'),
                                    'meta_file': row.get('meta_file'),
                                    'f1_score': float(row.get('f1_score', 0)),
                                    'accuracy': float(row.get('accuracy', 0)),
                                    'mape': float(row.get('mape', 0)),
                                    'weighted_score': float(row.get('weighted_score', 0)),
                                    'naming_scheme': row.get('naming_scheme', 'file_based'),
                                    'source_file': file_dir.name,
                                    'cache_system': 'file_based'
                                }
                                predictions_list.append(pred_info)
        
        if len(predictions_list) == 0:
            logger.info("No predictions found in file-based cache system")
        
        # ë‚ ì§œìˆœìœ¼ë¡œ ì •ë ¬ (ìµœì‹  ìˆœ)
        predictions_list.sort(key=lambda x: x['data_end_date'], reverse=True)
        
        logger.info(f"Retrieved {len(predictions_list)} predictions from cache systems")
        return predictions_list
        
    except Exception as e:
        logger.error(f"Error reading predictions list: {str(e)}")
        return []

def load_accumulated_predictions_from_csv(start_date, end_date=None, limit=None, file_path=None):
    """
    CSVì—ì„œ ëˆ„ì  ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë¹ ë¥´ê²Œ ë¶ˆëŸ¬ì˜¤ëŠ” í•¨ìˆ˜ (ìµœì í™”ë¨)
    ìƒˆë¡œìš´ íŒŒì¼ëª… ì²´ê³„ì™€ ìŠ¤ë§ˆíŠ¸ ìºì‹œ ì‹œìŠ¤í…œ ì‚¬ìš©
    
    Parameters:
    -----------
    start_date : str or datetime
        ì‹œì‘ ë‚ ì§œ (ë°ì´í„° ê¸°ì¤€ì¼)
    end_date : str or datetime, optional
        ì¢…ë£Œ ë‚ ì§œ (ë°ì´í„° ê¸°ì¤€ì¼)
    limit : int, optional
        ìµœëŒ€ ë¡œë“œí•  ì˜ˆì¸¡ ê°œìˆ˜
    file_path : str, optional
        í˜„ì¬ íŒŒì¼ ê²½ë¡œ (í•´ë‹¹ íŒŒì¼ì˜ ìºì‹œ ë””ë ‰í† ë¦¬ì—ì„œë§Œ ê²€ìƒ‰)
    
    Returns:
    --------
    list : ëˆ„ì  ì˜ˆì¸¡ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    """
    try:
        logger.info(f"ğŸ” [CACHE_LOAD] Loading predictions from {start_date} to {end_date or 'latest'}")
        
        # ë‚ ì§œ í˜•ì‹ í†µì¼
        if isinstance(start_date, str):
            start_date = pd.to_datetime(start_date)
        if end_date and isinstance(end_date, str):
            end_date = pd.to_datetime(end_date)
        
        # ğŸ”§ ìˆ˜ì •: ì €ì¥ëœ ì˜ˆì¸¡ ëª©ë¡ ì¡°íšŒ (í˜„ì¬ íŒŒì¼ + í˜¸í™˜ ê°€ëŠ¥í•œ íŒŒì¼ë“¤)
        all_predictions = []
        if file_path:
            try:
                # 1. í˜„ì¬ íŒŒì¼ì˜ ìºì‹œ
                all_predictions = get_saved_predictions_list_for_file(file_path, limit=1000)
                logger.info(f"ğŸ¯ [CACHE_LOAD] Current file: Found {len(all_predictions)} prediction files")
                
                # 2. ë‹¤ë¥¸ í˜¸í™˜ ê°€ëŠ¥í•œ íŒŒì¼ë“¤ì˜ ìºì‹œ
                upload_dir = Path(UPLOAD_FOLDER)
                existing_files = [f for f in upload_dir.glob('*.xlsx') if str(f) != file_path]
                
                for existing_file in existing_files:
                    try:
                        # í™•ì¥ ê´€ê³„ ë˜ëŠ” íŒŒì¼ëª… ìœ ì‚¬ì„± í™•ì¸
                        extension_result = check_data_extension(str(existing_file), file_path)
                        is_extension = extension_result.get('is_extension', False)
                        
                        # íŒŒì¼ëª… ìœ ì‚¬ì„± í™•ì¸ (í™•ì¥ ê´€ê³„ í™•ì¸ ì‹¤íŒ¨ ì‹œ)
                        if not is_extension:
                            existing_name = Path(existing_file).stem.lower()
                            current_name = Path(file_path).stem.lower()
                            if existing_name in current_name or current_name in existing_name:
                                is_extension = True
                        
                        if is_extension:
                            compatible_predictions = get_saved_predictions_list_for_file(str(existing_file), limit=500)
                            all_predictions.extend(compatible_predictions)
                            logger.info(f"ğŸ”— [CACHE_LOAD] Compatible file {existing_file.name}: Found {len(compatible_predictions)} additional predictions")
                            
                    except Exception as file_error:
                        logger.warning(f"âš ï¸ [CACHE_LOAD] Error checking file {existing_file.name}: {str(file_error)}")
                        continue
                
                logger.info(f"ğŸ¯ [CACHE_LOAD] Total predictions found: {len(all_predictions)}")
                
            except Exception as e:
                logger.warning(f"âš ï¸ [CACHE_LOAD] Error in file-specific search: {str(e)}")
                return []
        else:
            try:
                all_predictions = get_saved_predictions_list(limit=1000)
                logger.info(f"ğŸ¯ [CACHE_LOAD] Found {len(all_predictions)} prediction files (global)")
            except Exception as e:
                logger.warning(f"âš ï¸ [CACHE_LOAD] Error in global search: {str(e)}")
                return []
        
        # ë‚ ì§œ ë²”ìœ„ í•„í„°ë§ (ë°ì´í„° ê¸°ì¤€ì¼ ê¸°ì¤€)
        filtered_predictions = []
        for pred_info in all_predictions:
            # ì¸ë±ìŠ¤ì—ì„œ ë°ì´í„° ê¸°ì¤€ì¼ í™•ì¸
            data_end_date = pd.to_datetime(pred_info.get('data_end_date', pred_info.get('prediction_date')))
            
            # ë‚ ì§œ ë²”ìœ„ í™•ì¸
            if data_end_date >= start_date:
                if end_date is None or data_end_date <= end_date:
                    filtered_predictions.append(pred_info)
            
            # ì œí•œ ê°œìˆ˜ í™•ì¸
            if limit and len(filtered_predictions) >= limit:
                break
        
        logger.info(f"ğŸ“‹ [CACHE] Found {len(filtered_predictions)} matching prediction files in date range")
        if len(filtered_predictions) > 0:
            logger.info(f"ğŸ“… [CACHE] Available cached dates:")
            for pred in filtered_predictions:
                data_end_date = pred.get('data_end_date', pred.get('prediction_date'))
                logger.info(f"    - {data_end_date}")
        
        # ê° ì˜ˆì¸¡ ê²°ê³¼ ë¡œë“œ
        accumulated_results = []
        for i, pred_info in enumerate(filtered_predictions):
            try:
                # ë°ì´í„° ê¸°ì¤€ì¼ì„ ì‚¬ìš©í•˜ì—¬ ì˜ˆì¸¡ ì‹œì‘ì¼ ê³„ì‚°
                data_end_date = pd.to_datetime(pred_info.get('data_end_date', pred_info.get('prediction_date')))
                
                # ë°ì´í„° ê¸°ì¤€ì¼ë¡œë¶€í„° ì˜ˆì¸¡ ì‹œì‘ì¼ ê³„ì‚°
                prediction_start_date = data_end_date + pd.Timedelta(days=1)
                while prediction_start_date.weekday() >= 5 or is_holiday(prediction_start_date):
                    prediction_start_date += pd.Timedelta(days=1)
                
                # íŒŒì¼ë³„ ìºì‹œ ë””ë ‰í† ë¦¬ ì‚¬ìš©
                if file_path:
                    cache_dirs = get_file_cache_dirs(file_path)
                    loaded_result = load_prediction_with_attention_from_csv_in_dir(prediction_start_date, cache_dirs['predictions'])
                else:
                    loaded_result = load_prediction_with_attention_from_csv(prediction_start_date)
                
                if loaded_result['success']:
                    logger.info(f"  âœ… [CACHE] Successfully loaded cached prediction for {data_end_date.strftime('%Y-%m-%d')}")
                    # ëˆ„ì  ì˜ˆì¸¡ í˜•ì‹ì— ë§ê²Œ ë³€í™˜
                    # ì•ˆì „í•œ ë°ì´í„° êµ¬ì¡° ìƒì„±
                    predictions = loaded_result.get('predictions', [])
                    
                    # ì˜ˆì¸¡ ë°ì´í„°ê°€ ì¤‘ì²©ëœ ë”•ì…”ë„ˆë¦¬ êµ¬ì¡°ì¸ ê²½ìš° ì²˜ë¦¬
                    if isinstance(predictions, dict):
                        if 'future' in predictions:
                            predictions = predictions['future']
                        elif 'predictions' in predictions:
                            predictions = predictions['predictions']
                    
                    if not isinstance(predictions, list):
                        logger.warning(f"Loaded predictions is not a list for {data_end_date.strftime('%Y-%m-%d')}: {type(predictions)}")
                        predictions = []
                    
                    metadata = loaded_result.get('metadata', {})
                    if not isinstance(metadata, dict):
                        metadata = {}
                    
                    # ğŸ”§ metrics ì•ˆì „ì„± ì²˜ë¦¬: Noneì´ë©´ ê¸°ë³¸ê°’ ì„¤ì •
                    cached_metrics = metadata.get('metrics')
                    if not cached_metrics or not isinstance(cached_metrics, dict):
                        cached_metrics = {'f1': 0.0, 'accuracy': 0.0, 'mape': 0.0, 'weighted_score': 0.0}
                    
                    accumulated_item = {
                        'date': data_end_date.strftime('%Y-%m-%d'),  # ë°ì´í„° ê¸°ì¤€ì¼
                        'prediction_start_date': loaded_result.get('prediction_start_date'),  # ì˜ˆì¸¡ ì‹œì‘ì¼
                        'predictions': predictions,
                        'metrics': cached_metrics,
                        'interval_scores': metadata.get('interval_scores', {}),
                        'next_semimonthly_period': metadata.get('next_semimonthly_period'),
                        'actual_business_days': metadata.get('actual_business_days'),
                        'original_interval_scores': metadata.get('interval_scores', {}),
                        'has_attention': loaded_result.get('has_cached_attention', False)
                    }
                    accumulated_results.append(accumulated_item)
                    logger.info(f"  âœ… [CACHE] Added to results {i+1}/{len(filtered_predictions)}: {data_end_date.strftime('%Y-%m-%d')}")
                else:
                    logger.warning(f"  âŒ [CACHE] Failed to load prediction {i+1}/{len(filtered_predictions)}: {loaded_result.get('error')}")
                    
            except Exception as e:
                logger.error(f"  âŒ Error loading prediction {i+1}/{len(filtered_predictions)}: {str(e)}")
                continue
        
        logger.info(f"ğŸ¯ [CACHE] Successfully loaded {len(accumulated_results)} predictions from CSV cache files")
        return accumulated_results
        
    except Exception as e:
        logger.error(f"Error loading accumulated predictions from CSV: {str(e)}")
        logger.error(traceback.format_exc())
        return []

def delete_saved_prediction(prediction_date):
    """
    ì €ì¥ëœ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì‚­ì œí•˜ëŠ” í•¨ìˆ˜
    
    Parameters:
    -----------
    prediction_date : str or datetime
        ì‚­ì œí•  ì˜ˆì¸¡ ë‚ ì§œ
    
    Returns:
    --------
    dict : ì‚­ì œ ê²°ê³¼
    """
    try:
        # ë‚ ì§œ í˜•ì‹ í†µì¼
        if isinstance(prediction_date, str):
            pred_date = pd.to_datetime(prediction_date)
        else:
            pred_date = prediction_date
        
        date_str = pred_date.strftime('%Y%m%d')
        
        # íŒŒì¼ ê²½ë¡œë“¤ (TARGET_DATE ë°©ì‹)
        csv_filepath = os.path.join(PREDICTIONS_DIR, f"prediction_target_{date_str}.csv")
        meta_filepath = os.path.join(PREDICTIONS_DIR, f"prediction_target_{date_str}_meta.json")
        
        # íŒŒì¼ ì‚­ì œ
        deleted_files = []
        if os.path.exists(csv_filepath):
            os.remove(csv_filepath)
            deleted_files.append(csv_filepath)
        
        if os.path.exists(meta_filepath):
            os.remove(meta_filepath)
            deleted_files.append(meta_filepath)
        
        # ğŸš« ë ˆê±°ì‹œ ì¸ë±ìŠ¤ ì œê±° ê¸°ëŠ¥ì€ íŒŒì¼ë³„ ìºì‹œ ì‹œìŠ¤í…œì—ì„œ ì œê±°ë¨
        # íŒŒì¼ë³„ ìºì‹œì—ì„œëŠ” ê° íŒŒì¼ì˜ predictions_index.csvê°€ ìë™ìœ¼ë¡œ ê´€ë¦¬ë¨
        logger.info("âš ï¸ Legacy delete_saved_prediction function called - not supported in file-based cache system")
        
        return {
            'success': True,
            'deleted_files': deleted_files,
            'message': f'Prediction for {pred_date.strftime("%Y-%m-%d")} deleted successfully'
        }
        
    except Exception as e:
        logger.error(f"Error deleting saved prediction: {str(e)}")
        return {
            'success': False,
            'error': str(e)
        }

#######################################################################
# ì˜ˆì¸¡ ì‹ ë¢°ë„ ë° êµ¬ë§¤ ì‹ ë¢°ë„ ê³„ì‚° í•¨ìˆ˜
#######################################################################

def calculate_prediction_consistency(accumulated_predictions, target_period):
    """
    ë‹¤ìŒ ë°˜ì›”ì— ëŒ€í•œ ì—¬ëŸ¬ ë‚ ì§œì˜ ì˜ˆì¸¡ ì¼ê´€ì„±ì„ ê³„ì‚°
    
    Parameters:
    -----------
    accumulated_predictions: list
        ì—¬ëŸ¬ ë‚ ì§œì— ìˆ˜í–‰í•œ ì˜ˆì¸¡ ê²°ê³¼ ëª©ë¡
    target_period: str
        ë‹¤ìŒ ë°˜ì›” ê¸°ê°„ (ì˜ˆ: "2025-01-SM1")
    
    Returns:
    -----------
    dict: ì¼ê´€ì„± ì ìˆ˜ì™€ ê´€ë ¨ ë©”íŠ¸ë¦­
    """
    import numpy as np
    
    # ë‚ ì§œë³„ ì˜ˆì¸¡ ë°ì´í„° ì¶”ì¶œ
    period_predictions = {}
    
    for prediction in accumulated_predictions:
        # ì•ˆì „í•œ ë°ì´í„° ì ‘ê·¼
        if not isinstance(prediction, dict):
            continue
            
        prediction_date = prediction.get('date')
        next_period = prediction.get('next_semimonthly_period')
        predictions_list = prediction.get('predictions', [])
        
        if next_period != target_period:
            continue
            
        if prediction_date not in period_predictions:
            period_predictions[prediction_date] = []
        
        # predictions_listê°€ ë°°ì—´ì¸ì§€ í™•ì¸
        if not isinstance(predictions_list, list):
            logger.warning(f"predictions_list is not a list for {prediction_date}: {type(predictions_list)}")
            continue
            
        for pred in predictions_list:
            # predê°€ ë”•ì…”ë„ˆë¦¬ì¸ì§€ í™•ì¸
            if not isinstance(pred, dict):
                logger.warning(f"Prediction item is not a dict for {prediction_date}: {type(pred)}")
                continue
                
            pred_date = pred.get('Date') or pred.get('date')
            pred_value = pred.get('Prediction') or pred.get('prediction')
            
            # ê°’ì´ ìœ íš¨í•œì§€ í™•ì¸
            if pred_date and pred_value is not None:
                period_predictions[prediction_date].append({
                    'date': pred_date,
                    'value': pred_value
                })
    
    # ë‚ ì§œë³„ë¡œ ì •ë ¬
    prediction_dates = sorted(period_predictions.keys())
    
    if len(prediction_dates) < 2:
        return {
            "consistency_score": None,
            "message": "Insufficient prediction data (min 2 required)",
            "period": target_period,
            "dates_count": len(prediction_dates)
        }
    
    # ì¼ê´€ì„± ë¶„ì„ì„ ìœ„í•œ ë‚ ì§œ ë§¤í•‘
    date_predictions = {}
    
    for pred_date in prediction_dates:
        for p in period_predictions[pred_date]:
            target_date = p['date']
            if target_date not in date_predictions:
                date_predictions[target_date] = []
            
            date_predictions[target_date].append({
                'prediction_date': pred_date,
                'value': p['value']
            })
    
    # ê° íƒ€ê²Ÿ ë‚ ì§œë³„ ì˜ˆì¸¡ê°’ ë³€ë™ì„± ê³„ì‚°
    overall_variations = []
    
    for target_date, predictions in date_predictions.items():
        if len(predictions) >= 2:
            # ì˜ˆì¸¡ê°’ ì¶”ì¶œ (None ê°’ í•„í„°ë§)
            values = [p['value'] for p in predictions if p['value'] is not None]
            
            if len(values) < 2:
                continue
                
            # ê°’ì´ ëª¨ë‘ ê°™ì€ ê²½ìš° CVë¥¼ 0ìœ¼ë¡œ ì²˜ë¦¬
            if all(v == values[0] for v in values):
                cv = 0.0
                overall_variations.append(cv)
                continue
            
            mean_value = np.mean(values)
            std_value = np.std(values)
            
            # ë³€ë™ ê³„ìˆ˜ (Coefficient of Variation)
            cv = std_value / abs(mean_value) if mean_value != 0 else float('inf')
            overall_variations.append(cv)
    
    # ì „ì²´ ì¼ê´€ì„± ì ìˆ˜ ê³„ì‚° (ë³€ë™ ê³„ìˆ˜ í‰ê· ì„ 0-100 ì ìˆ˜ë¡œ ë³€í™˜)
    if overall_variations:
        avg_cv = np.mean(overall_variations)
        consistency_score = max(0, min(100, 100 - (avg_cv * 100)))
    else:
        consistency_score = None
    
    # ì‹ ë¢°ë„ ë“±ê¸‰ ë¶€ì—¬
    if consistency_score is not None:
        if consistency_score >= 90:
            grade = "Very High"
        elif consistency_score >= 75:
            grade = "High"
        elif consistency_score >= 60:
            grade = "Medium"
        elif consistency_score >= 40:
            grade = "Low"
        else:
            grade = "Very Low"
    else:
        grade = "Unable to determine"
    
    return {
        "consistency_score": consistency_score,
        "consistency_grade": grade,
        "target_period": target_period,
        "prediction_count": len(prediction_dates),
        "average_variation": avg_cv * 100 if overall_variations else None,
        "message": f"Consistency for period {target_period} based on {len(prediction_dates)} predictions"
    }

# ëˆ„ì  ì˜ˆì¸¡ì˜ êµ¬ë§¤ ì‹ ë¢°ë„ ê³„ì‚° í•¨ìˆ˜ (ì˜¬ë°”ë¥¸ ë²„ì „)
def calculate_accumulated_purchase_reliability(accumulated_predictions):
    """
    ëˆ„ì  ì˜ˆì¸¡ì˜ êµ¬ë§¤ ì‹ ë¢°ë„ ê³„ì‚° (ì˜¬ë°”ë¥¸ ë°©ì‹)
    
    ê° ì˜ˆì¸¡ë§ˆë‹¤ ìƒìœ„ 3ê°œ êµ¬ê°„(1ë“±:3ì , 2ë“±:2ì , 3ë“±:1ì )ì„ ì„ ì •í•˜ê³ ,
    ê°™ì€ êµ¬ê°„ì´ ì—¬ëŸ¬ ì˜ˆì¸¡ì—ì„œ ì„ íƒë˜ë©´ ì ìˆ˜ë¥¼ ëˆ„ì í•˜ì—¬,
    ìµœê³  ëˆ„ì  ì ìˆ˜ êµ¬ê°„ì˜ ì ìˆ˜ / (ì˜ˆì¸¡ íšŸìˆ˜ Ã— 3ì ) Ã— 100%ë¡œ ê³„ì‚°
    
    Returns:
        tuple: (reliability_percentage, debug_info)
    """
    print(f"ğŸ” [RELIABILITY] Function called with {len(accumulated_predictions) if accumulated_predictions else 0} predictions")
    
    if not accumulated_predictions or not isinstance(accumulated_predictions, list):
        print(f"âš ï¸ [RELIABILITY] Invalid input: accumulated_predictions is empty or not a list")
        return 0.0, {}
    
    try:
        prediction_count = len(accumulated_predictions)
        print(f"ğŸ“Š [RELIABILITY] Processing {prediction_count} predictions...")
        
        # ğŸ”‘ êµ¬ê°„ë³„ ëˆ„ì  ì ìˆ˜ë¥¼ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬
        interval_accumulated_scores = {}
        
        for i, pred in enumerate(accumulated_predictions):
            if not isinstance(pred, dict):
                continue
                
            interval_scores = pred.get('interval_scores', {})
            pred_date = pred.get('date')
            
            if interval_scores and isinstance(interval_scores, dict):
                # ëª¨ë“  êµ¬ê°„ì„ í‰ê·  ê°€ê²© ìˆœìœ¼ë¡œ ì •ë ¬ (ê°€ê²©ì´ ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
                valid_intervals = []
                for score_data in interval_scores.values():
                    if isinstance(score_data, dict) and 'avg_price' in score_data:
                        valid_intervals.append(score_data)
                
                if valid_intervals:
                    # í‰ê·  ê°€ê²© ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ë‚®ì€ ê°€ê²©ì´ ìš°ì„ )
                    valid_intervals.sort(key=lambda x: x.get('avg_price', float('inf')))
                    
                    # ìƒìœ„ 3ê°œ êµ¬ê°„ì— ì ìˆ˜ ë¶€ì—¬
                    for rank, interval in enumerate(valid_intervals[:3]):
                        score = 3 - rank  # 1ë“±: 3ì , 2ë“±: 2ì , 3ë“±: 1ì 
                        
                        # êµ¬ê°„ ì‹ë³„í‚¤ ìƒì„± (ì‹œì‘ì¼-ì¢…ë£Œì¼)
                        interval_key = f"{interval.get('start_date')} ~ {interval.get('end_date')} ({interval.get('days')}ì¼)"
                        
                        # ëˆ„ì  ì ìˆ˜ ê³„ì‚°
                        if interval_key not in interval_accumulated_scores:
                            interval_accumulated_scores[interval_key] = {
                                'total_score': 0,
                                'appearances': 0,
                                'details': [],
                                'avg_price': interval.get('avg_price', 0),
                                'days': interval.get('days', 0)
                            }
                        
                        interval_accumulated_scores[interval_key]['total_score'] += score
                        interval_accumulated_scores[interval_key]['appearances'] += 1
                        interval_accumulated_scores[interval_key]['details'].append({
                            'date': pred_date,
                            'rank': rank + 1,
                            'score': score,
                            'avg_price': interval.get('avg_price', 0)
                        })
                        
                        print(f"ğŸ“Š [RELIABILITY] ë‚ ì§œ {pred_date}: {rank+1}ë“± {interval_key} â†’ {score}ì  (í‰ê· ê°€: {interval.get('avg_price', 0):.2f})")
        
        # ìµœê³  ëˆ„ì  ì ìˆ˜ êµ¬ê°„ ì°¾ê¸°
        if interval_accumulated_scores:
            best_interval_key = max(interval_accumulated_scores.keys(), 
                                  key=lambda k: interval_accumulated_scores[k]['total_score'])
            best_total_score = interval_accumulated_scores[best_interval_key]['total_score']
            
            # ë§Œì  ê³„ì‚° (ê° ì˜ˆì¸¡ë§ˆë‹¤ ìµœëŒ€ 3ì ì”©)
            max_possible_total_score = prediction_count * 3
            
            # êµ¬ë§¤ ì‹ ë¢°ë„ ê³„ì‚°
            reliability_percentage = (best_total_score / max_possible_total_score) * 100 if max_possible_total_score > 0 else 0.0
            
            print(f"\nğŸ¯ [RELIABILITY] === êµ¬ê°„ë³„ ëˆ„ì  ì ìˆ˜ ë¶„ì„ ===")
            print(f"ğŸ“Š ì˜ˆì¸¡ íšŸìˆ˜: {prediction_count}ê°œ")
            print(f"ğŸ“Š êµ¬ê°„ë³„ ëˆ„ì  ì ìˆ˜:")
            
            # ëˆ„ì  ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ í‘œì‹œ
            sorted_intervals = sorted(interval_accumulated_scores.items(), 
                                    key=lambda x: x[1]['total_score'], reverse=True)
            
            for interval_key, data in sorted_intervals[:5]:  # ìƒìœ„ 5ê°œë§Œ í‘œì‹œ
                print(f"   - {interval_key}: {data['total_score']}ì  ({data['appearances']}íšŒ ì„ íƒ)")
            
            print(f"\nğŸ† ìµœê³  ì ìˆ˜ êµ¬ê°„: {best_interval_key}")
            print(f"ğŸ† ìµœê³  ëˆ„ì  ì ìˆ˜: {best_total_score}ì ")
            print(f"ğŸ† êµ¬ê°„ ì‹ ë¢°ë„: {best_total_score}/{max_possible_total_score} = {reliability_percentage:.1f}%")
            
            # ë””ë²„ê·¸ ì •ë³´ ìƒì„±
            debug_info = {
                'prediction_count': prediction_count,
                'interval_accumulated_scores': interval_accumulated_scores,
                'best_interval_key': best_interval_key,
                'best_total_score': best_total_score,
                'max_possible_total_score': max_possible_total_score,
                'reliability_percentage': reliability_percentage
            }
            
            return reliability_percentage, debug_info
        else:
            print(f"âš ï¸ [RELIABILITY] No valid interval scores found")
            return 0.0, {}
            
    except Exception as e:
        print(f"Error calculating accumulated purchase reliability: {str(e)}")
        return 0.0, {'error': str(e)} 

def calculate_accumulated_purchase_reliability_with_debug(accumulated_predictions):
    """
    ë””ë²„ê·¸ ì •ë³´ì™€ í•¨ê»˜ ëˆ„ì  ì˜ˆì¸¡ì˜ êµ¬ë§¤ ì‹ ë¢°ë„ ê³„ì‚°
    """
    if not accumulated_predictions or not isinstance(accumulated_predictions, list):
        return 0.0, {}
    
    debug_info = {
        'prediction_count': len(accumulated_predictions),
        'individual_scores': [],
        'total_best_score': 0,
        'max_possible_total_score': 0
    }
    
    try:
        total_best_score = 0
        prediction_count = len(accumulated_predictions)
        
        for i, pred in enumerate(accumulated_predictions):
            if not isinstance(pred, dict):
                continue
                
            pred_date = pred.get('date')
            interval_scores = pred.get('interval_scores', {})
            
            best_score = 0
            capped_score = 0  # âœ… ì´ˆê¸°í™” ì¶”ê°€
            valid_scores = []  # âœ… valid_scoresë„ ì™¸ë¶€ì—ì„œ ì´ˆê¸°í™”
            
            if interval_scores and isinstance(interval_scores, dict):
                # ìœ íš¨í•œ interval score ì°¾ê¸°
                for score_data in interval_scores.values():
                    if isinstance(score_data, dict) and 'score' in score_data:
                        score_value = score_data.get('score', 0)
                        if isinstance(score_value, (int, float)):
                            valid_scores.append(score_value)
                
                if valid_scores:
                    best_score = max(valid_scores)
                    # ì ìˆ˜ë¥¼ 3ì ìœ¼ë¡œ ì œí•œ (ê° ì˜ˆì¸¡ì˜ ìµœëŒ€ ì ìˆ˜)
                    capped_score = min(best_score, 3.0)
                    total_best_score += capped_score
            
            debug_info['individual_scores'].append({
                'date': pred_date,
                'original_best_score': best_score,
                'actual_score_used': capped_score if valid_scores else 0,
                'max_score_per_prediction': 3,
                'has_valid_scores': len(valid_scores) > 0
            })
        
        # ì „ì²´ ê³„ì‚° - 3ì ì´ ë§Œì 
        max_possible_total_score = prediction_count * 3
        reliability_percentage = (total_best_score / max_possible_total_score) * 100 if max_possible_total_score > 0 else 0.0
        
        debug_info['total_best_score'] = total_best_score
        debug_info['max_possible_total_score'] = max_possible_total_score
        debug_info['reliability_percentage'] = reliability_percentage
        
        logger.info(f"ğŸ¯ ì˜¬ë°”ë¥¸ ëˆ„ì  êµ¬ë§¤ ì‹ ë¢°ë„ ê³„ì‚°:")
        logger.info(f"  - ì˜ˆì¸¡ íšŸìˆ˜: {prediction_count}íšŒ")
        
        # ğŸ” ê°œë³„ ì ìˆ˜ ë””ë²„ê¹… ì •ë³´ ì¶œë ¥
        for score_info in debug_info['individual_scores']:
            original = score_info.get('original_best_score', 0)
            actual = score_info.get('actual_score_used', 0)
            logger.info(f"ğŸ“Š ë‚ ì§œ {score_info['date']}: ì›ë³¸ì ìˆ˜={original:.1f}, ì ìš©ì ìˆ˜={actual:.1f}, ìœ íš¨ì ìˆ˜ìˆìŒ={score_info['has_valid_scores']}")
        
        logger.info(f"  - ì´ íšë“ ì ìˆ˜: {total_best_score:.1f}ì ")
        logger.info(f"  - ìµœëŒ€ ê°€ëŠ¥ ì ìˆ˜: {max_possible_total_score}ì  ({prediction_count} Ã— 3)")
        logger.info(f"  - êµ¬ë§¤ ì‹ ë¢°ë„: {reliability_percentage:.1f}%")
        
        # âœ… ì¶”ê°€ ê²€ì¦ ë¡œê¹…
        if reliability_percentage == 100.0:
            logger.warning("âš ï¸ [RELIABILITY] êµ¬ë§¤ ì‹ ë¢°ë„ê°€ 100%ì…ë‹ˆë‹¤. ê³„ì‚° ê²€ì¦:")
            logger.warning(f"   - total_best_score: {total_best_score}")
            logger.warning(f"   - max_possible_total_score: {max_possible_total_score}")
            logger.warning(f"   - prediction_count: {prediction_count}")
            for i, score_info in enumerate(debug_info['individual_scores']):
                logger.warning(f"   - ì˜ˆì¸¡ {i+1}: {score_info}")
        elif reliability_percentage == 0.0:
            logger.warning("âš ï¸ [RELIABILITY] êµ¬ë§¤ ì‹ ë¢°ë„ê°€ 0%ì…ë‹ˆë‹¤. ê³„ì‚° ê²€ì¦:")
            logger.warning(f"   - total_best_score: {total_best_score}")
            logger.warning(f"   - max_possible_total_score: {max_possible_total_score}")
            logger.warning(f"   - prediction_count: {prediction_count}")
        
        return reliability_percentage, debug_info
            
    except Exception as e:
        logger.error(f"Error calculating accumulated purchase reliability: {str(e)}")
        return 0.0, {'error': str(e)}

def calculate_actual_business_days(predictions):
    """
    ì˜ˆì¸¡ ê²°ê³¼ì—ì„œ ì‹¤ì œ ì˜ì—…ì¼ ìˆ˜ë¥¼ ê³„ì‚°í•˜ëŠ” í—¬í¼ í•¨ìˆ˜
    """
    if not predictions:
        return 0
    
    try:
        actual_days = len([p for p in predictions 
                          if p.get('Date') and not p.get('is_synthetic', False)])
        return actual_days
    except Exception as e:
        logger.error(f"Error calculating actual business days: {str(e)}")
        return 0

def get_previous_semimonthly_period(semimonthly_period):
    """
    ì£¼ì–´ì§„ ë°˜ì›” ê¸°ê°„ì˜ ì´ì „ ë°˜ì›” ê¸°ê°„ì„ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜
    
    Parameters:
    -----------
    semimonthly_period : str
        "YYYY-MM-SM1" ë˜ëŠ” "YYYY-MM-SM2" í˜•ì‹ì˜ ë°˜ì›” ê¸°ê°„
    
    Returns:
    --------
    str
        ì´ì „ ë°˜ì›” ê¸°ê°„
    """
    parts = semimonthly_period.split('-')
    year = int(parts[0])
    month = int(parts[1])
    half = parts[2]
    
    if half == "SM1":
        # ìƒë°˜ì›”ì¸ ê²½ìš° ì´ì „ ì›”ì˜ í•˜ë°˜ì›”ë¡œ
        if month == 1:
            return f"{year-1}-12-SM2"
        else:
            return f"{year}-{month-1:02d}-SM2"
    else:
        # í•˜ë°˜ì›”ì¸ ê²½ìš° ê°™ì€ ì›”ì˜ ìƒë°˜ì›”ë¡œ
        return f"{year}-{month:02d}-SM1"

#######################################################################
# ì‹œê°í™” í•¨ìˆ˜
#######################################################################

def get_global_y_range(original_df, test_dates, predict_window):
    """
    í…ŒìŠ¤íŠ¸ êµ¬ê°„ì˜ ëª¨ë“  MOPJ ê°’ì„ ê¸°ë°˜ìœ¼ë¡œ ì „ì—­ yì¶• ë²”ìœ„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    
    Args:
        original_df: ì›ë³¸ ë°ì´í„°í”„ë ˆì„
        test_dates: í…ŒìŠ¤íŠ¸ ë‚ ì§œ ë°°ì—´
        predict_window: ì˜ˆì¸¡ ê¸°ê°„
    
    Returns:
        tuple: (y_min, y_max) ì „ì—­ ë²”ìœ„ ê°’
    """
    # í…ŒìŠ¤íŠ¸ êµ¬ê°„ ë°ì´í„° ì¶”ì¶œ
    test_values = []
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ì‹¤ì œ ê°’ ìˆ˜ì§‘
    for date in test_dates:
        if date in original_df.index and not pd.isna(original_df.loc[date, 'MOPJ']):
            test_values.append(original_df.loc[date, 'MOPJ'])
    
    # ì•ˆì „ì¥ì¹˜: ë°ì´í„°ê°€ ì—†ìœ¼ë©´ None ë°˜í™˜
    if not test_values:
        return None, None
    
    # ìµœì†Œ/ìµœëŒ€ ê³„ì‚° (ì•½ê°„ì˜ ë§ˆì§„ ì¶”ê°€)
    y_min = min(test_values) * 0.95
    y_max = max(test_values) * 1.05
    
    return y_min, y_max

def visualize_attention_weights(model, features, prev_value, sequence_end_date, feature_names=None, actual_sequence_dates=None):
    """
    ëª¨ë¸ì˜ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ë¥¼ ì‹œê°í™”í•˜ëŠ” í•¨ìˆ˜ - 2x2 ë ˆì´ì•„ì›ƒìœ¼ë¡œ ê°œì„ 
    sequence_end_date: ì‹œí€€ìŠ¤ ë°ì´í„°ì˜ ë§ˆì§€ë§‰ ë‚ ì§œ (ì˜ˆì¸¡ ì‹œì‘ì¼ ì „ë‚ )
    """
    model.eval()
    
    # íŠ¹ì„± ì´ë¦„ì´ ì—†ìœ¼ë©´ ì¸ë±ìŠ¤ë¡œ ìƒì„±
    if feature_names is None:
        feature_names = [f"Feature {i}" for i in range(features.shape[2])]
    else:
        # íŠ¹ì„± ìˆ˜ì— ë§ê²Œ ì¡°ì •
        feature_names = feature_names[:features.shape[2]]
    
    # í…ì„œê°€ ì•„ë‹ˆë©´ ë³€í™˜
    if not isinstance(features, torch.Tensor):
        features = torch.FloatTensor(features).to(next(model.parameters()).device)
    
    # prev_value ì²˜ë¦¬
    if prev_value is not None:
        if not isinstance(prev_value, torch.Tensor):
            try:
                prev_value = float(prev_value)
                prev_value = torch.FloatTensor([prev_value]).to(next(model.parameters()).device)
            except (TypeError, ValueError):
                logger.warning("Warning: prev_valueë¥¼ ìˆ«ìë¡œ ë³€í™˜í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 0ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
                prev_value = torch.FloatTensor([0.0]).to(next(model.parameters()).device)
    
    # ì‹œí€€ìŠ¤ ê¸¸ì´
    seq_len = features.shape[1]
    
    # ë‚ ì§œ ë¼ë²¨ ìƒì„± - ì‹¤ì œ ì‹œí€€ìŠ¤ ë‚ ì§œ ì‚¬ìš©
    date_labels = []
    if actual_sequence_dates is not None and len(actual_sequence_dates) == seq_len:
        # ì‹¤ì œ ë‚ ì§œ ì •ë³´ê°€ ì „ë‹¬ëœ ê²½ìš° ì‚¬ìš©
        for date in actual_sequence_dates:
            try:
                if isinstance(date, str):
                    date_labels.append(date)
                else:
                    date_labels.append(format_date(date, '%Y-%m-%d'))
            except:
                date_labels.append(str(date))
    else:
        # ì‹¤ì œ ë‚ ì§œ ì •ë³´ê°€ ì—†ìœ¼ë©´ ê¸°ì¡´ ë°©ì‹ ì‚¬ìš© (ì‹œí€€ìŠ¤ ë§ˆì§€ë§‰ ë‚ ì§œë¶€í„° ì—­ìˆœìœ¼ë¡œ)
        for i in range(seq_len):
            try:
                # ì‹œí€€ìŠ¤ ë§ˆì§€ë§‰ ë‚ ì§œì—ì„œ ê±°ê¾¸ë¡œ ê³„ì‚°
                date = sequence_end_date - timedelta(days=seq_len-i-1)
                date_labels.append(format_date(date, '%Y-%m-%d'))
            except:
                # ë‚ ì§œ ë³€í™˜ ì˜¤ë¥˜ ì‹œ ì¸ë±ìŠ¤ ì‚¬ìš©
                date_labels.append(f"T-{seq_len-i-1}")
    
    # GridSpecì„ ì‚¬ìš©í•œ ë ˆì´ì•„ì›ƒ ìƒì„± - ìƒë‹¨ 2ê°œ, í•˜ë‹¨ 1ê°œ í° ê·¸ë˜í”„
    fig = plt.figure(figsize=(24, 18))
    gs = GridSpec(2, 2, height_ratios=[1, 1.2], figure=fig)
    # ì˜ˆì¸¡ ì‹œì‘ì¼ ê³„ì‚° (ì‹œí€€ìŠ¤ ë§ˆì§€ë§‰ ë‚ ì§œ ë‹¤ìŒë‚ )
    prediction_date = sequence_end_date + timedelta(days=1)
    fig.suptitle(f"Attention Weight Analysis for Prediction {format_date(prediction_date, '%Y-%m-%d')}", 
                fontsize=24, fontweight='bold')
    
    # ì „ì²´ í°íŠ¸ í¬ê¸° ì„¤ì •
    plt.rcParams.update({'font.size': 16})
    
    # íŠ¹ì„± ì¤‘ìš”ë„ ê³„ì‚°ì„ ìœ„í•´ ë°ì´í„° ì¤€ë¹„
    feature_importance = np.zeros(len(feature_names))
    
    # íŠ¹ì„± ì¤‘ìš”ë„ë¥¼ ê°„ë‹¨í•œ ë°©ë²•ìœ¼ë¡œ ê³„ì‚°
    # ë§ˆì§€ë§‰ ì‹œì ì—ì„œ ê° íŠ¹ì„±ì˜ ì ˆëŒ€ê°’ ì‚¬ìš©
    feature_importance = np.mean(np.abs(features[0].cpu().numpy()), axis=0)
    
    # ì •ê·œí™”
    if np.sum(feature_importance) > 0:
        feature_importance = feature_importance / np.sum(feature_importance)
    
    # íŠ¹ì„± ì¤‘ìš”ë„ë¥¼ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬
    sorted_idx = np.argsort(feature_importance)[::-1]
    sorted_features = [feature_names[i] for i in sorted_idx]
    sorted_importance = feature_importance[sorted_idx]
    
    # í”Œë¡¯ 1: ì‹œê°„ì  ì¤‘ìš”ë„ (Time Step Importance) - ìƒë‹¨ ì™¼ìª½
    ax1 = fig.add_subplot(gs[0, 0])
    
    # ê° ì‹œì ì˜ í‰ê·  ì ˆëŒ€ê°’ìœ¼ë¡œ ì‹œê°„ì  ì¤‘ìš”ë„ ì¶”ì •
    temporal_importance = np.mean(np.abs(features[0].cpu().numpy()), axis=1)
    if np.sum(temporal_importance) > 0:
        temporal_importance = temporal_importance / np.sum(temporal_importance)
    
    try:
        # ë§‰ëŒ€ê·¸ë˜í”„ë¡œ ì‹œê°„ì  ì¤‘ìš”ë„ í‘œì‹œ
        bars = ax1.bar(range(len(date_labels)), temporal_importance, color='skyblue', alpha=0.7)
        
        # Xì¶• ë¼ë²¨ ê°„ê²© ì¡°ì • - ë„ˆë¬´ ë§ìœ¼ë©´ ì¼ë¶€ë§Œ í‘œì‹œ
        if len(date_labels) > 20:
            # 20ê°œ ì´ìƒì´ë©´ 7ê°œ ê°„ê²©ìœ¼ë¡œ í‘œì‹œ
            step = max(1, len(date_labels) // 7)
            tick_indices = list(range(0, len(date_labels), step))
            # ë§ˆì§€ë§‰ ë‚ ì§œë„ í¬í•¨
            if tick_indices[-1] != len(date_labels) - 1:
                tick_indices.append(len(date_labels) - 1)
            ax1.set_xticks(tick_indices)
            ax1.set_xticklabels([date_labels[i] for i in tick_indices], rotation=45, ha='right', fontsize=14)
        elif len(date_labels) > 10:
            # 10-20ê°œë©´ 3ê°œ ê°„ê²©ìœ¼ë¡œ í‘œì‹œ
            step = max(1, len(date_labels) // 5)
            tick_indices = list(range(0, len(date_labels), step))
            if tick_indices[-1] != len(date_labels) - 1:
                tick_indices.append(len(date_labels) - 1)
            ax1.set_xticks(tick_indices)
            ax1.set_xticklabels([date_labels[i] for i in tick_indices], rotation=45, ha='right', fontsize=14)
        else:
            # 10ê°œ ì´í•˜ë©´ ëª¨ë‘ í‘œì‹œ
            ax1.set_xticks(range(len(date_labels)))
            ax1.set_xticklabels(date_labels, rotation=45, ha='right', fontsize=14)
            
        ax1.set_title("Time Step Importance", fontsize=18, fontweight='bold', pad=20)
        ax1.set_xlabel("Sequence Dates", fontsize=16, fontweight='bold')
        ax1.set_ylabel("Relative Importance", fontsize=16, fontweight='bold')
        ax1.tick_params(axis='both', which='major', labelsize=14)
        
        # ë§ˆì§€ë§‰ ì‹œì  ê°•ì¡°
        ax1.bar(len(date_labels)-1, temporal_importance[-1], color='red', alpha=0.7)
        
        # ê·¸ë¦¬ë“œ ì¶”ê°€
        ax1.grid(True, alpha=0.3)
    except Exception as e:
        logger.error(f"ì‹œê°„ì  ì¤‘ìš”ë„ ì‹œê°í™” ì˜¤ë¥˜: {str(e)}")
        ax1.text(0.5, 0.5, "Visualization error", ha='center', va='center', fontsize=16)
    
    # í”Œë¡¯ 2: íŠ¹ì„±ë³„ ì¤‘ìš”ë„ (Feature Importance) - ìƒë‹¨ ì˜¤ë¥¸ìª½
    ax2 = fig.add_subplot(gs[0, 1])
    
    # ìƒìœ„ 10ê°œ íŠ¹ì„±ë§Œ í‘œì‹œ
    top_n = min(10, len(sorted_features))
    
    try:
        # ìˆ˜í‰ ë§‰ëŒ€ ì°¨íŠ¸ë¡œ í‘œì‹œ
        y_pos = range(top_n)
        bars = ax2.barh(y_pos, sorted_importance[:top_n], color='lightgreen', alpha=0.7)
        ax2.set_yticks(y_pos)
        ax2.set_yticklabels(sorted_features[:top_n], fontsize=14)
        ax2.set_title("Feature Importance", fontsize=18, fontweight='bold', pad=20)
        ax2.set_xlabel("Relative Importance", fontsize=16, fontweight='bold')
        ax2.tick_params(axis='both', which='major', labelsize=14)
        
        # ì¤‘ìš”ë„ ê°’ í‘œì‹œ
        for i, bar in enumerate(bars):
            width = bar.get_width()
            ax2.text(width + 0.01, bar.get_y() + bar.get_height()/2, 
                    f"{width:.3f}", va='center', fontsize=13, fontweight='bold')
        
        # ê·¸ë¦¬ë“œ ì¶”ê°€
        ax2.grid(True, alpha=0.3, axis='x')
    except Exception as e:
        logger.error(f"íŠ¹ì„± ì¤‘ìš”ë„ ì‹œê°í™” ì˜¤ë¥˜: {str(e)}")
        ax2.text(0.5, 0.5, "Visualization error", ha='center', va='center', fontsize=16)
    
    # í”Œë¡¯ 3: ìƒìœ„ íŠ¹ì„±ë“¤ì˜ ì‹œê³„ì—´ ê·¸ë˜í”„ (Top Features Time Series) - í•˜ë‹¨ ì „ì²´
    ax3 = fig.add_subplot(gs[1, :])
    
    try:
        # ìƒìœ„ 8ê°œ íŠ¹ì„± ì‚¬ìš© (ë” ë§ì€ íŠ¹ì„±ì„ ë³´ì—¬ì¤„ ìˆ˜ ìˆìŒ)
        top_n_series = min(8, len(sorted_features))
        colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f']
        
        for i in range(top_n_series):
            feature_idx = sorted_idx[i]
            feature_name = feature_names[feature_idx]
            
            # í•´ë‹¹ íŠ¹ì„±ì˜ ì‹œê³„ì—´ ë°ì´í„°
            feature_data = features[0, :, feature_idx].cpu().numpy()
            
            # min-max ì •ê·œí™”ë¡œ ëª¨ë“  íŠ¹ì„±ì„ ê°™ì€ ìŠ¤ì¼€ì¼ë¡œ í‘œì‹œ
            feature_min = feature_data.min()
            feature_max = feature_data.max()
            if feature_max > feature_min:  # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
                norm_data = (feature_data - feature_min) / (feature_max - feature_min)
            else:
                norm_data = np.zeros_like(feature_data)
            
            # íŠ¹ì„± ì¤‘ìš”ë„ì— ë¹„ë¡€í•˜ëŠ” ì„  ë‘ê»˜
            line_width = 2 + sorted_importance[i] * 6
            
            # í”Œë¡¯
            ax3.plot(range(len(date_labels)), norm_data, 
                    label=f"{feature_name[:20]}... ({sorted_importance[i]:.3f})" if len(feature_name) > 20 else f"{feature_name} ({sorted_importance[i]:.3f})",
                    linewidth=line_width, color=colors[i % len(colors)], alpha=0.8, marker='o', markersize=4)
        
        ax3.set_title("Top Features Time Series (Normalized)", fontsize=20, fontweight='bold', pad=25)
        ax3.set_xlabel("Time Steps", fontsize=18, fontweight='bold')
        ax3.set_ylabel("Normalized Value", fontsize=18, fontweight='bold')
        ax3.legend(fontsize=14, loc='best', ncol=2)  # 2ì—´ë¡œ ë²”ë¡€ í‘œì‹œ
        ax3.grid(True, linestyle='--', alpha=0.5)
        ax3.tick_params(axis='both', which='major', labelsize=15)
        
        # xì¶• ë¼ë²¨ì„ ê°„ì†Œí™” (ë„ˆë¬´ ë§ìœ¼ë©´ ê°€ë…ì„± ë–¨ì–´ì§)
        if len(date_labels) > 20:
            # 20ê°œ ì´ìƒì´ë©´ 7ê°œ ê°„ê²©ìœ¼ë¡œ í‘œì‹œ
            step = max(1, len(date_labels) // 7)
            tick_indices = list(range(0, len(date_labels), step))
            # ë§ˆì§€ë§‰ ë‚ ì§œë„ í¬í•¨
            if tick_indices[-1] != len(date_labels) - 1:
                tick_indices.append(len(date_labels) - 1)
            ax3.set_xticks(tick_indices)
            ax3.set_xticklabels([date_labels[i] for i in tick_indices], 
                              rotation=45, ha='right', fontsize=14)
        elif len(date_labels) > 10:
            # 10-20ê°œë©´ 5ê°œ ê°„ê²©ìœ¼ë¡œ í‘œì‹œ
            step = max(1, len(date_labels) // 5)
            tick_indices = list(range(0, len(date_labels), step))
            if tick_indices[-1] != len(date_labels) - 1:
                tick_indices.append(len(date_labels) - 1)
            ax3.set_xticks(tick_indices)
            ax3.set_xticklabels([date_labels[i] for i in tick_indices], 
                              rotation=45, ha='right', fontsize=14)
        else:
            # 10ê°œ ì´í•˜ë©´ ëª¨ë‘ í‘œì‹œ
            ax3.set_xticks(range(len(date_labels)))
            ax3.set_xticklabels(date_labels, rotation=45, ha='right', fontsize=14)
            
    except Exception as e:
        logger.error(f"ì‹œê³„ì—´ ì‹œê°í™” ì˜¤ë¥˜: {str(e)}")
        ax3.text(0.5, 0.5, "Visualization error", ha='center', va='center', fontsize=18)
    

    
    plt.tight_layout(pad=3.0)
    
    # ì´ë¯¸ì§€ë¥¼ ë©”ëª¨ë¦¬ì— ì €ì¥
    img_buf = io.BytesIO()
    plt.savefig(img_buf, format='png', dpi=300, bbox_inches='tight')
    plt.close()
    img_buf.seek(0)
    
    # Base64ë¡œ ì¸ì½”ë”©
    img_str = base64.b64encode(img_buf.read()).decode('utf-8')
    
    # íŒŒì¼ ì €ì¥ - íŒŒì¼ë³„ ìºì‹œ ë””ë ‰í† ë¦¬ ì‚¬ìš©
    try:
        cache_dirs = get_file_cache_dirs()  # í˜„ì¬ íŒŒì¼ì˜ ìºì‹œ ë””ë ‰í† ë¦¬ ê°€ì ¸ì˜¤ê¸°
        attn_dir = cache_dirs['plots']  # plots ë””ë ‰í† ë¦¬ì— ì €ì¥
        
        filename = os.path.join(attn_dir, f"attention_{format_date(prediction_date, '%Y%m%d')}.png")
        with open(filename, 'wb') as f:
            f.write(base64.b64decode(img_str))
    except Exception as e:
        logger.error(f"Error saving attention image: {str(e)}")
        filename = None
    
    return filename, img_str, {
        'feature_importance': dict(zip(sorted_features, sorted_importance.tolist())),
        'temporal_importance': dict(zip(date_labels, temporal_importance.tolist()))
    }

def plot_prediction_basic(sequence_df, prediction_start_date, start_day_value, 
                         f1, accuracy, mape, weighted_score_pct, 
                         current_date=None,  # ğŸ”‘ ì¶”ê°€: ë°ì´í„° ì»·ì˜¤í”„ ë‚ ì§œ
                         save_prefix=None, title_prefix="Basic Prediction Graph",
                         y_min=None, y_max=None, file_path=None):
    """
    ê¸°ë³¸ ì˜ˆì¸¡ ê·¸ë˜í”„ ì‹œê°í™” - ê³¼ê±°/ë¯¸ë˜ ëª…í™• êµ¬ë¶„
    ğŸ”‘ current_date ì´í›„ëŠ” ë¯¸ë˜ ì˜ˆì¸¡ìœ¼ë¡œë§Œ í‘œì‹œ (ë°ì´í„° ëˆ„ì¶œ ë°©ì§€)
    """
    
    fig = None
    
    try:
        logger.info(f"Creating prediction graph for prediction starting {format_date(prediction_start_date)}")
        
        # ğŸ“ ì €ì¥ ë””ë ‰í† ë¦¬ ì„¤ì • (íŒŒì¼ë³„ ìºì‹œ ë””ë ‰í† ë¦¬ ì‚¬ìš©)
        if save_prefix is None:
            try:
                cache_dirs = get_file_cache_dirs(file_path)
                save_dir = cache_dirs['plots']
            except Exception as e:
                logger.warning(f"Could not get cache directories for plots: {str(e)}")
                save_dir = Path("temp_plots")
                save_dir.mkdir(parents=True, exist_ok=True)
        else:
            save_dir = Path(save_prefix)
            save_dir.mkdir(parents=True, exist_ok=True)
        
        # DataFrameì˜ ë‚ ì§œ ì—´ì´ ë¬¸ìì—´ì¸ ê²½ìš° ë‚ ì§œ ê°ì²´ë¡œ ë³€í™˜
        if 'Date' in sequence_df.columns and isinstance(sequence_df['Date'].iloc[0], str):
            sequence_df['Date'] = pd.to_datetime(sequence_df['Date'])
        
        # âœ… current_date ê¸°ì¤€ìœ¼ë¡œ ê³¼ê±°/ë¯¸ë˜ ë¶„í• 
        if current_date is not None:
            current_date = pd.to_datetime(current_date)
            
            # ê³¼ê±° ë°ì´í„° (current_date ì´ì „): ì‹¤ì œê°’ê³¼ ì˜ˆì¸¡ê°’ ëª¨ë‘ í‘œì‹œ ê°€ëŠ¥
            past_df = sequence_df[sequence_df['Date'] <= current_date].copy()
            # ë¯¸ë˜ ë°ì´í„° (current_date ì´í›„): ì˜ˆì¸¡ê°’ë§Œ í‘œì‹œ
            future_df = sequence_df[sequence_df['Date'] > current_date].copy()
            
            # ê³¼ê±° ë°ì´í„°ì—ì„œ ì‹¤ì œê°’ì´ ìˆëŠ” ê²ƒë§Œ ê²€ì¦ìš©ìœ¼ë¡œ ì‚¬ìš©
            valid_df = past_df.dropna(subset=['Actual']) if 'Actual' in past_df.columns else pd.DataFrame()
            
            logger.info(f"  ğŸ“Š Data split - Past: {len(past_df)}, Future: {len(future_df)}, Validation: {len(valid_df)}")
        else:
            # current_dateê°€ ì—†ìœ¼ë©´ ê¸°ì¡´ ë°©ì‹ ì‚¬ìš© (í•˜ìœ„ í˜¸í™˜ì„±)
            valid_df = sequence_df.dropna(subset=['Actual']) if 'Actual' in sequence_df.columns else pd.DataFrame()
            future_df = sequence_df
            past_df = valid_df
        
        pred_df = sequence_df.dropna(subset=['Prediction'])
        
        # ê·¸ë˜í”„ ìƒì„±
        fig = plt.figure(figsize=(16, 9))
        plt.subplots_adjust(top=0.85)
        gs = GridSpec(2, 1, height_ratios=[3, 1], hspace=0.3, figure=fig)
        
        # ê·¸ë˜í”„ íƒ€ì´í‹€ê³¼ ì„œë¸Œíƒ€ì´í‹€
        if isinstance(prediction_start_date, str):
            main_title = f"{title_prefix} - Start: {prediction_start_date}"
        else:
            main_title = f"{title_prefix} - Start: {prediction_start_date.strftime('%Y-%m-%d')}"
        
        # âœ… ê³¼ê±°/ë¯¸ë˜ êµ¬ë¶„ ì •ë³´ê°€ í¬í•¨ëœ ì„œë¸Œíƒ€ì´í‹€
        if current_date is not None:
            validation_count = len(valid_df)
            future_count = len(future_df)
            subtitle = f"Data Cutoff: {current_date.strftime('%Y-%m-%d')} | Validation: {validation_count} pts | Future: {future_count} pts"
            if validation_count > 0:
                subtitle += f" | F1: {f1:.3f}, Acc: {accuracy:.1f}%, MAPE: {mape:.1f}%"
        else:
            # ê¸°ì¡´ ë°©ì‹
            if f1 == 0 and accuracy == 0 and mape == 0 and weighted_score_pct == 0:
                subtitle = "Future Prediction Only (No Validation Data Available)"
            else:
                subtitle = f"F1: {f1:.2f}, Acc: {accuracy:.2f}%, MAPE: {mape:.2f}%, Score: {weighted_score_pct:.2f}%"

        fig.text(0.5, 0.94, main_title, ha='center', fontsize=14, weight='bold')
        fig.text(0.5, 0.90, subtitle, ha='center', fontsize=12)
        
        # (1) ìƒë‹¨: ê°€ê²© ì˜ˆì¸¡ ê·¸ë˜í”„
        ax1 = fig.add_subplot(gs[0])
        ax1.set_title("Price Prediction: Past Validation vs Future Forecast", fontsize=13)
        ax1.grid(True, linestyle='--', alpha=0.5)

        if y_min is not None and y_max is not None:
            ax1.set_ylim(y_min, y_max)
        
        # ì˜ˆì¸¡ ì‹œì‘ ë‚ ì§œ ì²˜ë¦¬
        if isinstance(prediction_start_date, str):
            start_date = pd.to_datetime(prediction_start_date)
        else:
            start_date = prediction_start_date
        
        # ì‹œì‘ì¼ ì´ì „ ë‚ ì§œ ê³„ì‚° (ì—°ê²°ì ìš©)
        prev_date = start_date - pd.Timedelta(days=1)
        while prev_date.weekday() >= 5 or is_holiday(prev_date):
            prev_date -= pd.Timedelta(days=1)
        
        # âœ… 1. ê³¼ê±° ì‹¤ì œê°’ (íŒŒë€ìƒ‰ ì‹¤ì„ ) - ê°€ì¥ ì¤‘ìš”í•œ ê¸°ì¤€ì„ 
        if not valid_df.empty:
            real_dates = [prev_date] + valid_df['Date'].tolist()
            real_values = [start_day_value] + valid_df['Actual'].tolist()
            ax1.plot(real_dates, real_values, marker='o', color='blue', 
                    label='Actual (Past)', linewidth=2.5, markersize=5, zorder=3)
        
        # âœ… 2. ê³¼ê±° ì˜ˆì¸¡ê°’ (íšŒìƒ‰ ì ì„ ) - ëª¨ë¸ ì„±ëŠ¥ í™•ì¸ìš©
        if not valid_df.empty:
            past_pred_dates = [prev_date] + valid_df['Date'].tolist()
            past_pred_values = [start_day_value] + valid_df['Prediction'].tolist()
            ax1.plot(past_pred_dates, past_pred_values, marker='x', color='gray', 
                    label='Predicted (Past)', linewidth=1.5, linestyle=':', markersize=4, alpha=0.8, zorder=2)
        
        # âœ… 3. ë¯¸ë˜ ì˜ˆì¸¡ê°’ (ë¹¨ê°„ìƒ‰ ì ì„ ) - í•µì‹¬ ì˜ˆì¸¡
        if not future_df.empty:
            future_dates = future_df['Date'].tolist()
            future_values = future_df['Prediction'].tolist()
            
            # ì—°ê²°ì„  (ë§ˆì§€ë§‰ ì‹¤ì œê°’ â†’ ì²« ë¯¸ë˜ ì˜ˆì¸¡ê°’)
            if not valid_df.empty and future_dates:
                # ë§ˆì§€ë§‰ ê²€ì¦ ë°ì´í„°ì˜ ì‹¤ì œê°’ì—ì„œ ì²« ë¯¸ë˜ ì˜ˆì¸¡ìœ¼ë¡œ ì—°ê²°
                connection_x = [valid_df['Date'].iloc[-1], future_dates[0]]
                connection_y = [valid_df['Actual'].iloc[-1], future_values[0]]
                ax1.plot(connection_x, connection_y, '--', color='orange', alpha=0.6, linewidth=1.5, zorder=1)
            elif start_day_value is not None and future_dates:
                # ê²€ì¦ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì‹œì‘ê°’ì—ì„œ ì—°ê²°
                connection_x = [prev_date, future_dates[0]]
                connection_y = [start_day_value, future_values[0]]
                ax1.plot(connection_x, connection_y, '--', color='orange', alpha=0.6, linewidth=1.5, zorder=1)
            
            ax1.plot(future_dates, future_values, marker='o', color='red', 
                    label='Predicted (Future)', linewidth=2.5, linestyle='--', markersize=5, zorder=3)
        
        # âœ… 4. ë°ì´í„° ì»·ì˜¤í”„ ë¼ì¸ (ì´ˆë¡ìƒ‰ ì„¸ë¡œì„ )
        if current_date is not None:
            ax1.axvline(x=current_date, color='green', linestyle='-', alpha=0.8, 
                       linewidth=2.5, label=f'Data Cutoff', zorder=4)
            
            # ì»·ì˜¤í”„ ë‚ ì§œ í…ìŠ¤íŠ¸ ì¶”ê°€
            ax1.text(current_date, ax1.get_ylim()[1] * 0.95, 
                    f'{current_date.strftime("%m/%d")}', 
                    ha='center', va='top', fontsize=10, 
                    bbox=dict(boxstyle="round,pad=0.3", facecolor='lightgreen', alpha=0.7))
        else:
            # ì˜ˆì¸¡ ì‹œì‘ì ì— ìˆ˜ì§ì„  í‘œì‹œ (ê¸°ì¡´ ë°©ì‹)
            ax1.axvline(x=start_date, color='green', linestyle='--', alpha=0.7, 
                       linewidth=2, label='Prediction Start', zorder=4)
        
        # âœ… 5. ë°°ê²½ ìƒ‰ì¹  (ë°©í–¥ì„± ì¼ì¹˜ ì—¬ë¶€) - ê²€ì¦ ë°ì´í„°ë§Œ
        if not valid_df.empty and len(valid_df) > 1:
            for i in range(len(valid_df) - 1):
                curr_date = valid_df['Date'].iloc[i]
                next_date = valid_df['Date'].iloc[i + 1]
                
                curr_actual = valid_df['Actual'].iloc[i]
                next_actual = valid_df['Actual'].iloc[i + 1]
                curr_pred = valid_df['Prediction'].iloc[i]
                next_pred = valid_df['Prediction'].iloc[i + 1]
                
                # ë°©í–¥ ê³„ì‚°
                actual_dir = np.sign(next_actual - curr_actual)
                pred_dir = np.sign(next_pred - curr_pred)
                
                # ë°©í–¥ ì¼ì¹˜ ì—¬ë¶€ì— ë”°ë¥¸ ìƒ‰ìƒ
                color = 'lightblue' if actual_dir == pred_dir else 'lightcoral'
                ax1.axvspan(curr_date, next_date, color=color, alpha=0.15, zorder=0)
        
        ax1.set_xlabel("")
        ax1.set_ylabel("Price (USD/MT)", fontsize=11)
        ax1.legend(loc='upper left', fontsize=10)
        ax1.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d'))
        
        # âœ… (2) í•˜ë‹¨: ì˜¤ì°¨ ë¶„ì„ - ê²€ì¦ ë°ì´í„°ë§Œ ë˜ëŠ” ë³€í™”ëŸ‰
        ax2 = fig.add_subplot(gs[1])
        ax2.grid(True, linestyle='--', alpha=0.5)
        
        if not valid_df.empty and len(valid_df) > 0:
            # ê²€ì¦ ë°ì´í„°ì˜ ì ˆëŒ€ ì˜¤ì°¨
            error_dates = valid_df['Date'].tolist()
            error_values = [abs(row['Actual'] - row['Prediction']) for _, row in valid_df.iterrows()]
            
            if error_dates and error_values:
                bars = ax2.bar(error_dates, error_values, width=0.6, color='salmon', alpha=0.7, edgecolor='darkred', linewidth=0.5)
                ax2.set_title(f"Prediction Error - Validation Period ({len(error_dates)} points)", fontsize=11)
                
                # í‰ê·  ì˜¤ì°¨ ë¼ì¸
                avg_error = np.mean(error_values)
                ax2.axhline(y=avg_error, color='red', linestyle='--', alpha=0.8, 
                           label=f'Avg Error: {avg_error:.2f}')
                ax2.legend(fontsize=9)
            else:
                ax2.text(0.5, 0.5, "No validation errors to display", 
                        ha='center', va='center', transform=ax2.transAxes, fontsize=10)
                ax2.set_title("Error Analysis")
        else:
            # ì‹¤ì œê°’ì´ ì—†ëŠ” ê²½ìš°: ë¯¸ë˜ ì˜ˆì¸¡ì˜ ì¼ì¼ ë³€í™”ëŸ‰ í‘œì‹œ
            if not future_df.empty and len(future_df) > 1:
                change_dates = future_df['Date'].iloc[1:].tolist()
                change_values = np.diff(future_df['Prediction'].values)
                
                # ìƒìŠ¹/í•˜ë½ì— ë”°ë¥¸ ìƒ‰ìƒ êµ¬ë¶„
                colors = ['green' if change >= 0 else 'red' for change in change_values]
                
                bars = ax2.bar(change_dates, change_values, width=0.6, color=colors, alpha=0.7)
                ax2.set_title("Daily Price Changes - Future Predictions", fontsize=11)
                ax2.axhline(y=0, color='black', linestyle='-', alpha=0.5, linewidth=1)
                
                # ë²”ë¡€ ì¶”ê°€
                from matplotlib.patches import Patch
                legend_elements = [Patch(facecolor='green', alpha=0.7, label='Price Up'),
                                 Patch(facecolor='red', alpha=0.7, label='Price Down')]
                ax2.legend(handles=legend_elements, fontsize=9)
            else:
                ax2.text(0.5, 0.5, "Insufficient data for change analysis", 
                        ha='center', va='center', transform=ax2.transAxes, fontsize=10)
                ax2.set_title("Change Analysis")
        
        ax2.set_xlabel("Date", fontsize=11)
        ax2.set_ylabel("Value", fontsize=11)
        ax2.xaxis.set_major_locator(mdates.AutoDateLocator())
        ax2.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d'))
        plt.setp(ax2.xaxis.get_majorticklabels(), rotation=45, ha='right')
        
        plt.tight_layout()
        
        # íŒŒì¼ ê²½ë¡œ ìƒì„±
        if isinstance(prediction_start_date, str):
            date_str = pd.to_datetime(prediction_start_date).strftime('%Y%m%d')
        else:
            date_str = prediction_start_date.strftime('%Y%m%d')
        
        filename = f"prediction_start_{date_str}.png"
        full_path = save_dir / filename
        
        # ì´ë¯¸ì§€ë¥¼ ë©”ëª¨ë¦¬ì— ì €ì¥
        img_buf = io.BytesIO()
        plt.savefig(img_buf, format='png', dpi=300, bbox_inches='tight')
        img_buf.seek(0)
        
        # Base64ë¡œ ì¸ì½”ë”©
        img_str = base64.b64encode(img_buf.read()).decode('utf-8')
        
        # íŒŒì¼ë¡œ ì €ì¥
        plt.savefig(str(full_path), dpi=300, bbox_inches='tight')
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        plt.close(fig)
        plt.clf()
        img_buf.close()
        
        logger.info(f"Enhanced prediction graph saved: {full_path}")
        logger.info(f"  - Past validation points: {len(valid_df) if not valid_df.empty else 0}")
        logger.info(f"  - Future prediction points: {len(future_df) if not future_df.empty else 0}")
        
        return str(full_path), img_str
        
    except Exception as e:
        if fig is not None:
            plt.close(fig)
        plt.close('all')
        plt.clf()
        
        logger.error(f"Error in enhanced graph creation: {str(e)}")
        logger.error(traceback.format_exc())
        return None, None
    
def plot_moving_average_analysis(ma_results, sequence_start_date, save_prefix=None,
                               title_prefix="Moving Average Analysis", y_min=None, y_max=None, file_path=None):
    """ì´ë™í‰ê·  ë¶„ì„ ì‹œê°í™”"""
    try:
        # ì…ë ¥ ë°ì´í„° ê²€ì¦
        if not ma_results or len(ma_results) == 0:
            logger.warning("No moving average results to plot")
            return None, None
            
        # ma_results í˜•ì‹: {'ma5': [{'date': '...', 'prediction': X, 'actual': Y, 'ma': Z}, ...], 'ma10': [...]}
        windows = sorted(ma_results.keys())
        
        if len(windows) == 0:
            logger.warning("No moving average windows found")
            return None, None
        
        # ìœ íš¨í•œ ìœˆë„ìš° í•„í„°ë§
        valid_windows = []
        for window_key in windows:
            if window_key in ma_results and ma_results[window_key] and len(ma_results[window_key]) > 0:
                valid_windows.append(window_key)
        
        if len(valid_windows) == 0:
            logger.warning("No valid moving average data found")
            return None, None
        
        fig = plt.figure(figsize=(12, max(4, 4 * len(valid_windows))))
        
        if isinstance(sequence_start_date, str):
            title = f"{title_prefix} Starting {sequence_start_date}"
        else:
            title = f"{title_prefix} Starting {sequence_start_date.strftime('%Y-%m-%d')}"
            
        fig.suptitle(title, fontsize=16)
        
        for idx, window_key in enumerate(valid_windows):
            window_num = window_key.replace('ma', '')
            ax = fig.add_subplot(len(valid_windows), 1, idx+1)
            
            window_data = ma_results[window_key]
            
            # ë°ì´í„° ê²€ì¦
            if not window_data or len(window_data) == 0:
                ax.text(0.5, 0.5, f"No data for {window_key}", 
                       ha='center', va='center', transform=ax.transAxes)
                continue
            
            # ë‚ ì§œ, ì˜ˆì¸¡, ì‹¤ì œê°’, MA ì¶”ì¶œ
            dates = []
            predictions = []
            actuals = []
            ma_preds = []
            
            for item in window_data:
                try:
                    # ì•ˆì „í•œ ë°ì´í„° ì¶”ì¶œ
                    if isinstance(item['date'], str):
                        dates.append(pd.to_datetime(item['date']))
                    else:
                        dates.append(item['date'])
                    
                    # None ê°’ ì²˜ë¦¬
                    predictions.append(item.get('prediction', 0))
                    actuals.append(item.get('actual', None))
                    ma_preds.append(item.get('ma', None))
                except Exception as e:
                    logger.warning(f"Error processing MA data item: {str(e)}")
                    continue
            
            if len(dates) == 0:
                ax.text(0.5, 0.5, f"No valid data for {window_key}", 
                       ha='center', va='center', transform=ax.transAxes)
                continue
            
            # yì¶• ë²”ìœ„ ì„¤ì •
            if y_min is not None and y_max is not None:
                ax.set_ylim(y_min, y_max)
            
            # ì›ë³¸ ì‹¤ì œê°’ vs ì˜ˆì¸¡ê°’ (ì˜…ê²Œ)
            ax.plot(dates, actuals, marker='o', color='blue', alpha=0.3, label='Actual')
            ax.plot(dates, predictions, marker='o', color='red', alpha=0.3, label='Predicted')
            
            # ì´ë™í‰ê· 
            # ì‹¤ì œê°’(actuals)ê³¼ ì´ë™í‰ê· (ma_preds) ëª¨ë‘ Noneì´ ì•„ë‹Œ ì¸ë±ìŠ¤ë¥¼ ì„ íƒ
            valid_indices = [
                i for i in range(len(ma_preds))
                if (ma_preds[i] is not None and actuals[i] is not None)
            ]

            if valid_indices:
                valid_dates = [dates[i] for i in valid_indices]
                valid_ma = [ma_preds[i] for i in valid_indices]
                valid_actuals = [actuals[i] for i in valid_indices]
                
                # ë°°ì—´ë¡œ ë³€í™˜
                valid_actuals_arr = np.array(valid_actuals)
                valid_ma_arr = np.array(valid_ma)
                
                # ì‹¤ì œê°’ì´ 0ì¸ í•­ëª©ì€ ì œì™¸í•˜ì—¬ MAPE ê³„ì‚°
                non_zero_mask = valid_actuals_arr != 0
                if np.sum(non_zero_mask) > 0:
                    ma_mape = np.mean(np.abs((valid_actuals_arr[non_zero_mask] - valid_ma_arr[non_zero_mask]) /
                                           valid_actuals_arr[non_zero_mask])) * 100
                else:
                    ma_mape = 0.0
                
                ax.set_title(f"MA-{window_num} Analysis (MAPE: {ma_mape:.2f}%, Count: {len(valid_indices)})")
            else:
                ax.set_title(f"MA-{window_num} Analysis (Insufficient data)")
            
            ax.legend()
            ax.grid(True, linestyle='--', alpha=0.5)
            plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')
            ax.set_xlabel("Date")
            ax.set_ylabel("Price")
        
        plt.tight_layout()
        
        # ğŸ“ ì €ì¥ ë””ë ‰í† ë¦¬ ì„¤ì • (íŒŒì¼ë³„ ìºì‹œ ë””ë ‰í† ë¦¬ ì‚¬ìš©)
        if save_prefix is None:
            try:
                cache_dirs = get_file_cache_dirs(file_path)
                save_dir = cache_dirs['ma_plots']
            except Exception as e:
                logger.warning(f"Could not get cache directories for MA plots: {str(e)}")
                save_dir = Path("temp_ma_plots")
                save_dir.mkdir(parents=True, exist_ok=True)
        else:
            save_dir = Path(save_prefix)
            save_dir.mkdir(parents=True, exist_ok=True)
        
        if isinstance(sequence_start_date, str):
            date_str = pd.to_datetime(sequence_start_date).strftime('%Y%m%d')
        else:
            date_str = sequence_start_date.strftime('%Y%m%d')
            
        filename = save_dir / f"ma_analysis_{date_str}.png"
        
        # ì´ë¯¸ì§€ë¥¼ ë©”ëª¨ë¦¬ì— ì €ì¥
        img_buf = io.BytesIO()
        plt.savefig(img_buf, format='png', dpi=300)
        img_buf.seek(0)
        
        # Base64ë¡œ ì¸ì½”ë”©
        img_str = base64.b64encode(img_buf.read()).decode('utf-8')
        
        # íŒŒì¼ ì €ì¥
        plt.savefig(str(filename), dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info(f"Moving Average graph saved: {filename}")
        return str(filename), img_str
        
    except Exception as e:
        logger.error(f"Error in moving average visualization: {str(e)}")
        logger.error(traceback.format_exc())
        return None, None

def compute_performance_metrics_improved(validation_data, start_day_value):
    """
    ê²€ì¦ ë°ì´í„°ë§Œì„ ì‚¬ìš©í•œ ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
    """
    try:
        if not validation_data or len(validation_data) < 1:
            logger.info("No validation data available - this is normal for pure future predictions")
            return None
        
        # start_day_value ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
        if hasattr(start_day_value, 'iloc'):  # pandas Series/DataFrameì¸ ê²½ìš°
            start_val = float(start_day_value.iloc[0] if len(start_day_value) > 0 else start_day_value)
        elif hasattr(start_day_value, 'item'):  # numpy scalarì¸ ê²½ìš°
            start_val = float(start_day_value.item())
        else:
            start_val = float(start_day_value)
        
        # ê²€ì¦ ë°ì´í„°ì—ì„œ ê°’ ì¶”ì¶œ (DataFrame/Seriesë¥¼ numpyë¡œ ì•ˆì „í•˜ê²Œ ë³€í™˜)
        actual_vals = [start_val]
        pred_vals = [start_val]
        
        for item in validation_data:
            # actual ê°’ ì•ˆì „í•˜ê²Œ ì¶”ì¶œ
            actual_val = item['actual']
            if hasattr(actual_val, 'iloc'):  # pandas Series/DataFrameì¸ ê²½ìš°
                actual_val = float(actual_val.iloc[0] if len(actual_val) > 0 else actual_val)
            elif hasattr(actual_val, 'item'):  # numpy scalarì¸ ê²½ìš°
                actual_val = float(actual_val.item())
            else:
                actual_val = float(actual_val)
            actual_vals.append(actual_val)
            
            # prediction ê°’ ì•ˆì „í•˜ê²Œ ì¶”ì¶œ
            pred_val = item['prediction']
            if hasattr(pred_val, 'iloc'):  # pandas Series/DataFrameì¸ ê²½ìš°
                pred_val = float(pred_val.iloc[0] if len(pred_val) > 0 else pred_val)
            elif hasattr(pred_val, 'item'):  # numpy scalarì¸ ê²½ìš°
                pred_val = float(pred_val.item())
            else:
                pred_val = float(pred_val)
            pred_vals.append(pred_val)
        
        # F1 ì ìˆ˜ ê³„ì‚° (ê° ë‹¨ê³„ë³„ ë¡œê¹… ì¶”ê°€)
        try:
            f1, f1_report = calculate_f1_score(actual_vals, pred_vals)
        except Exception as e:
            logger.error(f"Error in F1 score calculation: {str(e)}")
            f1, f1_report = 0.0, "Error in F1 calculation"
            
        try:
            direction_accuracy = calculate_direction_accuracy(actual_vals, pred_vals)
        except Exception as e:
            logger.error(f"Error in direction accuracy calculation: {str(e)}")
            direction_accuracy = 0.0
            
        try:
            weighted_score, max_score = calculate_direction_weighted_score(actual_vals[1:], pred_vals[1:])
            weighted_score_pct = (weighted_score / max_score) * 100 if max_score > 0 else 0.0
        except Exception as e:
            logger.error(f"Error in weighted score calculation: {str(e)}")
            weighted_score_pct = 0.0
            
        try:
            mape = calculate_mape(actual_vals[1:], pred_vals[1:])
        except Exception as e:
            logger.error(f"Error in MAPE calculation: {str(e)}")
            mape = 0.0
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„
        cosine_similarity = None
        try:
            if len(actual_vals) > 1:
                # numpy ë°°ì—´ë¡œ ë³€í™˜í•˜ì—¬ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
                actual_vals_arr = np.array(actual_vals, dtype=float)
                pred_vals_arr = np.array(pred_vals, dtype=float)
                
                diff_actual = np.diff(actual_vals_arr)
                diff_pred = np.diff(pred_vals_arr)
                norm_actual = np.linalg.norm(diff_actual)
                norm_pred = np.linalg.norm(diff_pred)
                if norm_actual > 0 and norm_pred > 0:
                    cosine_similarity = np.dot(diff_actual, diff_pred) / (norm_actual * norm_pred)
        except Exception as e:
            logger.error(f"Error in cosine similarity calculation: {str(e)}")
            cosine_similarity = None
        
        return {
            'f1': float(f1),
            'accuracy': float(direction_accuracy),
            'mape': float(mape),
            'weighted_score': float(weighted_score_pct),
            'cosine_similarity': float(cosine_similarity) if cosine_similarity is not None else None,
            'f1_report': f1_report,
            'validation_points': len(validation_data)
        }
        
    except Exception as e:
        logger.error(f"Error computing improved metrics: {str(e)}")
        return None

def calculate_f1_score(actual, predicted):
    """ë°©í–¥ì„± ì˜ˆì¸¡ì˜ F1 ì ìˆ˜ ê³„ì‚°"""
    # ì…ë ¥ì„ numpy ë°°ì—´ë¡œ ë³€í™˜
    actual = np.array(actual)
    predicted = np.array(predicted)
    
    actual_directions = np.sign(np.diff(actual))
    predicted_directions = np.sign(np.diff(predicted))

    if len(actual_directions) < 2:
        return 0.0, "Insufficient data for classification report"
        
    try:
        # zero_division=0 íŒŒë¼ë¯¸í„° ì¶”ê°€
        f1 = f1_score(actual_directions, predicted_directions, average='macro', zero_division=0)
        report = classification_report(actual_directions, predicted_directions, 
                                    digits=2, zero_division=0)
    except Exception as e:
        logger.error(f"Error in calculating F1 score: {str(e)}")
        return 0.0, "Error in calculation"
        
    return f1, report

def calculate_direction_accuracy(actual, predicted):
    """ë“±ë½ ë°©í–¥ ì˜ˆì¸¡ì˜ ì •í™•ë„ ê³„ì‚°"""
    if len(actual) <= 1:
        return 0.0

    try:
        # ì…ë ¥ì„ numpy ë°°ì—´ë¡œ ë³€í™˜
        actual = np.array(actual)
        predicted = np.array(predicted)
        
        actual_directions = np.sign(np.diff(actual))
        predicted_directions = np.sign(np.diff(predicted))
        
        correct_predictions = np.sum(actual_directions == predicted_directions)
        total_predictions = len(actual_directions)
        
        accuracy = (correct_predictions / total_predictions) * 100
        return accuracy
    except Exception as e:
        logger.error(f"Error in calculating direction accuracy: {str(e)}")
        return 0.0
    
def calculate_direction_weighted_score(actual, predicted):
    """ë³€í™”ìœ¨ ê¸°ë°˜ì˜ ê°€ì¤‘ ì ìˆ˜ ê³„ì‚°"""
    if len(actual) <= 1:
        return 0.0, 1.0
        
    try:
        # ì…ë ¥ì„ numpy ë°°ì—´ë¡œ ë³€í™˜
        actual = np.array(actual)
        predicted = np.array(predicted)
        
        actual_changes = 100 * np.diff(actual) / actual[:-1]
        predicted_changes = 100 * np.diff(predicted) / predicted[:-1]

        def assign_class(change):
            if change > 6:
                return 1
            elif 4 < change <= 6:
                return 2
            elif 2 < change <= 4:
                return 3
            elif -2 <= change <= 2:
                return 4
            elif -4 <= change < -2:
                return 5
            elif -6 <= change < -4:
                return 6
            else:
                return 7

        actual_classes = np.array([assign_class(x) for x in actual_changes])
        predicted_classes = np.array([assign_class(x) for x in predicted_changes])

        score = 0
        for ac, pc in zip(actual_classes, predicted_classes):
            diff = abs(ac - pc)
            score += max(0, 3 - diff)

        max_score = 3 * len(actual_classes)
        return score, max_score
    except Exception as e:
        logger.error(f"Error in calculating weighted score: {str(e)}")
        return 0.0, 1.0

def calculate_mape(actual, predicted):
    """MAPE ê³„ì‚° í•¨ìˆ˜"""
    try:
        # ì…ë ¥ì„ numpy ë°°ì—´ë¡œ ë³€í™˜
        actual = np.array(actual)
        predicted = np.array(predicted)
        
        if len(actual) == 0:
            return 0.0
        # inf ë°©ì§€ë¥¼ ìœ„í•´ 0ì´ ì•„ë‹Œ ê°’ë§Œ ì‚¬ìš©
        mask = actual != 0
        if not np.any(mask):  # any() ëŒ€ì‹  np.any() ì‚¬ìš©
            return 0.0
        return np.mean(np.abs((actual[mask] - predicted[mask]) / actual[mask])) * 100
    except Exception as e:
        logger.error(f"Error in MAPE calculation: {str(e)}")
        return 0.0

def calculate_moving_averages_with_history(predictions, historical_data, target_col='MOPJ', windows=[5, 10, 23]):
    """ì˜ˆì¸¡ ë°ì´í„°ì™€ ê³¼ê±° ë°ì´í„°ë¥¼ ëª¨ë‘ í™œìš©í•œ ì´ë™í‰ê·  ê³„ì‚°"""
    try:
        # ì…ë ¥ ë°ì´í„° ê²€ì¦
        if not predictions or len(predictions) == 0:
            logger.warning("No predictions provided for moving average calculation")
            return {}
            
        if historical_data is None or historical_data.empty:
            logger.warning("No historical data provided for moving average calculation")
            return {}
            
        if target_col not in historical_data.columns:
            logger.warning(f"Target column {target_col} not found in historical data")
            return {}
        
        results = {}
        
        # ì˜ˆì¸¡ ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜ ë° ì •ë ¬
        try:
            pred_df = pd.DataFrame(predictions) if not isinstance(predictions, pd.DataFrame) else predictions.copy()
            
            # Date ì»¬ëŸ¼ ê²€ì¦
            if 'Date' not in pred_df.columns:
                logger.error("Date column not found in predictions")
                return {}
                
            pred_df['Date'] = pd.to_datetime(pred_df['Date'])
            pred_df = pred_df.sort_values('Date')
            
            # Prediction ì»¬ëŸ¼ ê²€ì¦
            if 'Prediction' not in pred_df.columns:
                logger.error("Prediction column not found in predictions")
                return {}
                
        except Exception as e:
            logger.error(f"Error processing prediction data: {str(e)}")
            return {}
        
        # ì˜ˆì¸¡ ì‹œì‘ì¼ í™•ì¸
        prediction_start_date = pred_df['Date'].min()
        logger.info(f"MA calculation - prediction start date: {prediction_start_date}")
        
        # ê³¼ê±° ë°ì´í„°ì—ì„œ íƒ€ê²Ÿ ì—´ ì¶”ì¶œ (ì˜ˆì¸¡ ì‹œì‘ì¼ ì´ì „)
        historical_series = pd.Series(
            data=historical_data.loc[historical_data.index < prediction_start_date, target_col],
            index=historical_data.loc[historical_data.index < prediction_start_date].index
        )
        
        # ìµœê·¼ 30ì¼ë§Œ ì‚¬ìš© (ì´ë™í‰ê·  ê³„ì‚°ì— ì¶©ë¶„)
        historical_series = historical_series.sort_index().tail(30)
        
        # ì˜ˆì¸¡ ë°ì´í„°ì—ì„œ ì‹œë¦¬ì¦ˆ ìƒì„±
        prediction_series = pd.Series(
            data=pred_df['Prediction'].values,
            index=pred_df['Date']
        )
        
        # ê³¼ê±°ì™€ ì˜ˆì¸¡ ë°ì´í„° ê²°í•©
        combined_series = pd.concat([historical_series, prediction_series])
        combined_series = combined_series[~combined_series.index.duplicated(keep='last')]
        combined_series = combined_series.sort_index()
        
        logger.info(f"Combined series for MA: {len(combined_series)} data points "
                   f"({len(historical_series)} historical, {len(prediction_series)} predicted)")
        
        # ê° ìœˆë„ìš° í¬ê¸°ë³„ ì´ë™í‰ê·  ê³„ì‚°
        for window in windows:
            # ì „ì²´ ë°ì´í„°ì— ëŒ€í•´ ì´ë™í‰ê·  ê³„ì‚°
            rolling_avg = combined_series.rolling(window=window, min_periods=1).mean()
            
            # ì˜ˆì¸¡ ê¸°ê°„ì— í•´ë‹¹í•˜ëŠ” ë¶€ë¶„ë§Œ ì¶”ì¶œ
            window_results = []
            
            for i, date in enumerate(pred_df['Date']):
                # í•´ë‹¹ ë‚ ì§œì˜ ì˜ˆì¸¡ ë° ì‹¤ì œê°’
                pred_value = pred_df['Prediction'].iloc[i]
                actual_value = pred_df['Actual'].iloc[i] if 'Actual' in pred_df.columns else None
                
                # í•´ë‹¹ ë‚ ì§œì˜ ì´ë™í‰ê·  ê°’
                ma_value = rolling_avg.loc[date] if date in rolling_avg.index else None
                
                # NaN ê°’ ì²˜ë¦¬
                if pd.isna(pred_value) or np.isnan(pred_value) or np.isinf(pred_value):
                    pred_value = None
                if pd.isna(actual_value) or np.isnan(actual_value) or np.isinf(actual_value):
                    actual_value = None
                if pd.isna(ma_value) or np.isnan(ma_value) or np.isinf(ma_value):
                    ma_value = None
                
                window_results.append({
                    'date': date,
                    'prediction': pred_value,
                    'actual': actual_value,
                    'ma': ma_value
                })
            
            results[f'ma{window}'] = window_results
            logger.info(f"MA{window} calculated: {len(window_results)} data points")
        
        logger.info(f"Moving average calculation completed with {len(results)} windows")
        return results
        
    except Exception as e:
        logger.error(f"Error calculating moving averages with history: {str(e)}")
        logger.error(traceback.format_exc())
        return {}

# 2. ì—¬ëŸ¬ ë‚ ì§œì— ëŒ€í•œ ëˆ„ì  ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜ ì¶”ê°€
def run_accumulated_predictions_with_save(file_path, start_date, end_date=None, save_to_csv=True, use_saved_data=True):
    """
    ì‹œì‘ ë‚ ì§œë¶€í„° ì¢…ë£Œ ë‚ ì§œê¹Œì§€ ê° ë‚ ì§œë³„ë¡œ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ëˆ„ì í•©ë‹ˆë‹¤. (ìˆ˜ì •ë¨)
    """
    global prediction_state

    try:
        # ìƒíƒœ ì´ˆê¸°í™”
        prediction_state['is_predicting'] = True
        prediction_state['prediction_progress'] = 5
        prediction_state['prediction_start_time'] = time.time()  # ì‹œì‘ ì‹œê°„ ê¸°ë¡
        prediction_state['error'] = None
        prediction_state['accumulated_predictions'] = []
        prediction_state['accumulated_metrics'] = {}
        prediction_state['prediction_dates'] = []
        prediction_state['accumulated_consistency_scores'] = {}
        prediction_state['current_file'] = file_path  # âœ… í˜„ì¬ íŒŒì¼ ê²½ë¡œ ì„¤ì •
        
        logger.info(f"Running accumulated predictions from {start_date} to {end_date}")

        # ì…ë ¥ ë‚ ì§œë¥¼ datetime ê°ì²´ë¡œ ë³€í™˜
        if isinstance(start_date, str):
            start_date = pd.to_datetime(start_date)
        if end_date is not None and isinstance(end_date, str):
            end_date = pd.to_datetime(end_date)

        # ì €ì¥ëœ ë°ì´í„° í™œìš© ì˜µì…˜ì´ ì¼œì ¸ ìˆìœ¼ë©´ ë¨¼ì € CSVì—ì„œ ë¡œë“œ ì‹œë„
        loaded_predictions = []
        if use_saved_data:
            logger.info("ğŸ” [CACHE] Attempting to load existing predictions from CSV files...")
            
            # ğŸ”§ ì¸ë±ìŠ¤ íŒŒì¼ì´ ì—†ìœ¼ë©´ ê¸°ì¡´ íŒŒì¼ë“¤ë¡œë¶€í„° ì¬ìƒì„±
            cache_dirs = get_file_cache_dirs(file_path)
            predictions_index_file = cache_dirs['predictions'] / 'predictions_index.csv'
            
            if not predictions_index_file.exists():
                logger.warning("âš ï¸ [CACHE] predictions_index.csv not found, attempting to rebuild from existing files...")
                if rebuild_predictions_index_from_existing_files():
                    logger.info("âœ… [CACHE] Successfully rebuilt predictions index")
                else:
                    logger.warning("âš ï¸ [CACHE] Failed to rebuild predictions index")
            
            loaded_predictions = load_accumulated_predictions_from_csv(start_date, end_date, file_path=file_path)  # âœ… íŒŒì¼ ê²½ë¡œ ì¶”ê°€
            logger.info(f"ğŸ“¦ [CACHE] Successfully loaded {len(loaded_predictions)} predictions from CSV cache")
            if len(loaded_predictions) > 0:
                logger.info(f"ğŸ’¡ [CACHE] Using cached predictions will significantly speed up processing!")

        # ë°ì´í„° ë¡œë“œ (ëˆ„ì  ì˜ˆì¸¡ìš© - LSTM ëª¨ë¸, 2022ë…„ ì´ì „ ë°ì´í„° ì œê±°)
        df = load_data(file_path, model_type='lstm')
        prediction_state['current_data'] = df
        prediction_state['prediction_progress'] = 10

        # ì¢…ë£Œ ë‚ ì§œê°€ ì§€ì •ë˜ì§€ ì•Šìœ¼ë©´ ë°ì´í„°ì˜ ë§ˆì§€ë§‰ ë‚ ì§œ ì‚¬ìš©
        if end_date is None:
            end_date = df.index.max()

        # ì‚¬ìš© ê°€ëŠ¥í•œ ë‚ ì§œ ì¶”ì¶œ í›„ ì •ë ¬
        available_dates = [date for date in df.index if start_date <= date <= end_date]
        available_dates.sort()
        
        if not available_dates:
            raise ValueError(f"ì§€ì •ëœ ê¸°ê°„ ë‚´ì— ì‚¬ìš© ê°€ëŠ¥í•œ ë‚ ì§œê°€ ì—†ìŠµë‹ˆë‹¤: {start_date} ~ {end_date}")

        total_dates = len(available_dates)
        logger.info(f"Accumulated prediction: {total_dates} dates from {start_date} to {end_date}")

        # ëˆ„ì  ì„±ëŠ¥ ì§€í‘œ ì´ˆê¸°í™”
        accumulated_metrics = {
            'f1': 0.0,
            'accuracy': 0.0,
            'mape': 0.0,
            'weighted_score': 0.0,
            'total_predictions': 0
        }

        # ì´ë¯¸ ë¡œë“œëœ ì˜ˆì¸¡ ê²°ê³¼ë“¤ì„ ë‚ ì§œë³„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
        loaded_by_date = {}
        for pred in loaded_predictions:
            loaded_by_date[pred['date']] = pred

        # âœ… ìºì‹œ í™œìš© í†µê³„ ì´ˆê¸°í™”
        cache_statistics = {
            'total_dates': 0,
            'cached_dates': 0,
            'new_predictions': 0,
            'cache_hit_rate': 0.0
        }

        all_predictions = []
        accumulated_interval_scores = {}

        # ê° ë‚ ì§œë³„ ì˜ˆì¸¡ ìˆ˜í–‰ ë˜ëŠ” ë¡œë“œ
        for i, current_date in enumerate(available_dates):
            current_date_str = format_date(current_date)
            cache_statistics['total_dates'] += 1
            
            logger.info(f"Processing date {i+1}/{total_dates}: {current_date_str}")
            
            # ì´ë¯¸ ë¡œë“œëœ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì‚¬ìš©
            if current_date_str in loaded_by_date:
                cache_statistics['cached_dates'] += 1  # âœ… ìºì‹œ ì‚¬ìš© ì‹œ ì¹´ìš´í„° ì¦ê°€
                logger.info(f"âš¡ [CACHE] Using cached prediction for {current_date_str} (skipping computation)")
                date_result = loaded_by_date[current_date_str]
                
                # ğŸ”§ ìºì‹œëœ metrics ì•ˆì „ì„± ì²˜ë¦¬
                metrics = date_result.get('metrics')
                if not metrics or not isinstance(metrics, dict):
                    logger.warning(f"âš ï¸ [CACHE] Invalid metrics for {current_date_str}, using defaults")
                    metrics = {'f1': 0.0, 'accuracy': 0.0, 'mape': 0.0, 'weighted_score': 0.0}
                
                # ëˆ„ì  ì„±ëŠ¥ ì§€í‘œ ì—…ë°ì´íŠ¸
                accumulated_metrics['f1'] += metrics.get('f1', 0.0)
                accumulated_metrics['accuracy'] += metrics.get('accuracy', 0.0)
                accumulated_metrics['mape'] += metrics.get('mape', 0.0)
                accumulated_metrics['weighted_score'] += metrics.get('weighted_score', 0.0)
                accumulated_metrics['total_predictions'] += 1
                
            else:
                # ìƒˆë¡œìš´ ì˜ˆì¸¡ ìˆ˜í–‰
                cache_statistics['new_predictions'] += 1
                logger.info(f"ğŸš€ [COMPUTE] Running new prediction for {current_date_str} (not in cache)")
                try:
                    # âœ… ëˆ„ì  ì˜ˆì¸¡ì—ì„œë„ ëª¨ë“  ìƒˆ ì˜ˆì¸¡ì„ ì €ì¥í•˜ë„ë¡ ë³´ì¥
                    results = generate_predictions_with_save(df, current_date, save_to_csv=True, file_path=file_path)
                    
                    # ì˜ˆì¸¡ ë°ì´í„° íƒ€ì… ì•ˆì „ í™•ì¸
                    predictions = results.get('predictions_flat', results.get('predictions', []))
                    
                    # ì˜ˆì¸¡ ë°ì´í„°ê°€ ì¤‘ì²©ëœ ë”•ì…”ë„ˆë¦¬ êµ¬ì¡°ì¸ ê²½ìš° ì²˜ë¦¬
                    if isinstance(predictions, dict):
                        if 'future' in predictions:
                            predictions = predictions['future']
                        elif 'predictions' in predictions:
                            predictions = predictions['predictions']
                    
                    if not predictions or not isinstance(predictions, list):
                        logger.warning(f"No valid predictions found for {current_date_str}: {type(predictions)}")
                        continue
                        
                    # ì‹¤ì œ ì˜ˆì¸¡í•œ ì˜ì—…ì¼ ìˆ˜ ê³„ì‚° (ì•ˆì „í•œ ë°©ì‹)
                    actual_business_days = 0
                    try:
                        for p in predictions:
                            # pê°€ ë”•ì…”ë„ˆë¦¬ì¸ì§€ í™•ì¸
                            if isinstance(p, dict):
                                date_key = p.get('Date') or p.get('date')
                                is_synthetic = p.get('is_synthetic', False)
                                if date_key and not is_synthetic:
                                    actual_business_days += 1
                            else:
                                logger.warning(f"Prediction item is not dict for {current_date_str}: {type(p)}")
                    except Exception as calc_error:
                        logger.error(f"Error calculating business days: {str(calc_error)}")
                        actual_business_days = len(predictions)  # ê¸°ë³¸ê°’
                    
                    metrics = results.get('metrics', {})
                    if not metrics:
                        # ë©”íŠ¸ë¦­ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì„¤ì •
                        metrics = {'f1': 0.0, 'accuracy': 0.0, 'mape': 0.0, 'weighted_score': 0.0}
                    
                    # ëˆ„ì  ì„±ëŠ¥ ì§€í‘œ ì—…ë°ì´íŠ¸
                    accumulated_metrics['f1'] += metrics.get('f1', 0.0)
                    accumulated_metrics['accuracy'] += metrics.get('accuracy', 0.0)
                    accumulated_metrics['mape'] += metrics.get('mape', 0.0)
                    accumulated_metrics['weighted_score'] += metrics.get('weighted_score', 0.0)
                    accumulated_metrics['total_predictions'] += 1

                    # ì•ˆì „í•œ ë°ì´í„° êµ¬ì¡° ìƒì„±
                    safe_predictions = predictions if isinstance(predictions, list) else []
                    safe_interval_scores = results.get('interval_scores', {})
                    if not isinstance(safe_interval_scores, dict):
                        safe_interval_scores = {}
                    
                    date_result = {
                        'date': current_date_str,
                        'predictions': safe_predictions,
                        'metrics': metrics,
                        'interval_scores': safe_interval_scores,
                        'actual_business_days': actual_business_days,
                        'next_semimonthly_period': results.get('next_semimonthly_period'),
                        'original_interval_scores': safe_interval_scores,
                        'ma_results': results.get('ma_results', {}),  # ğŸ”‘ ì´ë™í‰ê·  ë°ì´í„° ì¶”ê°€
                        'attention_data': results.get('attention_data', {})  # ğŸ”‘ Attention ë°ì´í„° ì¶”ê°€
                    }
                    
                except Exception as e:
                    logger.error(f"Error in prediction for date {current_date}: {str(e)}")
                    logger.error(traceback.format_exc())
                    continue

            # êµ¬ê°„ ì ìˆ˜ ëˆ„ì  ì²˜ë¦¬ (ì•ˆì „í•œ ë°©ì‹)
            interval_scores = date_result.get('interval_scores', {})
            if isinstance(interval_scores, dict):
                for interval in interval_scores.values():
                    if not interval or not isinstance(interval, dict) or 'days' not in interval or interval['days'] is None:
                        continue
                    interval_key = f"{format_date(interval['start_date'])}-{format_date(interval['end_date'])}"
                    if interval_key in accumulated_interval_scores:
                        accumulated_interval_scores[interval_key]['score'] += interval['score']
                        accumulated_interval_scores[interval_key]['count'] += 1
                        accumulated_interval_scores[interval_key]['avg_price'] = (
                            (accumulated_interval_scores[interval_key]['avg_price'] *
                             (accumulated_interval_scores[interval_key]['count'] - 1) +
                             interval['avg_price']) / accumulated_interval_scores[interval_key]['count']
                        )
                    else:
                        accumulated_interval_scores[interval_key] = interval.copy()
                        accumulated_interval_scores[interval_key]['count'] = 1

            all_predictions.append(date_result)
            prediction_state['prediction_progress'] = 10 + int(90 * (i + 1) / total_dates)

        # í‰ê·  ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
        if accumulated_metrics['total_predictions'] > 0:
            count = accumulated_metrics['total_predictions']
            accumulated_metrics['f1'] /= count
            accumulated_metrics['accuracy'] /= count
            accumulated_metrics['mape'] /= count
            accumulated_metrics['weighted_score'] /= count

        # ì˜ˆì¸¡ ì‹ ë¢°ë„ ê³„ì‚°
        logger.info("Calculating prediction consistency scores...")
        unique_periods = set()
        for pred in all_predictions:
            if 'next_semimonthly_period' in pred and pred['next_semimonthly_period']:
                unique_periods.add(pred['next_semimonthly_period'])
        
        accumulated_consistency_scores = {}
        for period in unique_periods:
            try:
                consistency_data = calculate_prediction_consistency(all_predictions, period)
                accumulated_consistency_scores[period] = consistency_data
                logger.info(f"Consistency score for {period}: {consistency_data.get('consistency_score', 'N/A')}")
            except Exception as e:
                logger.error(f"Error calculating consistency for period {period}: {str(e)}")

        # accumulated_interval_scores ì²˜ë¦¬
        accumulated_scores_list = [
            v for v in accumulated_interval_scores.values()
            if v is not None and isinstance(v, dict) and 'days' in v and v['days'] is not None
        ]
        accumulated_scores_list.sort(key=lambda x: x['score'], reverse=True)

        accumulated_purchase_reliability, debug_info = calculate_accumulated_purchase_reliability(all_predictions)
        
        # âœ… ìºì‹œ í™œìš©ë¥  ê³„ì‚°
        cache_statistics['cache_hit_rate'] = (cache_statistics['cached_dates'] / cache_statistics['total_dates'] * 100) if cache_statistics['total_dates'] > 0 else 0.0
        logger.info(f"ğŸ¯ [CACHE] Final statistics: {cache_statistics['cached_dates']}/{cache_statistics['total_dates']} cached ({cache_statistics['cache_hit_rate']:.1f}%), {cache_statistics['new_predictions']} new predictions computed")
        
        # ê²°ê³¼ ì €ì¥
        prediction_state['accumulated_predictions'] = all_predictions
        prediction_state['accumulated_metrics'] = accumulated_metrics
        prediction_state['prediction_dates'] = [p['date'] for p in all_predictions]
        prediction_state['accumulated_interval_scores'] = accumulated_scores_list
        prediction_state['accumulated_consistency_scores'] = accumulated_consistency_scores
        prediction_state['accumulated_purchase_reliability'] = accumulated_purchase_reliability
        prediction_state['accumulated_purchase_debug'] = debug_info
        prediction_state['cache_statistics'] = cache_statistics  # âœ… ìºì‹œ í†µê³„ ì¶”ê°€

        if all_predictions:
            latest = all_predictions[-1]
            prediction_state['latest_predictions'] = latest['predictions']
            prediction_state['current_date'] = latest['date']

        prediction_state['is_predicting'] = False
        prediction_state['prediction_progress'] = 100
        logger.info(f"Accumulated prediction completed for {len(all_predictions)} dates")
        
    except Exception as e:
        logger.error(f"Error in accumulated prediction: {str(e)}")
        logger.error(traceback.format_exc())
        prediction_state['error'] = str(e)
        prediction_state['is_predicting'] = False
        prediction_state['prediction_progress'] = 0
        prediction_state['accumulated_consistency_scores'] = {}

# 3. ë°±ê·¸ë¼ìš´ë“œì—ì„œ ëˆ„ì  ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜
def background_accumulated_prediction(file_path, start_date, end_date=None):
    """ë°±ê·¸ë¼ìš´ë“œì—ì„œ ëˆ„ì  ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜"""
    thread = Thread(target=run_accumulated_predictions_with_save, args=(file_path, start_date, end_date))
    thread.daemon = True
    thread.start()
    return thread

# 6. ëˆ„ì  ê²°ê³¼ ë³´ê³ ì„œ ìƒì„± í•¨ìˆ˜
def generate_accumulated_report():
    """ëˆ„ì  ì˜ˆì¸¡ ê²°ê³¼ ë³´ê³ ì„œ ìƒì„±"""
    global prediction_state
    
    if not prediction_state['accumulated_predictions']:
        return None
    
    try:
        metrics = prediction_state['accumulated_metrics']
        all_preds = prediction_state['accumulated_predictions']
        
        # ë³´ê³ ì„œ íŒŒì¼ ì´ë¦„ ìƒì„± - íŒŒì¼ë³„ ìºì‹œ ë””ë ‰í† ë¦¬ ì‚¬ìš©
        start_date = all_preds[0]['date']
        end_date = all_preds[-1]['date']
        try:
            cache_dirs = get_file_cache_dirs()
            report_dir = cache_dirs['predictions']
            report_filename = os.path.join(report_dir, f"accumulated_report_{start_date}_to_{end_date}.txt")
        except Exception as e:
            logger.warning(f"Could not get cache directories for accumulated report: {str(e)}")
            report_filename = f"accumulated_report_{start_date}_to_{end_date}.txt"
        
        with open(report_filename, 'w', encoding='utf-8') as f:
            f.write(f"===== Accumulated Prediction Report =====\n")
            f.write(f"Period: {start_date} to {end_date}\n")
            f.write(f"Total Predictions: {metrics['total_predictions']}\n\n")
            
            # ëˆ„ì  ì„±ëŠ¥ ì§€í‘œ
            f.write("Average Performance Metrics:\n")
            f.write(f"- F1 Score: {metrics['f1']:.4f}\n")
            f.write(f"- Direction Accuracy: {metrics['accuracy']:.2f}%\n")
            f.write(f"- MAPE: {metrics['mape']:.2f}%\n")
            f.write(f"- Weighted Score: {metrics['weighted_score']:.2f}%\n\n")
            
            # ë‚ ì§œë³„ ìƒì„¸ ì •ë³´
            f.write("Performance By Date:\n")
            for pred in all_preds:
                date = pred['date']
                m = pred['metrics']
                f.write(f"\n* {date}:\n")
                f.write(f"  - F1 Score: {m['f1']:.4f}\n")
                f.write(f"  - Accuracy: {m['accuracy']:.2f}%\n")
                f.write(f"  - MAPE: {m['mape']:.2f}%\n")
                f.write(f"  - Weighted Score: {m['weighted_score']:.2f}%\n")
                
                # êµ¬ë§¤ êµ¬ê°„ ì •ë³´
                if pred['interval_scores']:
                    best_interval = decide_purchase_interval(pred['interval_scores'])
                    f.write("Best Purchase Interval:\n")
                    f.write(f"- Start Date: {best_interval['start_date']}\n")
                    f.write(f"- End Date: {best_interval['end_date']}\n")
                    f.write(f"- Duration: {best_interval['days']} days\n")
                    f.write(f"- Average Price: {best_interval['avg_price']:.2f}\n")
                    f.write(f"- Score: {best_interval['score']}\n")
                    f.write(f"- Selection Reason: {best_interval.get('selection_reason', '')}\n\n")
        
        return report_filename
    
    except Exception as e:
        logger.error(f"Error generating accumulated report: {str(e)}")
        return None

# 9. ëˆ„ì  ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™” í•¨ìˆ˜
def visualize_accumulated_metrics():
    """ëˆ„ì  ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”"""
    global prediction_state
    
    if not prediction_state['accumulated_predictions']:
        return None, None
    
    try:
        # ë°ì´í„° ì¤€ë¹„
        dates = []
        f1_scores = []
        accuracies = []
        mapes = []
        weighted_scores = []
        
        for pred in prediction_state['accumulated_predictions']:
            dates.append(pred['date'])
            m = pred['metrics']
            f1_scores.append(m['f1'])
            accuracies.append(m['accuracy'])
            mapes.append(m['mape'])
            weighted_scores.append(m['weighted_score'])
        
        # ë‚ ì§œë¥¼ datetimeìœ¼ë¡œ ë³€í™˜
        dates = [pd.to_datetime(d) for d in dates]
        
        # ê·¸ë˜í”„ ìƒì„±
        fig, axs = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('Accumulated Prediction Metrics', fontsize=16)
        
        # F1 Score
        axs[0, 0].plot(dates, f1_scores, marker='o', color='blue')
        axs[0, 0].set_title('F1 Score')
        axs[0, 0].set_ylim(0, 1)
        axs[0, 0].grid(True)
        plt.setp(axs[0, 0].xaxis.get_majorticklabels(), rotation=45)
        
        # Accuracy
        axs[0, 1].plot(dates, accuracies, marker='o', color='green')
        axs[0, 1].set_title('Direction Accuracy (%)')
        axs[0, 1].set_ylim(0, 100)
        axs[0, 1].grid(True)
        plt.setp(axs[0, 1].xaxis.get_majorticklabels(), rotation=45)
        
        # MAPE
        axs[1, 0].plot(dates, mapes, marker='o', color='red')
        axs[1, 0].set_title('MAPE (%)')
        axs[1, 0].set_ylim(0, max(mapes) * 1.2)
        axs[1, 0].grid(True)
        plt.setp(axs[1, 0].xaxis.get_majorticklabels(), rotation=45)
        
        # Weighted Score
        axs[1, 1].plot(dates, weighted_scores, marker='o', color='purple')
        axs[1, 1].set_title('Weighted Score (%)')
        axs[1, 1].set_ylim(0, 100)
        axs[1, 1].grid(True)
        plt.setp(axs[1, 1].xaxis.get_majorticklabels(), rotation=45)
        
        plt.tight_layout()
        
        # ì´ë¯¸ì§€ ì €ì¥
        img_buf = io.BytesIO()
        plt.savefig(img_buf, format='png', dpi=300)
        img_buf.seek(0)
        
        # Base64ë¡œ ì¸ì½”ë”©
        img_str = base64.b64encode(img_buf.read()).decode('utf-8')
        
        # íŒŒì¼ë¡œ ì €ì¥ - íŒŒì¼ë³„ ìºì‹œ ë””ë ‰í† ë¦¬ ì‚¬ìš©
        try:
            cache_dirs = get_file_cache_dirs()
            plots_dir = cache_dirs['plots']
            filename = os.path.join(plots_dir, 'accumulated_metrics.png')
        except Exception as e:
            logger.warning(f"Could not get cache directories for accumulated metrics: {str(e)}")
            filename = 'accumulated_metrics.png'
        
        plt.savefig(filename, dpi=300, bbox_inches='tight')
        plt.close()
        
        return filename, img_str
        
    except Exception as e:
        logger.error(f"Error visualizing accumulated metrics: {str(e)}")
        return None, None

#######################################################################
# ì˜ˆì¸¡ ë° ëª¨ë¸ í•™ìŠµ í•¨ìˆ˜
#######################################################################

def prepare_data(train_data, val_data, sequence_length, predict_window, target_col_idx, augment=False):
    """í•™ìŠµ ë° ê²€ì¦ ë°ì´í„°ë¥¼ ì‹œí€€ìŠ¤ í˜•íƒœë¡œ ì¤€ë¹„"""
    X_train, y_train, prev_train = [], [], []
    for i in range(len(train_data) - sequence_length - predict_window + 1):
        seq = train_data[i:i+sequence_length]
        target = train_data[i+sequence_length:i+sequence_length+predict_window, target_col_idx]
        prev_value = train_data[i+sequence_length-1, target_col_idx]
        X_train.append(seq)
        y_train.append(target)
        prev_train.append(prev_value)
        if augment:
            # ê°„ë‹¨í•œ ë°ì´í„° ì¦ê°•
            noise = np.random.normal(0, 0.001, seq.shape)
            aug_seq = seq + noise
            X_train.append(aug_seq)
            y_train.append(target)
            prev_train.append(prev_value)
    
    X_val, y_val, prev_val = [], [], []
    for i in range(len(val_data) - sequence_length - predict_window + 1):
        X_val.append(val_data[i:i+sequence_length])
        y_val.append(val_data[i+sequence_length:i+sequence_length+predict_window, target_col_idx])
        prev_val.append(val_data[i+sequence_length-1, target_col_idx])
    
    return map(np.array, [X_train, y_train, prev_train, X_val, y_val, prev_val])



def train_model(features, target_col, current_date, historical_data, device, params):
    """LSTM ëª¨ë¸ í•™ìŠµ"""
    try:
        # ì¼ê´€ëœ í•™ìŠµ ê²°ê³¼ë¥¼ ìœ„í•œ ì‹œë“œ ê³ ì •
        set_seed()
        
        # ë””ë°”ì´ìŠ¤ ì‚¬ìš© ì •ë³´ ë¡œê¹…
        log_device_usage(device, "LSTM ëª¨ë¸ í•™ìŠµ ì‹œì‘")
        
        # íŠ¹ì„± ì´ë¦„ í™•ì¸
        if target_col not in features:
            features.append(target_col)
        
        # í•™ìŠµ ë°ì´í„° ì¤€ë¹„ (í˜„ì¬ ë‚ ì§œê¹Œì§€)
        train_df = historical_data[features].copy()
        target_col_idx = train_df.columns.get_loc(target_col)
        
        # ìŠ¤ì¼€ì¼ë§
        scaler = StandardScaler()
        train_data = scaler.fit_transform(train_df)
        
        # í•˜ì´í¼íŒŒë¼ë¯¸í„°
        sequence_length = params.get('sequence_length', 20)
        hidden_size = params.get('hidden_size', 128)
        num_layers = params.get('num_layers', 2)
        dropout = params.get('dropout', 0.2)
        learning_rate = params.get('learning_rate', 0.001)
        num_epochs = params.get('num_epochs', 100)
        batch_size = params.get('batch_size', 32)
        alpha = params.get('loss_alpha', 0.7)  # DirectionalLoss alpha
        beta = params.get('loss_beta', 0.2)    # DirectionalLoss beta
        patience = params.get('patience', 20)   # ì¡°ê¸° ì¢…ë£Œ ì¸ë‚´
        predict_window = params.get('predict_window', 23)  # ì˜ˆì¸¡ ê¸°ê°„
        
        # 80/20 ë¶„í•  (ì—°ëŒ€ìˆœ)
        train_size = int(len(train_data) * 0.8)
        train_set = train_data[:train_size]
        val_set = train_data[train_size:]
        
        # ì‹œí€€ìŠ¤ ë°ì´í„° ì¤€ë¹„
        X_train, y_train, prev_train, X_val, y_val, prev_val = prepare_data(
            train_set, val_set, sequence_length, predict_window, target_col_idx
        )
        
        # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
        if len(X_train) < batch_size:
            batch_size = max(1, len(X_train) // 2)
            logger.warning(f"ë°°ì¹˜ í¬ê¸°ê°€ ë°ì´í„° í¬ê¸°ë³´ë‹¤ ì»¤ì„œ ì¡°ì •: {batch_size} (ë°ì´í„°: {len(X_train)})")
        
        if len(X_train) == 0 or len(X_val) == 0:
            raise ValueError("Insufficient data for training")
        
        logger.info(f"ğŸ¯ ì‚¬ìš©í•  ë°°ì¹˜ í¬ê¸°: {batch_size}")
        
        # ë°ì´í„°ì…‹ ë° ë¡œë” ìƒì„± (CPUì—ì„œ ìƒì„±, í•™ìŠµ ì‹œ GPUë¡œ ì´ë™)
        train_dataset = TimeSeriesDataset(X_train, y_train, torch.device('cpu'), prev_train)
        
        # GPU í™œìš©ë¥  ìµœì í™”ë¥¼ ìœ„í•œ DataLoader ì„¤ì •
        num_workers = 0 if device.type == 'cuda' else 2  # CUDAì—ì„œëŠ” ë©€í‹°í”„ë¡œì„¸ì‹± ë¹„í™œì„±í™”
        pin_memory = device.type == 'cuda'  # GPU ì‚¬ìš© ì‹œ pin_memory í™œì„±í™”
        
        train_loader = DataLoader(
            train_dataset,
            batch_size=batch_size,
            shuffle=True,
            worker_init_fn=seed_worker,
            generator=g,
            num_workers=num_workers,
            pin_memory=pin_memory,
            persistent_workers=False if num_workers == 0 else True
        )
        
        val_dataset = TimeSeriesDataset(X_val, y_val, torch.device('cpu'), prev_val)
        val_loader = DataLoader(
            val_dataset, 
            batch_size=min(batch_size, len(X_val)),  # ê²€ì¦ì—ì„œë„ ë°°ì¹˜ í¬ê¸° ìµœì í™”
            shuffle=False,
            num_workers=num_workers,
            pin_memory=pin_memory
        )
        
        logger.info(f"ğŸ”§ DataLoader ì„¤ì •: workers={num_workers}, pin_memory={pin_memory}, train_batch={batch_size}, val_batch={min(batch_size, len(X_val))}")
        
        # ëª¨ë¸ ìƒì„±
        logger.info("ğŸ“ˆ ImprovedLSTMPredictor ì‚¬ìš©")
        model = ImprovedLSTMPredictor(
            input_size=train_data.shape[1],
            hidden_size=hidden_size,
            num_layers=num_layers,
            dropout=dropout,
            output_size=predict_window
        ).to(device)
        
        # ëª¨ë¸ì´ GPUì— ì˜¬ë¼ê°”ëŠ”ì§€ í™•ì¸
        model_device = next(model.parameters()).device
        logger.info(f"ğŸ¤– ImprovedLSTM ëª¨ë¸ì´ {model_device}ì— ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤")
        log_device_usage(model_device, "ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        
        # ì†ì‹¤ í•¨ìˆ˜ ìƒì„±
        logger.info(f"ğŸ“ˆ DirectionalLoss ì‚¬ìš©: alpha={alpha}, beta={beta}")
        criterion = DirectionalLoss(alpha=alpha, beta=beta)
        
        # ìµœì í™”ê¸° ë° ìŠ¤ì¼€ì¤„ëŸ¬
        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='min', factor=0.5,
            patience=patience//2
        )
        
        # í•™ìŠµ
        best_val_loss = float('inf')
        best_model_state = None
        patience_counter = 0
        
        # GPU ìµœì í™” ì„¤ì •
        if device.type == 'cuda':
            torch.backends.cudnn.benchmark = True  # ì…ë ¥ í¬ê¸°ê°€ ì¼ì •í•  ë•Œ ì„±ëŠ¥ í–¥ìƒ
            torch.cuda.empty_cache()  # ìºì‹œ ì •ë¦¬
            
        log_device_usage(device, "ëª¨ë¸ í•™ìŠµ ì¤‘")
        
        for epoch in range(num_epochs):
            # í•™ìŠµ ëª¨ë“œ
            model.train()
            train_loss = 0
            batch_count = 0
            
            for X_batch, y_batch, prev_batch in train_loader:
                optimizer.zero_grad()
                
                # ëª¨ë¸ê³¼ ê°™ì€ ë””ë°”ì´ìŠ¤ë¡œ ë°ì´í„° ì´ë™
                X_batch = X_batch.to(device, non_blocking=device.type=='cuda')
                y_batch = y_batch.to(device, non_blocking=device.type=='cuda')
                prev_batch = prev_batch.to(device, non_blocking=device.type=='cuda')
                
                # ëª¨ë¸ ì˜ˆì¸¡ ë° ì†ì‹¤ ê³„ì‚°
                y_pred = model(X_batch, prev_batch)
                loss = criterion(y_pred, y_batch, prev_batch)
                
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()
                train_loss += loss.item()
                batch_count += 1
                
            # ì²« ë²ˆì§¸ ì—í¬í¬ì™€ ì£¼ê¸°ì ìœ¼ë¡œ GPU ìƒíƒœ ë¡œê¹…
            if epoch == 0 or (epoch + 1) % 10 == 0:
                log_device_usage(device, f"ì—í¬í¬ {epoch+1}/{num_epochs}")
            
            # ê²€ì¦ ëª¨ë“œ
            model.eval()
            val_loss = 0
            
            with torch.no_grad():
                for X_batch, y_batch, prev_batch in val_loader:
                    # ëª¨ë¸ê³¼ ê°™ì€ ë””ë°”ì´ìŠ¤ë¡œ ë°ì´í„° ì´ë™
                    X_batch = X_batch.to(device, non_blocking=device.type=='cuda')
                    y_batch = y_batch.to(device, non_blocking=device.type=='cuda')
                    prev_batch = prev_batch.to(device, non_blocking=device.type=='cuda')
                    
                    # ëª¨ë¸ ì˜ˆì¸¡ ë° ì†ì‹¤ ê³„ì‚°
                    y_pred = model(X_batch, prev_batch)
                    loss = criterion(y_pred, y_batch, prev_batch)
                    
                    val_loss += loss.item()
                
                val_loss /= len(val_loader)
                
                # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
                scheduler.step(val_loss)
                
                # ëª¨ë¸ ì €ì¥ (ìµœì )
                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                    best_model_state = model.state_dict().copy()
                    patience_counter = 0
                else:
                    patience_counter += 1
                
                # ì¡°ê¸° ì¢…ë£Œ
                if patience_counter >= patience:
                    logger.info(f"Early stopping at epoch {epoch+1}")
                    break
        
        # ìµœì  ëª¨ë¸ ë³µì›
        if best_model_state is not None:
            model.load_state_dict(best_model_state)
        
        logger.info(f"Model training completed with best validation loss: {best_val_loss:.4f}")
        
        # í•™ìŠµ ì™„ë£Œ í›„ GPU ìƒíƒœ í™•ì¸
        log_device_usage(device, "ëª¨ë¸ í•™ìŠµ ì™„ë£Œ")
        
        # GPU ìºì‹œ ì •ë¦¬
        if device.type == 'cuda':
            torch.cuda.empty_cache()
            logger.info("ğŸ§¹ GPU ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
        
        # ëª¨ë¸, ìŠ¤ì¼€ì¼ëŸ¬, íŒŒë¼ë¯¸í„° ë°˜í™˜
        return model, scaler, target_col_idx
    
    except Exception as e:
        logger.error(f"Error in model training: {str(e)}")
        logger.error(traceback.format_exc())
        raise e

def generate_predictions(df, current_date, predict_window=23, features=None, target_col='MOPJ', file_path=None):
    """
    ê°œì„ ëœ ì˜ˆì¸¡ ìˆ˜í–‰ í•¨ìˆ˜ - ì˜ˆì¸¡ ì‹œì‘ì¼ì˜ ë°˜ì›” ê¸°ê°„ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì‚¬ìš©
    ğŸ”‘ ë°ì´í„° ëˆ„ì¶œ ë°©ì§€: current_date ì´í›„ì˜ ì‹¤ì œê°’ì€ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
    """
    try:
        # ì¼ê´€ëœ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ìœ„í•œ ì‹œë“œ ê³ ì •
        set_seed()
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        log_device_usage(device, "ëª¨ë¸ í•™ìŠµ ì‹œì‘")
        
        # í˜„ì¬ ë‚ ì§œê°€ ë¬¸ìì—´ì´ë©´ datetimeìœ¼ë¡œ ë³€í™˜
        if isinstance(current_date, str):
            current_date = pd.to_datetime(current_date)
        
        # í˜„ì¬ ë‚ ì§œ ê²€ì¦ (ë°ì´í„° ê¸°ì¤€ì¼)
        if current_date not in df.index:
            closest_date = df.index[df.index <= current_date][-1]
            logger.warning(f"Current date {current_date} not found in dataframe. Using closest date: {closest_date}")
            current_date = closest_date
        
        # ì˜ˆì¸¡ ì‹œì‘ì¼ ê³„ì‚°
        prediction_start_date = current_date + pd.Timedelta(days=1)
        while prediction_start_date.weekday() >= 5 or is_holiday(prediction_start_date):
            prediction_start_date += pd.Timedelta(days=1)
        
        # ë°˜ì›” ê¸°ê°„ ê³„ì‚°
        data_semimonthly_period = get_semimonthly_period(current_date)
        prediction_semimonthly_period = get_semimonthly_period(prediction_start_date)
        
        # âœ… í•µì‹¬ ìˆ˜ì •: ì˜ˆì¸¡ ì‹œì‘ì¼ ê¸°ì¤€ìœ¼ë¡œ ë‹¤ìŒ ë°˜ì›” ê³„ì‚°
        next_semimonthly_period = get_next_semimonthly_period(prediction_start_date)
        
        logger.info(f"ğŸ¯ Prediction Setup:")
        logger.info(f"  ğŸ“… Data base date: {current_date} (period: {data_semimonthly_period})")
        logger.info(f"  ğŸš€ Prediction start date: {prediction_start_date} (period: {prediction_semimonthly_period})")
        logger.info(f"  ğŸ¯ Purchase interval target period: {next_semimonthly_period}")
        
        # 23ì¼ì¹˜ ì˜ˆì¸¡ì„ ìœ„í•œ ë‚ ì§œ ìƒì„±
        all_business_days = get_next_n_business_days(current_date, df, predict_window)
        
        # âœ… í•µì‹¬ ìˆ˜ì •: ì˜ˆì¸¡ ì‹œì‘ì¼ ê¸°ì¤€ìœ¼ë¡œ êµ¬ë§¤ êµ¬ê°„ ê³„ì‚°
        semimonthly_business_days, purchase_target_period = get_next_semimonthly_dates(prediction_start_date, df)
        
        logger.info(f"  ğŸ“Š Total predictions: {len(all_business_days)} days")
        logger.info(f"  ğŸ›’ Purchase target period: {purchase_target_period}")
        logger.info(f"  ğŸ“ˆ Purchase interval business days: {len(semimonthly_business_days)}")
        
        if not all_business_days:
            raise ValueError(f"No future business days found after {current_date}")

        # âœ… í•µì‹¬ ìˆ˜ì •: LSTM ë‹¨ê¸° ì˜ˆì¸¡ì„ ìœ„í•´ 2022ë…„ ì´í›„ ë°ì´í„°ë§Œ ì‚¬ìš©
        cutoff_date_2022 = pd.to_datetime('2022-01-01')
        available_data = df[df.index <= current_date].copy()
        
        # 2022ë…„ ì´í›„ ë°ì´í„°ê°€ ì¶©ë¶„í•œ ê²½ìš° í•´ë‹¹ ê¸°ê°„ë§Œ ì‚¬ìš© (ë‹¨ê¸° ì˜ˆì¸¡ ì •í™•ë„ í–¥ìƒ)
        recent_data = available_data[available_data.index >= cutoff_date_2022]
        if len(recent_data) >= 50:
            historical_data = recent_data.copy()
            logger.info(f"  ğŸ¯ Using recent data for LSTM: 2022+ ({len(historical_data)} records)")
        else:
            historical_data = available_data.copy()
            logger.info(f"  ğŸ“Š Using full available data: insufficient recent data ({len(recent_data)} < 50)")
        
        logger.info(f"  ğŸ“Š Training data: {len(historical_data)} records up to {format_date(current_date)}")
        logger.info(f"  ğŸ“Š Training data range: {format_date(historical_data.index.min())} ~ {format_date(historical_data.index.max())}")
        
        # ìµœì†Œ ë°ì´í„° ìš”êµ¬ì‚¬í•­ í™•ì¸
        if len(historical_data) < 50:
            raise ValueError(f"Insufficient training data: {len(historical_data)} records (minimum 50 required)")
        
        if features is None:
            selected_features, _ = select_features_from_groups(
                historical_data, 
                variable_groups,
                target_col=target_col,
                vif_threshold=50.0,
                corr_threshold=0.8
            )
        else:
            selected_features = features
            
        if target_col not in selected_features:
            selected_features.append(target_col)
        
        logger.info(f"  ğŸ”§ Selected features ({len(selected_features)}): {selected_features}")
        
        # âœ… í•µì‹¬ ìˆ˜ì •: ë‚ ì§œë³„ ë‹¤ë¥¸ ìŠ¤ì¼€ì¼ë§ ë³´ì¥
        scaler = StandardScaler()
        scaled_data = scaler.fit_transform(historical_data[selected_features])
        target_col_idx = selected_features.index(target_col)
        
        logger.info(f"  âš–ï¸  Scaler fitted on data up to {format_date(current_date)}")
        logger.info(f"  ğŸ“Š Scaled data shape: {scaled_data.shape}")
        
        # âœ… í•µì‹¬: ì˜ˆì¸¡ ì‹œì‘ì¼ì˜ ë°˜ì›” ê¸°ê°„ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì‚¬ìš©
        optimized_params = optimize_hyperparameters_semimonthly_kfold(
            train_data=scaled_data,
            input_size=len(selected_features),
            target_col_idx=target_col_idx,
            device=device,
            current_period=prediction_semimonthly_period,  # âœ… ì˜ˆì¸¡ ì‹œì‘ì¼ì˜ ë°˜ì›” ê¸°ê°„
            file_path=file_path,  # ğŸ”‘ íŒŒì¼ ê²½ë¡œ ì „ë‹¬
            n_trials=30,
            k_folds=10,
            use_cache=True
        )
        
        logger.info(f"âœ… Using hyperparameters for prediction start period: {prediction_semimonthly_period}")
        
        # âœ… í•µì‹¬ ìˆ˜ì •: ëª¨ë¸ í•™ìŠµ ì‹œ í˜„ì¬ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ë°ì´í„° ë¶„í•  ë³´ì¥
        logger.info(f"  ğŸš€ Training model with data up to {format_date(current_date)}")
        model, model_scaler, model_target_col_idx = train_model(
            selected_features,
            target_col,
            current_date,
            historical_data,
            device,
            optimized_params
        )
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ì¼ê´€ì„± í™•ì¸
        if model_target_col_idx != target_col_idx:
            logger.warning(f"Target column index mismatch: {model_target_col_idx} vs {target_col_idx}")
            target_col_idx = model_target_col_idx
        
        logger.info(f"  âœ… Model trained successfully for prediction starting {format_date(prediction_start_date)}")
        
        # âœ… í•µì‹¬ ìˆ˜ì •: ì˜ˆì¸¡ ë°ì´í„° ì¤€ë¹„ ì‹œ ë‚ ì§œë³„ ë‹¤ë¥¸ ì‹œí€€ìŠ¤ ë³´ì¥ (ë°ì´í„° ëˆ„ì¶œ ë°©ì§€)
        seq_len = optimized_params['sequence_length']
        
        # ğŸ”‘ ì¤‘ìš”: current_dateë¥¼ ì˜ˆì¸¡í•˜ë ¤ë©´ current_date ì´ì „ì˜ ë°ì´í„°ë§Œ ì‚¬ìš©
        available_dates_before_current = [d for d in df.index if d < current_date]
        
        if len(available_dates_before_current) < seq_len:
            logger.warning(f"âš ï¸  Insufficient historical data before {format_date(current_date)}: {len(available_dates_before_current)} < {seq_len}")
            # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  ì´ì „ ë°ì´í„° ì‚¬ìš©
            sequence_dates = available_dates_before_current
        else:
            # ë§ˆì§€ë§‰ seq_lenê°œì˜ ì´ì „ ë‚ ì§œ ì‚¬ìš©
            sequence_dates = available_dates_before_current[-seq_len:]
        
        # ì‹œí€€ìŠ¤ ë°ì´í„° ì¶”ì¶œ (current_date ì œì™¸!)
        sequence = df.loc[sequence_dates][selected_features].values
        
        logger.info(f"  ğŸ“Š Sequence data: {sequence.shape} from {format_date(sequence_dates[0])} to {format_date(sequence_dates[-1])}")
        logger.info(f"  ğŸš« Excluded current_date: {format_date(current_date)} (preventing data leakage)")
        
        # ëª¨ë¸ì—ì„œ ë°˜í™˜ëœ ìŠ¤ì¼€ì¼ëŸ¬ ì‚¬ìš© (ì¼ê´€ì„± ë³´ì¥)
        sequence = model_scaler.transform(sequence)
        prev_value = sequence[-1, target_col_idx]
        
        logger.info(f"  ğŸ“ˆ Previous value (scaled): {prev_value:.4f}")
        logger.info(f"  ğŸ“Š Sequence length used: {len(sequence)} (required: {seq_len})")
        
        # ì˜ˆì¸¡ ìˆ˜í–‰
        future_predictions = []  # ë¯¸ë˜ ì˜ˆì¸¡ (ì‹¤ì œê°’ ì—†ìŒ)
        validation_data = []     # ê²€ì¦ ë°ì´í„° (ì‹¤ì œê°’ ìˆìŒ)
        
        with torch.no_grad():
            # 23ì˜ì—…ì¼ ì „ì²´ì— ëŒ€í•´ ì˜ˆì¸¡ ìˆ˜í–‰
            max_pred_days = min(predict_window, len(all_business_days))
            current_sequence = sequence.copy()
            
            # í…ì„œë¡œ ë³€í™˜
            X = torch.FloatTensor(current_sequence).unsqueeze(0).to(device)
            prev_tensor = torch.FloatTensor([prev_value]).to(device)
            
            # ì „ì²´ ì‹œí€€ìŠ¤ ì˜ˆì¸¡
            pred = model(X, prev_tensor).cpu().numpy()[0]
            
            # âœ… í•µì‹¬ ìˆ˜ì •: ê° ë‚ ì§œë³„ ì˜ˆì¸¡ ìƒì„± (ë°ì´í„° ëˆ„ì¶œ ë°©ì§€)
            for j, pred_date in enumerate(all_business_days[:max_pred_days]):
                # âœ… ìŠ¤ì¼€ì¼ ì—­ë³€í™˜ ì‹œ ì¼ê´€ëœ ìŠ¤ì¼€ì¼ëŸ¬ ì‚¬ìš©
                dummy_matrix = np.zeros((1, len(selected_features)))
                dummy_matrix[0, target_col_idx] = pred[j]
                pred_value = model_scaler.inverse_transform(dummy_matrix)[0, target_col_idx]
                
                # ì˜ˆì¸¡ê°’ ê²€ì¦ ë° ì •ë¦¬
                if np.isnan(pred_value) or np.isinf(pred_value):
                    logger.warning(f"Invalid prediction value for {pred_date}: {pred_value}, skipping")
                    continue
                
                pred_value = float(pred_value)
                
                # âœ… ì‹¤ì œ ë°ì´í„° ë§ˆì§€ë§‰ ë‚ ì§œ í™•ì¸
                last_data_date = df.index.max()
                actual_value = None
                
                # âœ… ì‹¤ì œê°’ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ ë° ì„¤ì •
                if (pred_date in df.index and 
                    pd.notna(df.loc[pred_date, target_col]) and 
                    pred_date <= last_data_date):
                    
                    actual_value = float(df.loc[pred_date, target_col])
                    
                    if np.isnan(actual_value) or np.isinf(actual_value):
                        actual_value = None
                
                # ê¸°ë³¸ ì˜ˆì¸¡ ì •ë³´ (ì‹¤ì œê°’ í¬í•¨)
                prediction_item = {
                    'date': format_date(pred_date, '%Y-%m-%d'),
                    'prediction': pred_value,
                    'actual': actual_value,  # ğŸ”‘ ì‹¤ì œê°’ í•­ìƒ í¬í•¨
                    'prediction_from': format_date(current_date, '%Y-%m-%d'),
                    'day_offset': j + 1,
                    'is_business_day': pred_date.weekday() < 5 and not is_holiday(pred_date),
                    'is_synthetic': pred_date not in df.index,
                    'semimonthly_period': data_semimonthly_period,
                    'next_semimonthly_period': next_semimonthly_period
                }
                
                # âœ… ì‹¤ì œê°’ì´ ìˆëŠ” ê²½ìš° ê²€ì¦ ë°ì´í„°ì—ë„ ì¶”ê°€
                if actual_value is not None:
                    validation_item = {
                        **prediction_item,
                        'error': abs(pred_value - actual_value),
                        'error_pct': abs(pred_value - actual_value) / actual_value * 100 if actual_value != 0 else 0.0
                    }
                    validation_data.append(validation_item)
                    
                    # ğŸ“Š ê²€ì¦ íƒ€ì… êµ¬ë¶„ ë¡œê·¸
                    if pred_date <= current_date:
                        logger.debug(f"  âœ… Training validation: {format_date(pred_date)} - Pred: {pred_value:.2f}, Actual: {actual_value:.2f}")
                    else:
                        logger.debug(f"  ğŸ¯ Test validation: {format_date(pred_date)} - Pred: {pred_value:.2f}, Actual: {actual_value:.2f}")
                elif pred_date > last_data_date:
                    logger.debug(f"  ğŸ”® Future: {format_date(pred_date)} - Pred: {pred_value:.2f} (no actual - beyond data)")
                
                future_predictions.append(prediction_item)
        
        # ğŸ“Š ê²€ì¦ ë°ì´í„° í†µê³„
        training_validation = len([v for v in validation_data if pd.to_datetime(v['date']) <= current_date])
        test_validation = len([v for v in validation_data if pd.to_datetime(v['date']) > current_date])
        
        logger.info(f"ğŸ“Š Prediction Results:")
        logger.info(f"  ğŸ“ˆ Total predictions: {len(future_predictions)}")
        logger.info(f"  âœ… Training validation (â‰¤ {format_date(current_date)}): {training_validation}")
        logger.info(f"  ğŸ¯ Test validation (> {format_date(current_date)}): {test_validation}")
        logger.info(f"  ğŸ“‹ Total validation points: {len(validation_data)}")
        logger.info(f"  ğŸ”® Pure future predictions (> {format_date(df.index.max())}): {len(future_predictions) - len(validation_data)}")
        
        if len(validation_data) == 0:
            logger.info("  â„¹ï¸  Pure future prediction - no validation data available")
        
        # âœ… êµ¬ê°„ í‰ê·  ë° ì ìˆ˜ ê³„ì‚° - ì˜¬ë°”ë¥¸ êµ¬ë§¤ ëŒ€ìƒ ê¸°ê°„ ì‚¬ìš©
        temp_predictions_for_interval = []
        for pred in future_predictions:
            pred_date = pd.to_datetime(pred['date'])
            if pred_date in semimonthly_business_days:  # ì´ì œ ì˜¬ë°”ë¥¸ ë‹¤ìŒ ë°˜ì›” ë‚ ì§œë“¤
                temp_predictions_for_interval.append({
                    'Date': pred_date,
                    'Prediction': pred['prediction']
                })
        
        logger.info(f"  ğŸ›’ Predictions for interval calculation: {len(temp_predictions_for_interval)} (target period: {purchase_target_period})")
        
        interval_averages, interval_scores, analysis_info = calculate_interval_averages_and_scores(
            temp_predictions_for_interval, 
            semimonthly_business_days
        )

        # ìµœì¢… êµ¬ë§¤ êµ¬ê°„ ê²°ì •
        best_interval = decide_purchase_interval(interval_scores)

        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚° (ê²€ì¦ ë°ì´í„°ê°€ ìˆì„ ë•Œë§Œ)
        metrics = None
        if validation_data:
            start_day_value = df.loc[current_date, target_col]
            if not (pd.isna(start_day_value) or np.isnan(start_day_value) or np.isinf(start_day_value)):
                try:
                    temp_df_for_metrics = pd.DataFrame([
                        {
                            'Date': pd.to_datetime(item['date']),
                            'Prediction': item['prediction'],
                            'Actual': item['actual']
                        } for item in validation_data
                    ])
                    
                    if not temp_df_for_metrics.empty:
                        metrics = compute_performance_metrics_improved(temp_df_for_metrics, start_day_value)
                        logger.info(f"  ğŸ“Š Computed metrics from {len(validation_data)} validation points")
                    else:
                        logger.info("  âš ï¸  No valid data for metrics computation")
                except Exception as e:
                    logger.error(f"Error computing metrics: {str(e)}")
                    metrics = None
            else:
                logger.warning("Invalid start_day_value for metrics computation")
        else:
            logger.info("  â„¹ï¸  No validation data available - pure future prediction")
        
        # âœ… ì´ë™í‰ê·  ê³„ì‚° ì‹œ ì‹¤ì œê°’ë„ í¬í•¨ (ê²€ì¦ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°)
        temp_predictions_for_ma = []
        for pred in future_predictions:
            pred_date = pd.to_datetime(pred['date'])
            actual_val = None
            
            # ì‹¤ì œ ë°ì´í„°ê°€ ì¡´ì¬í•˜ëŠ” ë‚ ì§œë©´ ì‹¤ì œê°’ ì„¤ì •
            if (pred_date in df.index and 
                pd.notna(df.loc[pred_date, target_col]) and 
                pred_date <= df.index.max()):
                actual_val = float(df.loc[pred_date, target_col])
            
            temp_predictions_for_ma.append({
                'Date': pred_date,
                'Prediction': pred['prediction'],
                'Actual': actual_val
            })
        
        logger.info(f"  ğŸ“ˆ Calculating moving averages with historical data up to {format_date(current_date)}")
        ma_results = calculate_moving_averages_with_history(
            temp_predictions_for_ma, 
            historical_data,  # ì´ë¯¸ current_dateê¹Œì§€ë¡œ í•„í„°ë§ë¨
            target_col=target_col
        )
        
        # íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„
        attention_data = None
        try:
            sequence_tensor = torch.FloatTensor(sequence).unsqueeze(0).to(device)
            prev_tensor = torch.FloatTensor([float(prev_value)]).to(device)
            
            # ì‹¤ì œ ì‹œí€€ìŠ¤ ë‚ ì§œ ì •ë³´ ì „ë‹¬ (sequence_dates ë³€ìˆ˜ ì‚¬ìš©)
            actual_sequence_end_date = current_date  # current_dateê°€ ì‹¤ì œ ë°ì´í„°ì˜ ë§ˆì§€ë§‰ ë‚ ì§œ
            attention_file, attention_img, feature_importance = visualize_attention_weights(
                model, sequence_tensor, prev_tensor, actual_sequence_end_date, selected_features, sequence_dates
            )
            
            attention_data = {
                'image': attention_img,
                'file_path': attention_file,
                'feature_importance': feature_importance
            }
        except Exception as e:
            logger.error(f"Error in attention analysis: {str(e)}")
        
        # ì‹œê°í™” ìƒì„±
        plots = {
            'basic_plot': {'file': None, 'image': None},
            'ma_plot': {'file': None, 'image': None}
        }

        start_day_value = None
        if current_date in df.index and not pd.isna(df.loc[current_date, target_col]):
            start_day_value = df.loc[current_date, target_col]

        # ğŸ“Š ì‹œê°í™”ìš© ë°ì´í„° ì¤€ë¹„ - ì‹¤ì œê°’ í¬í•¨
        temp_df_for_plot_data = []
        for item in future_predictions:
            pred_date = pd.to_datetime(item['date'])
            actual_val = None
            
            # ì‹¤ì œ ë°ì´í„°ê°€ ì¡´ì¬í•˜ëŠ” ë‚ ì§œë©´ ì‹¤ì œê°’ ì„¤ì •
            if (pred_date in df.index and 
                pd.notna(df.loc[pred_date, target_col]) and 
                pred_date <= df.index.max()):
                actual_val = float(df.loc[pred_date, target_col])
            
            temp_df_for_plot_data.append({
                'Date': pred_date,
                'Prediction': item['prediction'],
                'Actual': actual_val
            })
        
        temp_df_for_plot = pd.DataFrame(temp_df_for_plot_data)

        if metrics:
            f1_score = metrics['f1']
            accuracy = metrics['accuracy']
            mape = metrics['mape']
            weighted_score = metrics['weighted_score']
            visualization_type = "with validation data"
        else:
            f1_score = accuracy = mape = weighted_score = 0.0
            visualization_type = "future prediction only"

        if start_day_value is not None and not temp_df_for_plot.empty:
            try:
                basic_plot_file, basic_plot_img = plot_prediction_basic(
                    temp_df_for_plot,
                    prediction_start_date,
                    start_day_value,
                    f1_score,
                    accuracy,
                    mape,
                    weighted_score,
                    current_date=current_date,  # ğŸ”‘ ë°ì´í„° ì»·ì˜¤í”„ ë‚ ì§œ ì „ë‹¬
                    save_prefix=None,  # íŒŒì¼ë³„ ìºì‹œ ì‹œìŠ¤í…œ ì‚¬ìš©
                    title_prefix=f"Prediction Graph ({visualization_type})",
                    file_path=file_path
                )
                
                ma_plot_file, ma_plot_img = plot_moving_average_analysis(
                    ma_results,
                    prediction_start_date,
                    save_prefix=None,  # íŒŒì¼ë³„ ìºì‹œ ì‹œìŠ¤í…œ ì‚¬ìš©
                    title_prefix=f"Moving Average Analysis ({visualization_type})",
                    file_path=file_path
                )
                
                plots['basic_plot'] = {'file': basic_plot_file, 'image': basic_plot_img}
                plots['ma_plot'] = {'file': ma_plot_file, 'image': ma_plot_img}
                
                logger.info(f"  ğŸ“Š Visualizations created ({visualization_type})")
                
            except Exception as e:
                logger.error(f"Error creating visualizations: {str(e)}")
                logger.error(traceback.format_exc())
        else:
            logger.warning("  âš ï¸  No start day value or empty predictions - skipping visualizations")
        
        # ê²°ê³¼ ë°˜í™˜ (ê¸‰ë“±ë½ ëª¨ë“œ ì •ë³´ í¬í•¨)
        return {
            'predictions': future_predictions,
            'predictions_flat': future_predictions,  # í˜¸í™˜ì„±ì„ ìœ„í•œ ì¶”ê°€
            'validation_data': validation_data,
            'interval_scores': interval_scores,
            'interval_averages': interval_averages,
            'best_interval': best_interval,
            'ma_results': ma_results,
            'metrics': metrics,
            'selected_features': selected_features,
            'attention_data': attention_data,
            'plots': plots,
            'current_date': format_date(current_date, '%Y-%m-%d'),
            'data_end_date': format_date(current_date, '%Y-%m-%d'),
            'semimonthly_period': data_semimonthly_period,
            'next_semimonthly_period': purchase_target_period,  # âœ… ìˆ˜ì •: ì˜¬ë°”ë¥¸ êµ¬ë§¤ ëŒ€ìƒ ê¸°ê°„
            'prediction_semimonthly_period': prediction_semimonthly_period,
            'hyperparameter_period_used': prediction_semimonthly_period,
            'purchase_target_period': purchase_target_period,  # âœ… ì¶”ê°€
            'model_type': 'ImprovedLSTMPredictor',
            'loss_function': 'DirectionalLoss'
        }
        
    except Exception as e:
        logger.error(f"Error in prediction generation: {str(e)}")
        logger.error(traceback.format_exc())
        raise e

def generate_predictions_compatible(df, current_date, predict_window=23, features=None, target_col='MOPJ'):
    """
    ê¸°ì¡´ í”„ë¡ íŠ¸ì—”ë“œì™€ í˜¸í™˜ë˜ëŠ” ì˜ˆì¸¡ í•¨ìˆ˜
    (ìƒˆë¡œìš´ êµ¬ì¡° + ê¸°ì¡´ í˜•íƒœ ë³€í™˜)
    """
    try:
        # ì¼ê´€ëœ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ìœ„í•œ ì‹œë“œ ê³ ì •
        set_seed()
        
        # ìƒˆë¡œìš´ generate_predictions í•¨ìˆ˜ ì‹¤í–‰
        new_results = generate_predictions(df, current_date, predict_window, features, target_col)
        
        # ê¸°ì¡´ í˜•íƒœë¡œ ë³€í™˜
        if isinstance(new_results.get('predictions'), dict):
            # ìƒˆë¡œìš´ êµ¬ì¡°ì¸ ê²½ìš°
            future_predictions = new_results['predictions']['future']
            validation_data = new_results['predictions']['validation']
            
            # futureì™€ validationì„ í•©ì³ì„œ ê¸°ì¡´ í˜•íƒœë¡œ ë³€í™˜
            all_predictions = future_predictions + validation_data
        else:
            # ê¸°ì¡´ êµ¬ì¡°ì¸ ê²½ìš°
            all_predictions = new_results.get('predictions_flat', new_results.get('predictions', []))
        
        # ê¸°ì¡´ í•„ë“œëª…ìœ¼ë¡œ ë³€í™˜
        compatible_predictions = convert_to_legacy_format(all_predictions)
        
        # ê²°ê³¼ì— ê¸°ì¡´ í˜•íƒœ ì¶”ê°€
        new_results['predictions'] = compatible_predictions  # ê¸°ì¡´ í˜¸í™˜ì„±
        new_results['predictions_new'] = new_results.get('predictions')  # ìƒˆë¡œìš´ êµ¬ì¡°ë„ ìœ ì§€
        
        logger.info(f"Generated {len(compatible_predictions)} compatible predictions")
        
        return new_results
        
    except Exception as e:
        logger.error(f"Error in compatible prediction generation: {str(e)}")
        raise e

def generate_predictions_with_save(df, current_date, predict_window=23, features=None, target_col='MOPJ', save_to_csv=True, file_path=None):
    """
    ì˜ˆì¸¡ ìˆ˜í–‰ ë° ìŠ¤ë§ˆíŠ¸ ìºì‹œ ì €ì¥ì´ í¬í•¨ëœ í•¨ìˆ˜ (ìˆ˜ì •ë¨)
    """
    try:
        # ì¼ê´€ëœ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ìœ„í•œ ì‹œë“œ ê³ ì •
        set_seed()
        
        logger.info(f"Starting prediction with smart cache save for {current_date}")
        
        # ê¸°ì¡´ generate_predictions í•¨ìˆ˜ ì‹¤í–‰
        results = generate_predictions(df, current_date, predict_window, features, target_col, file_path)
        
        # ìŠ¤ë§ˆíŠ¸ ìºì‹œ ì €ì¥ ì˜µì…˜ì´ í™œì„±í™”ëœ ê²½ìš°
        if save_to_csv:
            logger.info("Saving prediction with smart cache system...")
            
            # ìƒˆë¡œìš´ ìŠ¤ë§ˆíŠ¸ ìºì‹œ ì €ì¥ í•¨ìˆ˜ ì‚¬ìš©
            save_result = save_prediction_simple(results, current_date)
            results['save_info'] = save_result
            
            if save_result['success']:
                logger.info(f"âœ… Smart cache save completed successfully")
                logger.info(f"  - Prediction Start Date: {save_result.get('prediction_start_date')}")
                logger.info(f"  - File: {save_result.get('file', 'N/A')}")
                
                # ìºì‹œ ì •ë³´ ì¶”ê°€ (ì•ˆì „í•œ í‚¤ ì ‘ê·¼)
                results['cache_info'] = {
                    'saved': True,
                    'prediction_start_date': save_result.get('prediction_start_date'),
                    'file': save_result.get('file'),
                    'success': save_result.get('success', False)
                }
            else:
                logger.warning(f"âŒ Failed to save prediction with smart cache: {save_result.get('error')}")
                results['cache_info'] = {
                    'saved': False,
                    'error': save_result.get('error')
                }
        else:
            logger.info("Skipping smart cache save (save_to_csv=False)")
            results['save_info'] = {'success': False, 'reason': 'save_to_csv=False'}
            results['cache_info'] = {
                'saved': False,
                'reason': 'save_to_csv=False'
            }
        
        return results
        
    except Exception as e:
        logger.error(f"Error in generate_predictions_with_save: {str(e)}")
        logger.error(traceback.format_exc())
        
        # ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ ì˜ˆì¸¡ ê²°ê³¼ëŠ” ë°˜í™˜í•˜ë˜, ì €ì¥ ì‹¤íŒ¨ ì •ë³´ í¬í•¨
        if 'results' in locals():
            results['save_info'] = {'success': False, 'error': str(e)}
            results['cache_info'] = {'saved': False, 'error': str(e)}
            return results
        else:
            # ì˜ˆì¸¡ ìì²´ê°€ ì‹¤íŒ¨í•œ ê²½ìš°
            raise e

def generate_predictions_with_attention_save(df, current_date, predict_window=23, features=None, target_col='MOPJ', save_to_csv=True, file_path=None):
    """
    ì˜ˆì¸¡ ìˆ˜í–‰ ë° attention í¬í•¨ CSV ì €ì¥ í•¨ìˆ˜
    
    Parameters:
    -----------
    df : pandas.DataFrame
        ì „ì²´ ë°ì´í„°
    current_date : str or datetime
        í˜„ì¬ ë‚ ì§œ (ë°ì´í„° ê¸°ì¤€ì¼)
    predict_window : int
        ì˜ˆì¸¡ ê¸°ê°„ (ê¸°ë³¸ 23ì¼)
    features : list, optional
        ì‚¬ìš©í•  íŠ¹ì„± ëª©ë¡
    target_col : str
        íƒ€ê²Ÿ ì»¬ëŸ¼ëª… (ê¸°ë³¸ 'MOPJ')
    save_to_csv : bool
        CSV ì €ì¥ ì—¬ë¶€ (ê¸°ë³¸ True)
    
    Returns:
    --------
    dict : ì˜ˆì¸¡ ê²°ê³¼ (attention ë°ì´í„° í¬í•¨)
    """
    try:
        # ì¼ê´€ëœ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ìœ„í•œ ì‹œë“œ ê³ ì •
        set_seed()
        
        logger.info(f"Starting prediction with attention save for {current_date}")
        
        # ê¸°ì¡´ generate_predictions í•¨ìˆ˜ ì‹¤í–‰
        results = generate_predictions(df, current_date, predict_window, features, target_col, file_path)
        
        # attention í¬í•¨ ì €ì¥ ì˜µì…˜ì´ í™œì„±í™”ëœ ê²½ìš°
        if save_to_csv:
            logger.info("Saving prediction with attention data...")
            save_result = save_prediction_simple(results, current_date)
            results['save_info'] = save_result
            
            if save_result['success']:
                logger.info(f"SUCCESS: Prediction with attention saved successfully")
                logger.info(f"  - CSV: {save_result['csv_file']}")
                logger.info(f"  - Metadata: {save_result['meta_file']}")
                logger.info(f"  - Attention: {save_result['attention_file'] if save_result.get('attention_file') else 'Not saved'}")
            else:
                logger.warning(f"âŒ Failed to save prediction with attention: {save_result.get('error')}")
        else:
            logger.info("Skipping CSV save (save_to_csv=False)")
            results['save_info'] = {'success': False, 'reason': 'save_to_csv=False'}
        
        return results
        
    except Exception as e:
        logger.error(f"Error in generate_predictions_with_attention_save: {str(e)}")
        logger.error(traceback.format_exc())
        
        # ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ ì˜ˆì¸¡ ê²°ê³¼ëŠ” ë°˜í™˜í•˜ë˜, ì €ì¥ ì‹¤íŒ¨ ì •ë³´ í¬í•¨
        if 'results' in locals():
            results['save_info'] = {'success': False, 'error': str(e)}
            return results
        else:
            # ì˜ˆì¸¡ ìì²´ê°€ ì‹¤íŒ¨í•œ ê²½ìš°
            raise e

#######################################################################
# ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì²˜ë¦¬
#######################################################################
# ğŸ”§ SyntaxError ìˆ˜ì • - check_existing_prediction í•¨ìˆ˜ (3987ë¼ì¸ ê·¼ì²˜)

def check_existing_prediction(current_date, file_path=None):
    """
    íŒŒì¼ë³„ ë””ë ‰í† ë¦¬ êµ¬ì¡°ì—ì„œ ì €ì¥ëœ ì˜ˆì¸¡ì„ í™•ì¸í•˜ê³  ë¶ˆëŸ¬ì˜¤ëŠ” í•¨ìˆ˜
    ğŸ¯ í˜„ì¬ íŒŒì¼ì˜ ë””ë ‰í† ë¦¬ì—ì„œ ìš°ì„  ê²€ìƒ‰
    """
    try:
        # í˜„ì¬ ë‚ ì§œ(ë°ì´í„° ê¸°ì¤€ì¼)ì—ì„œ ì²« ë²ˆì§¸ ì˜ˆì¸¡ ë‚ ì§œ ê³„ì‚°
        if isinstance(current_date, str):
            current_date = pd.to_datetime(current_date)
        
        # ë‹¤ìŒ ì˜ì—…ì¼ ì°¾ê¸° (í˜„ì¬ ë‚ ì§œì˜ ë‹¤ìŒ ì˜ì—…ì¼ì´ ì²« ë²ˆì§¸ ì˜ˆì¸¡ ë‚ ì§œ)
        next_date = current_date + pd.Timedelta(days=1)
        while next_date.weekday() >= 5 or is_holiday(next_date):
            next_date += pd.Timedelta(days=1)
        
        first_prediction_date = next_date
        date_str = first_prediction_date.strftime('%Y%m%d')
        
        # ë°˜ì›” ì •ë³´ ê³„ì‚° (ìºì‹œ ì •í™•ì„±ì„ ìœ„í•´)
        current_semimonthly = get_semimonthly_period(first_prediction_date)
        
        logger.info(f"ğŸ” Checking cache for prediction starting: {first_prediction_date.strftime('%Y-%m-%d')}")
        logger.info(f"  ğŸ“… Data end date: {current_date.strftime('%Y-%m-%d')}")
        logger.info(f"  ğŸ“… Expected prediction start: {first_prediction_date.strftime('%Y-%m-%d')}")
        logger.info(f"  ğŸ“… Current semimonthly period: {current_semimonthly}")
        logger.info(f"  ğŸ“„ Expected filename pattern: prediction_start_{date_str}.*")
        
        # ğŸ¯ 1ë‹¨ê³„: í˜„ì¬ íŒŒì¼ì˜ ìºì‹œ ë””ë ‰í† ë¦¬ì—ì„œ ì •í™•í•œ ë‚ ì§œ ë§¤ì¹˜ë¡œ ìºì‹œ ì°¾ê¸°
        try:
            # ğŸ”§ ìˆ˜ì •: íŒŒì¼ ê²½ë¡œë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì „ë‹¬
            cache_dirs = get_file_cache_dirs(file_path)
            file_predictions_dir = cache_dirs['predictions']
            
            logger.info(f"  ğŸ“ Cache directory: {cache_dirs['root']}")
            logger.info(f"  ğŸ“ Predictions directory: {file_predictions_dir}")
            logger.info(f"  ğŸ“ Directory exists: {file_predictions_dir.exists()}")
            
        except Exception as e:
            logger.error(f"âŒ Failed to get cache directories: {str(e)}")
            return None
        
        if file_predictions_dir.exists():
            exact_csv = file_predictions_dir / f"prediction_start_{date_str}.csv"
            exact_meta = file_predictions_dir / f"prediction_start_{date_str}_meta.json"
            
            logger.info(f"  ğŸ” Looking for: {exact_csv}")
            logger.info(f"  ğŸ” CSV exists: {exact_csv.exists()}")
            logger.info(f"  ğŸ” Meta exists: {exact_meta.exists()}")
            
            if exact_csv.exists() and exact_meta.exists():
                logger.info(f"âœ… Found exact prediction cache in file directory: {exact_csv.name}")
                return load_prediction_with_attention_from_csv_in_dir(first_prediction_date, file_predictions_dir)
            
            # í•´ë‹¹ íŒŒì¼ ë””ë ‰í† ë¦¬ì—ì„œ ë‹¤ë¥¸ ë‚ ì§œì˜ ì˜ˆì¸¡ ì°¾ê¸°
            logger.info("ğŸ” Searching for other predictions in file directory...")
            prediction_files = list(file_predictions_dir.glob("prediction_start_*_meta.json"))
            
            logger.info(f"  ğŸ“‹ Found {len(prediction_files)} prediction files:")
            for i, pf in enumerate(prediction_files):
                logger.info(f"    {i+1}. {pf.name}")
            
            if prediction_files:
                # ë°˜ì›” ê¸°ê°„ ë§¤ì¹­í•˜ëŠ” ìºì‹œ ì°¾ê¸°
                compatible_cache = None
                
                for meta_file in prediction_files:
                    try:
                        with open(meta_file, 'r', encoding='utf-8') as f:
                            meta_data = json.load(f)
                        
                        cached_data_end_date = meta_data.get('data_end_date')
                        if cached_data_end_date:
                            cached_data_end_date = pd.to_datetime(cached_data_end_date)
                            cached_semimonthly = get_semimonthly_period(cached_data_end_date)
                            
                            logger.info(f"    ğŸ” Checking file cache: {meta_file.name}")
                            logger.info(f"      ğŸ“… Current semimonthly: {current_semimonthly}")
                            logger.info(f"      ğŸ“… Cached semimonthly:  {cached_semimonthly}")
                            
                            if cached_semimonthly == current_semimonthly:
                                cached_date_str = meta_file.stem.replace('prediction_start_', '').replace('_meta', '')
                                cached_prediction_date = pd.to_datetime(cached_date_str, format='%Y%m%d')
                                
                                logger.info(f"ğŸ¯ Found compatible prediction in file directory!")
                                logger.info(f"  ğŸ“… Cached prediction date: {cached_prediction_date.strftime('%Y-%m-%d')}")
                                logger.info(f"  ğŸ“… Semimonthly period match: {current_semimonthly}")
                                logger.info(f"  ğŸ“„ Using file: {meta_file.name}")
                                
                                return load_prediction_with_attention_from_csv_in_dir(cached_prediction_date, file_predictions_dir)
                            else:
                                logger.info(f"    âŒ Semimonthly period mismatch - skipping")
                                
                    except Exception as e:
                        logger.debug(f"    âš ï¸ Error reading meta file {meta_file}: {str(e)}")
                        continue
                
                logger.info("âŒ No compatible cache found in file directory (semimonthly mismatch)")
        else:
            logger.warning(f"âŒ Predictions directory does not exist: {file_predictions_dir}")
        
        # ğŸ¯ 2ë‹¨ê³„: ë‹¤ë¥¸ íŒŒì¼ë“¤ì˜ ìºì‹œì—ì„œ í˜¸í™˜ ê°€ëŠ¥í•œ ì˜ˆì¸¡ ì°¾ê¸°
        current_file_path = file_path or prediction_state.get('current_file', None)
        if current_file_path:
            # ğŸ”§ ìˆ˜ì •: ëª¨ë“  ê¸°ì¡´ íŒŒì¼ì˜ ìºì‹œ ë””ë ‰í† ë¦¬ íƒìƒ‰
            upload_dir = Path(UPLOAD_FOLDER)
            existing_files = [f for f in upload_dir.glob('*.xlsx') if str(f) != current_file_path]
            
            logger.info(f"ğŸ” [PREDICTION_CACHE] ë‹¤ë¥¸ íŒŒì¼ë“¤ì˜ ìºì‹œ íƒìƒ‰: {len(existing_files)}ê°œ íŒŒì¼")
            
            for existing_file in existing_files:
                try:
                    # ê¸°ì¡´ íŒŒì¼ì˜ ìºì‹œ ë””ë ‰í† ë¦¬ í™•ì¸
                    existing_cache_dirs = get_file_cache_dirs(str(existing_file))
                    existing_predictions_dir = existing_cache_dirs['predictions']
                    
                    if existing_predictions_dir.exists():
                        # ë™ì¼í•œ ë°˜ì›” ê¸°ê°„ì˜ ì˜ˆì¸¡ íŒŒì¼ ì°¾ê¸°
                        pattern = f"prediction_start_*_meta.json"
                        meta_files = list(existing_predictions_dir.glob(pattern))
                        
                        logger.info(f"    ğŸ“ {existing_file.name}: {len(meta_files)}ê°œ ì˜ˆì¸¡ íŒŒì¼")
                        
                        for meta_file in meta_files:
                            try:
                                with open(meta_file, 'r', encoding='utf-8') as f:
                                    meta_data = json.load(f)
                                
                                # ì˜ˆì¸¡ ì‹œì‘ì¼ë¡œë¶€í„° ë°˜ì›” ê¸°ê°„ ì¶”ì¶œ
                                cached_date_str = meta_file.stem.replace('prediction_start_', '').replace('_meta', '')
                                cached_prediction_date = pd.to_datetime(cached_date_str, format='%Y%m%d')
                                cached_semimonthly = get_semimonthly_period(cached_prediction_date)
                                
                                if cached_semimonthly == current_semimonthly:
                                    logger.info(f"    ğŸ¯ í˜¸í™˜ ê°€ëŠ¥í•œ ì˜ˆì¸¡ ë°œê²¬! {existing_file.name} -> {cached_prediction_date.strftime('%Y-%m-%d')}")
                                    logger.info(f"    ğŸ“… ë°˜ì›” ê¸°ê°„ ì¼ì¹˜: {current_semimonthly}")
                                    
                                    return load_prediction_with_attention_from_csv_in_dir(cached_prediction_date, existing_predictions_dir)
                                    
                            except Exception as e:
                                logger.debug(f"    âš ï¸ ë©”íƒ€ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ {meta_file}: {str(e)}")
                                continue
                except Exception as e:
                    logger.debug(f"    âš ï¸ ìºì‹œ ë””ë ‰í† ë¦¬ ì ‘ê·¼ ì‹¤íŒ¨ {existing_file.name}: {str(e)}")
                    continue
                    
            logger.info("âŒ ë‹¤ë¥¸ íŒŒì¼ë“¤ì˜ ìºì‹œì—ì„œë„ í˜¸í™˜ ê°€ëŠ¥í•œ ì˜ˆì¸¡ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤")
            
        logger.info("âŒ No compatible prediction cache found")
        return None
        
    except Exception as e:
        logger.error(f"âŒ Error checking existing prediction: {str(e)}")
        logger.error(traceback.format_exc())
        return None

def generate_visualizations_realtime(predictions, df, current_date, metadata):
    """ì‹¤ì‹œê°„ìœ¼ë¡œ ì‹œê°í™” ìƒì„± (ì €ì¥í•˜ì§€ ì•ŠìŒ)"""
    try:
        # DataFrameìœ¼ë¡œ ë³€í™˜
        sequence_df = pd.DataFrame(predictions)
        if 'Date' in sequence_df.columns:
            sequence_df['Date'] = pd.to_datetime(sequence_df['Date'])
        
        # ì‹œì‘ê°’ ê³„ì‚°
        if isinstance(current_date, str):
            current_date = pd.to_datetime(current_date)
        
        start_day_value = df.loc[current_date, 'MOPJ'] if current_date in df.index else None
        
        if start_day_value is not None:
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
            metrics = compute_performance_metrics_improved(sequence_df, start_day_value)
            
            # ê¸°ë³¸ ê·¸ë˜í”„ ìƒì„± (ë©”ëª¨ë¦¬ì—ë§Œ)
            _, basic_plot_img = plot_prediction_basic(
                sequence_df, 
                metadata.get('prediction_start_date', current_date),
                start_day_value,
                metrics['f1'],
                metrics['accuracy'], 
                metrics['mape'],
                metrics['weighted_score'],
                save_prefix=None  # íŒŒì¼ë³„ ìºì‹œ ë””ë ‰í† ë¦¬ ìë™ ì‚¬ìš©
                )
                
            # ì´ë™í‰ê·  ê³„ì‚° ë° ì‹œê°í™”
            historical_data = df[df.index <= current_date].copy()
            ma_results = calculate_moving_averages_with_history(predictions, historical_data, target_col='MOPJ')
            
            _, ma_plot_img = plot_moving_average_analysis(
                ma_results,
                metadata.get('prediction_start_date', current_date),
                save_prefix=None  # íŒŒì¼ë³„ ìºì‹œ ë””ë ‰í† ë¦¬ ìë™ ì‚¬ìš©
            )
            
            # ìƒíƒœì— ì €ì¥
            prediction_state['latest_plots'] = {
                'basic_plot': {'file': None, 'image': basic_plot_img},
                'ma_plot': {'file': None, 'image': ma_plot_img}
            }
            prediction_state['latest_ma_results'] = ma_results
            prediction_state['latest_metrics'] = metrics
            
        else:
            logger.warning("Cannot generate visualizations: start day value not available")
            prediction_state['latest_plots'] = {
                'basic_plot': {'file': None, 'image': None},
                'ma_plot': {'file': None, 'image': None}
            }
            prediction_state['latest_ma_results'] = {}
            prediction_state['latest_metrics'] = {}
            
    except Exception as e:
        logger.error(f"Error generating realtime visualizations: {str(e)}")
        prediction_state['latest_plots'] = {
            'basic_plot': {'file': None, 'image': None},
            'ma_plot': {'file': None, 'image': None}
        }
        prediction_state['latest_ma_results'] = {}
        prediction_state['latest_metrics'] = {}

def regenerate_visualizations_from_cache(predictions, df, current_date, metadata):
    """
    ìºì‹œëœ ë°ì´í„°ë¡œë¶€í„° ì‹œê°í™”ë¥¼ ì¬ìƒì„±í•˜ëŠ” í•¨ìˆ˜
    ğŸ”‘ current_dateë¥¼ ì „ë‹¬í•˜ì—¬ ê³¼ê±°/ë¯¸ë˜ êµ¬ë¶„ ì‹œê°í™” ìƒì„±
    """
    try:
        logger.info("ğŸ¨ Regenerating visualizations from cached data...")
        
        # DataFrameìœ¼ë¡œ ë³€í™˜ (ì•ˆì „í•œ ë°©ì‹)
        temp_df_for_plot = pd.DataFrame([
            {
                'Date': pd.to_datetime(item.get('Date') or item.get('date')),
                'Prediction': safe_serialize_value(item.get('Prediction') or item.get('prediction')),
                'Actual': safe_serialize_value(item.get('Actual') or item.get('actual'))
            } for item in predictions if item.get('Date') or item.get('date')
        ])
        
        logger.info(f"  ğŸ“Š Plot data prepared: {len(temp_df_for_plot)} predictions")
        
        # current_date ì²˜ë¦¬
        if isinstance(current_date, str):
            current_date = pd.to_datetime(current_date)
        
        # ì‹œì‘ê°’ ê³„ì‚°
        start_day_value = None
        if current_date in df.index and not pd.isna(df.loc[current_date, 'MOPJ']):
            start_day_value = df.loc[current_date, 'MOPJ']
            logger.info(f"  ğŸ“ˆ Start day value: {start_day_value:.2f}")
        else:
            logger.warning(f"  âš ï¸  Start day value not available for {current_date}")
        
        # ë©”íƒ€ë°ì´í„°ì—ì„œ ë©”íŠ¸ë¦­ ê°€ì ¸ì˜¤ê¸° (ì•ˆì „í•œ ë°©ì‹)
        metrics = metadata.get('metrics')
        if metrics:
            f1_score = safe_serialize_value(metrics.get('f1', 0.0))
            accuracy = safe_serialize_value(metrics.get('accuracy', 0.0))
            mape = safe_serialize_value(metrics.get('mape', 0.0))
            weighted_score = safe_serialize_value(metrics.get('weighted_score', 0.0))
            logger.info(f"  ğŸ“Š Metrics loaded - F1: {f1_score:.3f}, Acc: {accuracy:.1f}%, MAPE: {mape:.1f}%")
        else:
            f1_score = accuracy = mape = weighted_score = 0.0
            logger.info("  â„¹ï¸  No metrics available - using default values")
        
        plots = {
            'basic_plot': {'file': None, 'image': None},
            'ma_plot': {'file': None, 'image': None}
        }
        
        # ì‹œê°í™” ìƒì„± (ë°ì´í„°ê°€ ì¶©ë¶„í•œ ê²½ìš°ë§Œ)
        if start_day_value is not None and not temp_df_for_plot.empty:
            logger.info("  ğŸ¨ Generating basic prediction plot...")
            
            # ì˜ˆì¸¡ ì‹œì‘ì¼ ê³„ì‚°
            prediction_start_date = metadata.get('prediction_start_date')
            if isinstance(prediction_start_date, str):
                prediction_start_date = pd.to_datetime(prediction_start_date)
            elif prediction_start_date is None:
                # ë©”íƒ€ë°ì´í„°ì— ì—†ìœ¼ë©´ current_date ë‹¤ìŒ ì˜ì—…ì¼ë¡œ ê³„ì‚°
                prediction_start_date = current_date + pd.Timedelta(days=1)
                while prediction_start_date.weekday() >= 5 or is_holiday(prediction_start_date):
                    prediction_start_date += pd.Timedelta(days=1)
                logger.info(f"  ğŸ“… Calculated prediction start date: {prediction_start_date}")
            
            # âœ… í•µì‹¬ ìˆ˜ì •: current_date ì „ë‹¬í•˜ì—¬ ê³¼ê±°/ë¯¸ë˜ êµ¬ë¶„ ì‹œê°í™”
            basic_plot_file, basic_plot_img = plot_prediction_basic(
                temp_df_for_plot,
                prediction_start_date,
                start_day_value,
                f1_score,
                accuracy,
                mape,
                weighted_score,
                current_date=current_date,  # ğŸ”‘ í•µì‹¬ ìˆ˜ì •: current_date ì „ë‹¬
                save_prefix=None,  # íŒŒì¼ë³„ ìºì‹œ ë””ë ‰í† ë¦¬ ìë™ ì‚¬ìš©
                title_prefix="Cached Prediction Analysis"
            )
            
            if basic_plot_file:
                logger.info(f"  âœ… Basic plot generated: {basic_plot_file}")
            else:
                logger.warning("  âŒ Basic plot generation failed")
            
            # ì´ë™í‰ê·  ê³„ì‚° ë° ì‹œê°í™”
            logger.info("  ğŸ“ˆ Calculating moving averages...")
            historical_data = df[df.index <= current_date].copy()
            
            # ìºì‹œëœ ì˜ˆì¸¡ ë°ì´í„°ë¥¼ ì´ë™í‰ê·  ê³„ì‚°ìš©ìœ¼ë¡œ ë³€í™˜
            ma_input_data = []
            for pred in predictions:
                try:
                    ma_item = {
                        'Date': pd.to_datetime(pred.get('Date') or pred.get('date')),
                        'Prediction': safe_serialize_value(pred.get('Prediction') or pred.get('prediction')),
                        'Actual': safe_serialize_value(pred.get('Actual') or pred.get('actual'))
                    }
                    ma_input_data.append(ma_item)
                except Exception as e:
                    logger.warning(f"  âš ï¸  Error processing MA data item: {str(e)}")
                    continue
            
            ma_results = calculate_moving_averages_with_history(
                ma_input_data, historical_data, target_col='MOPJ'
            )
            
            if ma_results:
                logger.info(f"  ğŸ“Š MA calculated for {len(ma_results)} windows")
                
                # ì´ë™í‰ê·  ì‹œê°í™”
                ma_plot_file, ma_plot_img = plot_moving_average_analysis(
                    ma_results,
                    prediction_start_date,
                    save_prefix=None,  # íŒŒì¼ë³„ ìºì‹œ ë””ë ‰í† ë¦¬ ìë™ ì‚¬ìš©
                    title_prefix="Cached Moving Average Analysis"
                )
                
                if ma_plot_file:
                    logger.info(f"  âœ… MA plot generated: {ma_plot_file}")
                else:
                    logger.warning("  âŒ MA plot generation failed")
            else:
                logger.warning("  âš ï¸  Moving average calculation failed")
                ma_plot_file, ma_plot_img = None, None
            
            plots = {
                'basic_plot': {'file': basic_plot_file, 'image': basic_plot_img},
                'ma_plot': {'file': ma_plot_file, 'image': ma_plot_img}
            }
            
            logger.info("  âœ… Visualizations regenerated from cache successfully")
        else:
            if start_day_value is None:
                logger.warning("  âŒ Cannot regenerate visualizations: start day value not available")
            if temp_df_for_plot.empty:
                logger.warning("  âŒ Cannot regenerate visualizations: no prediction data")
        
        return plots
        
    except Exception as e:
        logger.error(f"âŒ Error regenerating visualizations from cache: {str(e)}")
        logger.error(traceback.format_exc())
        return {
            'basic_plot': {'file': None, 'image': None},
            'ma_plot': {'file': None, 'image': None}
        }

#######################################################################
# VARMAX ê´€ë ¨ í´ë˜ìŠ¤ ë° í•¨ìˆ˜
#######################################################################

class VARMAXSemiMonthlyForecaster:
    """VARMAX ê¸°ë°˜ ë°˜ì›”ë³„ ì‹œê³„ì—´ ì˜ˆì¸¡ í´ë˜ìŠ¤ - ì„¸ ë²ˆì§¸ íƒ­ìš©"""
    
    def __init__(self, file_path, result_var='MOPJ', pred_days=50):
        # ì¼ê´€ëœ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ìœ„í•œ ì‹œë“œ ê³ ì •
        set_seed()
        
        self.file_path = file_path
        self.result_var = result_var
        self.pred_days = pred_days
        self.df1 = None
        self.df_origin = None
        self.target_df = None
        self.df_train = None
        self.df_test = None
        self.ts_exchange = None
        self.exogenous_data = None
        self.varx_result = None
        self.pred_df = None
        self.final_forecast_var = None
        self.final_value = None
        self.final_index = None
        self.filtered_vars = None
        self.var_num = None  # ê¸°ë³¸ê°’
        self.r2_train = None
        self.r2_test = None
        self.pred_index = None
        self.selected_vars = []
        self.mape_value = None

    def load_data(self):
        """ë°ì´í„° ë¡œë“œ (VARMAX ëª¨ë¸ìš© - ëª¨ë“  ë°ì´í„° ì‚¬ìš©, ìµœê·¼ 800ê°œë¡œ ì œí•œ)"""
        try:
            # VARMAX ëª¨ë¸ì€ ì¥ê¸°ì˜ˆì¸¡ì´ë¯€ë¡œ ëª¨ë“  ë°ì´í„° ì‚¬ìš© (2022ë…„ ì´ì „ í¬í•¨)
            df_full = load_data(self.file_path, model_type='varmax')
            # ê¸°ì¡´ ë¡œì§ ìœ ì§€: ìµœê·¼ 800ê°œ ë°ì´í„°ë§Œ ì‚¬ìš©
            self.df_origin = df_full.iloc[-800:]
            logger.info(f"VARMAX data loaded: {self.df_origin.shape} (last 800 records from full dataset)")
            logger.info(f"Date range: {self.df_origin.index.min()} to {self.df_origin.index.max()}")
        except Exception as e:
            logger.error(f"Data loading failed: {str(e)}")
            raise e

    def select_variables(self, current_date=None):
        """ë³€ìˆ˜ ì„ íƒ - í˜„ì¬ ë‚ ì§œê¹Œì§€ì˜ ë°ì´í„°ë§Œ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ëˆ„ì¶œ ë°©ì§€"""
        try:
            # ğŸ”‘ ìˆ˜ì •: í˜„ì¬ ë‚ ì§œê¹Œì§€ì˜ ë°ì´í„°ë§Œ ì‚¬ìš©
            if current_date is not None:
                if isinstance(current_date, str):
                    current_date = pd.to_datetime(current_date)
                recent_data = self.df_origin[self.df_origin.index <= current_date]
                logger.info(f"ğŸ”§ Variable selection using data up to {current_date.strftime('%Y-%m-%d')} ({len(recent_data)} records)")
            else:
                recent_data = self.df_origin
                logger.info(f"ğŸ”§ Variable selection using all available data ({len(recent_data)} records)")
            
            correlations = recent_data.corr()[self.result_var]
            correlations = correlations.drop(self.result_var)
            correlations = correlations.sort_values(ascending=False)
            select = correlations.index.tolist()
            self.selected_vars = select
            
            # ë³€ìˆ˜ ê·¸ë£¹ ì •ì˜ (ì›ë³¸ ì½”ë“œì™€ ë™ì¼)
            variable_groups = {
                'crude_oil': ['WTI', 'Brent', 'Dubai'],
                'gasoline': ['Gasoline_92RON', 'Gasoline_95RON', 'Europe_M.G_10ppm', 'RBOB (NYMEX)_M1'],
                'naphtha': ['MOPAG', 'MOPS', 'Europe_CIF NWE'],
                'lpg': ['C3_LPG', 'C4_LPG'],
                'product': ['EL_CRF NEA', 'EL_CRF SEA', 'PL_FOB Korea', 'BZ_FOB Korea', 'BZ_FOB SEA', 'BZ_FOB US M1', 'BZ_FOB US M2', 'TL_FOB Korea', 'TL_FOB US M1', 'TL_FOB US M2',
                'MX_FOB Korea', 'PX_FOB Korea', 'SM_FOB Korea', 'RPG Value_FOB PG', 'FO_HSFO 180 CST', 'MTBE_FOB Singapore'],
                'spread': ['Monthly Spread','BZ_H2-TIME SPREAD', 'Brent_WTI', 'MOPJ_MOPAG', 'MOPJ_MOPS', 'Naphtha_Spread', 'MG92_E Nap', 'C3_MOPJ', 'C4_MOPJ', 'Nap_Dubai',
                'MG92_Nap_MOPS', '95R_92R_Asia', 'M1_M2_RBOB', 'RBOB_Brent_m1', 'RBOB_Brent_m2', 'EL_MOPJ', 'PL_MOPJ', 'BZ_MOPJ', 'TL_MOPJ', 'PX_MOPJ', 'HD_EL', 'LD_EL', 'LLD_EL', 'PP_PL',
                'SM_EL+BZ', 'US_FOBK_BZ', 'NAP_HSFO_180', 'MTBE_MOPJ'],
                'economics': ['Dow_Jones', 'Euro', 'Gold'],
                'freight': ['Freight_55_PG', 'Freight_55_Maili', 'Freight_55_Yosu', 'Freight_55_Daes', 'Freight_55_Chiba',
                'Freight_75_PG', 'Freight_75_Maili', 'Freight_75_Yosu', 'Freight_75_Daes', 'Freight_75_Chiba', 'Flat Rate_PG', 'Flat Rate_Maili', 'Flat Rate_Yosu', 'Flat Rate_Daes',
                'Flat Rate_Chiba']
            }
            
            # ê·¸ë£¹ë³„ ìµœì  ë³€ìˆ˜ ì„ íƒ
            self.filtered_vars = []
            for group, variables in variable_groups.items():
                filtered_group_vars = [var for var in variables if var in self.selected_vars]
                if filtered_group_vars:
                    best_var = max(filtered_group_vars, key=lambda x: abs(correlations[x]))
                    self.filtered_vars.append(best_var)
            
            self.selected_vars = sorted(self.filtered_vars, key=lambda x: abs(correlations[x]), reverse=True)
            logger.info(f"Selected {len(self.selected_vars)} variables for VARMAX prediction")
            logger.info(f"Top 5 selected variables: {self.selected_vars[:5]}")
            
        except Exception as e:
            logger.error(f"Variable selection failed: {str(e)}")
            raise e

    def prepare_data_for_prediction(self, current_date):
        """ì˜ˆì¸¡ìš© ë°ì´í„° ì¤€ë¹„"""
        try:
            # í˜„ì¬ ë‚ ì§œê¹Œì§€ì˜ ë°ì´í„°ë§Œ ì‚¬ìš©
            if isinstance(current_date, str):
                current_date = pd.to_datetime(current_date)
            
            # í˜„ì¬ ë‚ ì§œê¹Œì§€ì˜ ë°ì´í„° í•„í„°ë§
            historical_data = self.df_origin[self.df_origin.index <= current_date]
            
            filtered_values = self.selected_vars
            input_columns = filtered_values[:self.var_num]
            output_column = [self.result_var]
            
            self.final_value = historical_data.iloc[-1][self.result_var]
            self.final_index = historical_data.index[-1]

            self.target_df = historical_data[input_columns + output_column]
            
            self.df_train = self.target_df
            
            # ì™¸ìƒë³€ìˆ˜ (í™˜ìœ¨) ì„¤ì •
            if 'Exchange' in self.df_origin.columns:
                self.ts_exchange = historical_data['Exchange']
                self.exogenous_data = pd.DataFrame(self.ts_exchange, index=self.ts_exchange.index)
            else:
                self.exogenous_data = None
                
        except Exception as e:
            logger.error(f"Data preparation failed: {str(e)}")
            raise e

    def fit_varmax_model(self):
        """VARMAX ëª¨ë¸ í•™ìŠµ"""
        try:
            if not VARMAX_AVAILABLE:
                raise ImportError("VARMAX dependencies not available")
                
            logger.info("ğŸ”„ [VARMAX_FIT] Starting VARMAX model fitting...")
            logger.info(f"ğŸ”„ [VARMAX_FIT] Training data shape: {self.df_train.shape}")
            logger.info(f"ğŸ”„ [VARMAX_FIT] Exogenous data available: {self.exogenous_data is not None}")
            
            best_p = 7
            best_q = 0
            
            logger.info(f"ğŸ”„ [VARMAX_FIT] Creating VARMAX model with order=({best_p}, {best_q})")
            varx_model = VARMAX(endog=self.df_train, exog=self.exogenous_data, order=(best_p, best_q))
            
            logger.info("ğŸ”„ [VARMAX_FIT] Starting model fitting (this may take a while)...")
            
            # ğŸ”‘ global prediction_stateì— ì ‘ê·¼í•˜ì—¬ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
            global prediction_state
            prediction_state['varmax_prediction_progress'] = 50
            
            self.varx_result = varx_model.fit(disp=False, maxiter=1000)
            
            if hasattr(self.varx_result, 'converged') and not self.varx_result.converged:
                logger.warning("âš ï¸ [VARMAX_FIT] VARMAX model did not converge (res.converged=False)")
            else:
                logger.info("âœ… [VARMAX_FIT] VARMAX model converged successfully")
                
            logger.info("âœ… [VARMAX_FIT] VARMAX model fitted successfully")
            
            # ğŸ”‘ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
            prediction_state['varmax_prediction_progress'] = 60
            
        except Exception as e:
            logger.error(f"âŒ [VARMAX_FIT] VARMAX fitting failed: {str(e)}")
            logger.error(f"âŒ [VARMAX_FIT] Fitting error traceback: {traceback.format_exc()}")
            
            # ğŸ”‘ ì—ëŸ¬ ìƒíƒœ ì—…ë°ì´íŠ¸
            prediction_state['varmax_error'] = f"Model fitting failed: {str(e)}"
            prediction_state['varmax_is_predicting'] = False
            prediction_state['varmax_prediction_progress'] = 0
            
            raise e

    def forecast_varmax(self):
        """VARMAX ì˜ˆì¸¡ ìˆ˜í–‰"""
        try:
            # ë¯¸ë˜ ì™¸ìƒë³€ìˆ˜ ì¤€ë¹„
            if self.exogenous_data is not None:
                # ë§ˆì§€ë§‰ ê°’ì„ ì˜ˆì¸¡ ê¸°ê°„ë§Œí¼ ë°˜ë³µ
                last_exog_value = self.ts_exchange.iloc[-1]
                future_dates = pd.bdate_range(start=self.final_index + pd.Timedelta(days=1), periods=self.pred_days)
                exog_future = pd.DataFrame([last_exog_value] * self.pred_days, 
                                         index=future_dates, 
                                         columns=self.exogenous_data.columns)
            else:
                exog_future = None
                future_dates = pd.bdate_range(start=self.final_index + pd.Timedelta(days=1), periods=self.pred_days)
                
            # VARMAX ì˜ˆì¸¡
            varx_forecast = self.varx_result.forecast(steps=self.pred_days, exog=exog_future)
            self.pred_index = future_dates
            self.pred_df = pd.DataFrame(varx_forecast.values, index=self.pred_index, columns=self.df_train.columns)
            logger.info(f"VARMAX forecast completed for {self.pred_days} days")
            
        except Exception as e:
            logger.error(f"VARMAX forecasting failed: {str(e)}")
            raise e

    def residual_correction(self):
        """ëœë¤í¬ë ˆìŠ¤íŠ¸ë¥¼ ì´ìš©í•œ ì”ì°¨ ë³´ì •"""
        try:
            if not VARMAX_AVAILABLE:
                logger.warning("VARMAX not available, skipping residual correction")
                self.final_forecast_var = self.pred_df[[self.result_var]]
                self.r2_train = 0.0
                self.r2_test = 0.0
                return
                
            # ì”ì°¨ ê³„ì‚°
            residuals_origin = self.df_train - self.varx_result.fittedvalues
            residuals_real = residuals_origin.iloc[1:]
            X = residuals_real.iloc[:, :-1]
            y = residuals_real.iloc[:, -1]
            
            # í…ŒìŠ¤íŠ¸ í¬ê¸° ê³„ì‚°
            test_size_value = min(0.3, (self.pred_days + 1) / len(self.target_df))
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size_value, shuffle=False)
            
            # ëœë¤í¬ë ˆìŠ¤íŠ¸ ëª¨ë¸ í•™ìŠµ
            rfr_model = RandomForestRegressor(n_estimators=100, random_state=42)
            rfr_model.fit(X_train, y_train)
            
            # ì„±ëŠ¥ í‰ê°€
            y_train_pred = rfr_model.predict(X_train)
            y_test_pred = rfr_model.predict(X_test)
            self.r2_train = r2_score(y_train, y_train_pred)
            self.r2_test = r2_score(y_test, y_test_pred)
            
            # ì˜ˆì¸¡ì— ì”ì°¨ ë³´ì • ì ìš©
            var_predictions = self.pred_df[[self.result_var]]
            
            # ìµœê·¼ ì”ì°¨ ë°ì´í„°ë¡œ ì˜ˆì¸¡
            recent_residuals = residuals_real.iloc[-self.pred_days:, :-1]
            if len(recent_residuals) < self.pred_days:
                # ë°ì´í„°ê°€ ë¶€ì¡±í•˜ë©´ ë§ˆì§€ë§‰ í–‰ì„ ë°˜ë³µ
                last_residual = residuals_real.iloc[-1:, :-1]
                additional_rows = self.pred_days - len(recent_residuals)
                repeated_residuals = pd.concat([last_residual] * additional_rows, ignore_index=True)
                recent_residuals = pd.concat([recent_residuals, repeated_residuals])[:self.pred_days]
            
            rfr_predictions = rfr_model.predict(recent_residuals.iloc[:len(var_predictions)])
            rfr_pred_df = pd.DataFrame(rfr_predictions, 
                                     index=var_predictions.index, 
                                     columns=var_predictions.columns)
            
            # ìµœì¢… ì˜ˆì¸¡ê°’ = VARMAX ì˜ˆì¸¡ + ì”ì°¨ ë³´ì •
            self.final_forecast_var = var_predictions.add(rfr_pred_df)
            
            logger.info(f"Residual correction completed. Train R2: {self.r2_train:.4f}, Test R2: {self.r2_test:.4f}")
            
        except Exception as e:
            logger.error(f"Residual correction failed: {str(e)}")
            # ë³´ì • ì‹¤íŒ¨ ì‹œ ì›ë³¸ VARMAX ì˜ˆì¸¡ê°’ ì‚¬ìš©
            self.final_forecast_var = self.pred_df[[self.result_var]]
            self.r2_train = 0.0
            self.r2_test = 0.0

    def calculate_performance_metrics(self, actual_data=None):
        """ì„±ëŠ¥ ì§€í‘œ ê³„ì‚° (ì‹¤ì œ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°)"""
        if actual_data is None:
            return {
                'f1': 0.0,
                'accuracy': 0.0,
                'mape': 0.0,
                'weighted_score': 0.0,
                'r2_train': self.r2_train or 0.0,
                'r2_test': self.r2_test or 0.0
            }
        
        try:
            # ë°©í–¥ì„± ì˜ˆì¸¡ ì„±ëŠ¥
            pred_series = self.final_forecast_var[self.result_var]
            actual_series = actual_data
            
            pred_trend = (pred_series.diff() > 0).astype(int)[1:]
            actual_trend = (actual_series.diff() > 0).astype(int)[1:]
            
            # ê³µí†µ ì¸ë±ìŠ¤ë¡œ ë§ì¶¤
            common_idx = pred_trend.index.intersection(actual_trend.index)
            if len(common_idx) > 0:
                pred_trend_common = pred_trend.loc[common_idx]
                actual_trend_common = actual_trend.loc[common_idx]
                
                if VARMAX_AVAILABLE:
                    precision = precision_score(actual_trend_common, pred_trend_common, zero_division=0)
                    recall = recall_score(actual_trend_common, pred_trend_common, zero_division=0)
                    f1 = f1_score(actual_trend_common, pred_trend_common, zero_division=0)
                    accuracy = (actual_trend_common == pred_trend_common).mean() * 100
                else:
                    precision = recall = f1 = accuracy = 0.0
            else:
                precision = recall = f1 = accuracy = 0.0
            
            # MAPE ê³„ì‚°
            common_values_pred = pred_series.loc[common_idx] if len(common_idx) > 0 else pred_series
            common_values_actual = actual_series.loc[common_idx] if len(common_idx) > 0 else actual_series
            
            mask = common_values_actual != 0
            if mask.any():
                mape = np.mean(np.abs((common_values_actual[mask] - common_values_pred[mask]) / common_values_actual[mask])) * 100
            else:
                mape = 0.0
            
            return {
                'f1': f1,
                'accuracy': accuracy,
                'mape': mape,
                'weighted_score': f1 * 100,  # F1 ì ìˆ˜ë¥¼ ê°€ì¤‘ ì ìˆ˜ë¡œ ì‚¬ìš©
                'r2_train': self.r2_train or 0.0,
                'r2_test': self.r2_test or 0.0,
                'precision': precision,
                'recall': recall
            }
            
        except Exception as e:
            logger.error(f"Performance calculation failed: {str(e)}")
            return {
                'f1': 0.0,
                'accuracy': 0.0,
                'mape': 0.0,
                'weighted_score': 0.0,
                'r2_train': self.r2_train or 0.0,
                'r2_test': self.r2_test or 0.0
            }

    def calculate_moving_averages(self, predictions, current_date, windows=[5, 10, 23]):
        """ì´ë™í‰ê·  ê³„ì‚° (ê¸°ì¡´ app.py ë°©ì‹ê³¼ ë™ì¼)"""
        try:
            results = {}
            
            # ì˜ˆì¸¡ ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
            pred_df = pd.DataFrame(predictions)
            pred_df['Date'] = pd.to_datetime(pred_df['Date'])
            pred_df = pred_df.sort_values('Date')
            
            # ê³¼ê±° ë°ì´í„° ì¶”ê°€ (ì´ë™í‰ê·  ê³„ì‚°ìš©)
            historical_data = self.df_origin[self.df_origin.index <= current_date]
            historical_series = historical_data[self.result_var].tail(30)  # ìµœê·¼ 30ì¼
            
            # ì˜ˆì¸¡ ì‹œë¦¬ì¦ˆ ìƒì„±
            prediction_series = pd.Series(
                data=pred_df['Prediction'].values,
                index=pred_df['Date']
            )
            
            # ê³¼ê±°ì™€ ì˜ˆì¸¡ ë°ì´í„° ê²°í•©
            combined_series = pd.concat([historical_series, prediction_series])
            combined_series = combined_series[~combined_series.index.duplicated(keep='last')]
            combined_series = combined_series.sort_index()
            
            # ê° ìœˆë„ìš°ë³„ ì´ë™í‰ê·  ê³„ì‚°
            for window in windows:
                rolling_avg = combined_series.rolling(window=window, min_periods=1).mean()
                
                window_results = []
                for i, row in pred_df.iterrows():
                    date = row['Date']
                    pred_value = row['Prediction']
                    actual_value = row['Actual']
                    
                    # í•´ë‹¹ ë‚ ì§œì˜ ì´ë™í‰ê· 
                    ma_value = rolling_avg.loc[date] if date in rolling_avg.index else None
                    
                    window_results.append({
                        'date': date,
                        'prediction': pred_value,
                        'actual': actual_value,
                        'ma': ma_value
                    })
                
                results[f'ma{window}'] = window_results
            
            return results
            
        except Exception as e:
            logger.error(f"Moving average calculation failed: {str(e)}")
            return {}

    def calculate_moving_averages_varmax(self, predictions, current_date, windows=[5, 10, 20, 30]):
        try:
            results = {}
            
            # ì˜ˆì¸¡ ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
            pred_df = pd.DataFrame(predictions)
            pred_df['Date'] = pd.to_datetime(pred_df['Date'])
            pred_df = pred_df.sort_values('Date')
            
            # ê³¼ê±° ë°ì´í„° ì¶”ê°€ (ì´ë™í‰ê·  ê³„ì‚°ìš©)
            historical_data = self.df_origin[self.df_origin.index <= current_date]
            historical_series = historical_data[self.result_var].tail(30)  # ìµœê·¼ 30ì¼
            
            # ì˜ˆì¸¡ ì‹œë¦¬ì¦ˆ ìƒì„±
            prediction_series = pd.Series(
                data=pred_df['Prediction'].values,
                index=pred_df['Date']
            )
            
            # ê³¼ê±°ì™€ ì˜ˆì¸¡ ë°ì´í„° ê²°í•©
            combined_series = pd.concat([historical_series, prediction_series])
            combined_series = combined_series[~combined_series.index.duplicated(keep='last')]
            combined_series = combined_series.sort_index()
            
            # ê° ìœˆë„ìš°ë³„ ì´ë™í‰ê·  ê³„ì‚°
            for window in windows:
                rolling_avg = combined_series.rolling(window=window, min_periods=1).mean()
                
                window_results = []
                for i, row in pred_df.iterrows():
                    date = row['Date']
                    pred_value = row['Prediction']
                    actual_value = row['Actual']
                    
                    # í•´ë‹¹ ë‚ ì§œì˜ ì´ë™í‰ê· 
                    ma_value = rolling_avg.loc[date] if date in rolling_avg.index else None
                    
                    window_results.append({
                        'date': date,
                        'prediction': pred_value,
                        'actual': actual_value,
                        'ma': ma_value
                    })
                
                results[f'ma{window}'] = window_results
            
            return results
            
        except Exception as e:
            logger.error(f"Moving average calculation failed: {str(e)}")
            return {}

    def calculate_half_month_averages(self, predictions, current_date):
        """VarmaxResult ì»´í¬ë„ŒíŠ¸ìš© ë°˜ì›” í‰ê·  ë°ì´í„° ê³„ì‚°"""
        try:
            # ì˜ˆì¸¡ ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
            pred_df = pd.DataFrame(predictions)
            pred_df['Date'] = pd.to_datetime(pred_df['Date'])
            pred_df = pred_df.sort_values('Date')
            
            # ë°˜ì›” ê¸°ê°„ë³„ë¡œ ê·¸ë£¹í™”
            half_month_groups = {}
            
            for _, row in pred_df.iterrows():
                date = row['Date']
                
                # ë°˜ì›” ë¼ë²¨ ìƒì„± (ì˜ˆ: 25_05_1 = 2025ë…„ 5ì›” ìƒë°˜ê¸°)
                year = date.year % 100  # ì—°ë„ ë§ˆì§€ë§‰ ë‘ ìë¦¬
                month = date.month
                half = 1 if date.day <= 15 else 2
                
                half_month_label = f"{year:02d}_{month:02d}_{half}"
                
                if half_month_label not in half_month_groups:
                    half_month_groups[half_month_label] = []
                
                half_month_groups[half_month_label].append(row['Prediction'])
            
            # ê° ë°˜ì›” ê¸°ê°„ì˜ í‰ê·  ê³„ì‚°
            half_month_data = []
            for label, values in half_month_groups.items():
                avg_value = np.mean(values)
                half_month_data.append({
                    'half_month_label': label,
                    'half_month_avg': float(avg_value),
                    'count': len(values)
                })
            
            # ë¼ë²¨ìˆœìœ¼ë¡œ ì •ë ¬
            half_month_data.sort(key=lambda x: x['half_month_label'])
            
            logger.info(f"ë°˜ì›” í‰ê·  ë°ì´í„° ê³„ì‚° ì™„ë£Œ: {len(half_month_data)}ê°œ ê¸°ê°„")
            
            return half_month_data
            
        except Exception as e:
            logger.error(f"Half month averages calculation failed: {str(e)}")
            return []

    def prepare_variable_for_prediction(self, current_date):
        """ì˜ˆì¸¡ìš© ë°ì´í„° ì¤€ë¹„"""
        try:
            # í˜„ì¬ ë‚ ì§œê¹Œì§€ì˜ ë°ì´í„°ë§Œ ì‚¬ìš©
            if isinstance(current_date, str):
                current_date = pd.to_datetime(current_date)
            
            # í˜„ì¬ ë‚ ì§œê¹Œì§€ì˜ ë°ì´í„° í•„í„°ë§
            historical_data = self.df_origin[self.df_origin.index <= current_date]
            
            filtered_values = self.selected_vars
            input_columns = filtered_values[:self.var_num]
            output_column = [self.result_var]
            
            self.final_value = historical_data.iloc[-1-self.pred_days][self.result_var]
            self.final_index = historical_data.index[-1-self.pred_days]

            self.target_df = historical_data[input_columns + output_column]
            
            self.df_train = self.target_df[:-self.pred_days]
            
            # ì™¸ìƒë³€ìˆ˜ (í™˜ìœ¨) ì„¤ì •
            if 'Exchange' in self.df_origin.columns:
                self.ts_exchange = historical_data['Exchange']
                self.ts_exchange = self.ts_exchange[:-self.pred_days]
                self.exogenous_data = pd.DataFrame(self.ts_exchange, index=self.ts_exchange.index)
            else:
                self.exogenous_data = None
                
        except Exception as e:
            logger.error(f"Data preparation failed: {str(e)}")
            raise e

    def calculate_mape(self, predicted, actual):
        mape = np.mean(np.abs((actual - predicted) / actual)) * 100
        return mape 

    def generate_variables_varmax(self, current_date, var_num):
        """ë³€ìˆ˜ ìˆ˜ ì˜ˆì¸¡ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        try:
            self.var_num = var_num
            self.load_data()
            self.select_variables(current_date)
            self.prepare_variable_for_prediction(current_date)
            self.fit_varmax_model()
            logger.info("VARMAX ë³€ìˆ˜ ì„ ì • ëª¨ë¸ í•™ìŠµ ì™„ë£Œ")

            self.forecast_varmax()
            logger.info("VARMAX ë³€ìˆ˜ ì„ ì • ëª¨ë¸ ì˜ˆì¸¡ ì™„ë£Œ")

            self.residual_correction()
            logger.info(f"ì”ì°¨ ë³´ì • ì™„ë£Œ (R2 train={self.r2_train:.3f}, test={self.r2_test:.3f})")
            historical_data = self.df_origin[self.df_origin.index <= current_date]
            test_data = historical_data[-self.pred_days:]
            self.final_forecast_var.index = test_data.index
            self.mape_value = self.calculate_mape(self.final_forecast_var[self.result_var], test_data[self.result_var])

        except Exception as e:
            logger.error(f"VARMAX variables generation failed: {str(e)}")
            raise e

    def generate_predictions_varmax(self, current_date, var_num):
        """VARMAX ì˜ˆì¸¡ ìˆ˜í–‰"""
        try:
            global prediction_state
            
            logger.info(f"ğŸ”„ [VARMAX_GEN] Starting VARMAX prediction generation")
            logger.info(f"ğŸ”„ [VARMAX_GEN] Parameters: current_date={current_date}, var_num={var_num}")
            
            self.var_num = var_num
            logger.info(f"ğŸ”„ [VARMAX_GEN] Step 1: Loading data...")
            prediction_state['varmax_prediction_progress'] = 35
            self.load_data()
            
            logger.info(f"ğŸ”„ [VARMAX_GEN] Step 2: Selecting variables...")
            prediction_state['varmax_prediction_progress'] = 40
            self.select_variables(current_date)
            
            logger.info(f"ğŸ”„ [VARMAX_GEN] Step 3: Preparing data for prediction...")
            prediction_state['varmax_prediction_progress'] = 45
            self.prepare_data_for_prediction(current_date)
            
            logger.info(f"ğŸ”„ [VARMAX_GEN] Step 4: Fitting VARMAX model...")
            # fit_varmax_model ë‚´ì—ì„œ 50â†’60ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë¨
            self.fit_varmax_model()
            
            logger.info(f"ğŸ”„ [VARMAX_GEN] Step 5: Forecasting...")
            prediction_state['varmax_prediction_progress'] = 65
            self.forecast_varmax()
            
            logger.info(f"ğŸ”„ [VARMAX_GEN] Step 6: Residual correction...")
            prediction_state['varmax_prediction_progress'] = 70
            self.residual_correction()
            
            logger.info(f"ğŸ”„ [VARMAX_GEN] Step 7: Converting results to standard format...")
            prediction_state['varmax_prediction_progress'] = 75
            # ì˜ˆì¸¡ ê²°ê³¼ë¥¼ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            predictions = []
            for date, value in self.final_forecast_var.iterrows():
                predictions.append({
                    'Date': format_date(date),
                    'Prediction': float(value[self.result_var]),
                    'Actual': None  # ì‹¤ì œê°’ì€ ë¯¸ë˜ì´ë¯€ë¡œ None
                })
            logger.info(f"ğŸ”„ [VARMAX_GEN] Converted {len(predictions)} predictions")
            
            logger.info(f"ğŸ”„ [VARMAX_GEN] Step 8: Calculating performance metrics...")
            prediction_state['varmax_prediction_progress'] = 80
            # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
            metrics = self.calculate_performance_metrics()
            
            logger.info(f"ğŸ”„ [VARMAX_GEN] Step 9: Calculating moving averages...")
            prediction_state['varmax_prediction_progress'] = 85
            # ì´ë™í‰ê·  ê³„ì‚° (VARMAXìš©)
            ma_results = self.calculate_moving_averages_varmax(predictions, current_date)
            
            logger.info(f"ğŸ”„ [VARMAX_GEN] Step 10: Calculating half-month averages...")
            prediction_state['varmax_prediction_progress'] = 90
            # ë°˜ì›” í‰ê·  ë°ì´í„° ê³„ì‚° (VarmaxResult ì»´í¬ë„ŒíŠ¸ìš©)
            half_month_data = self.calculate_half_month_averages(predictions, current_date)
            
            logger.info(f"âœ… [VARMAX_GEN] All steps completed successfully!")
            logger.info(f"âœ… [VARMAX_GEN] Final results: {len(predictions)} predictions, {len(ma_results)} MA windows")
            
            return {
                'success': True,
                'predictions': predictions,  # ì›ë˜ ì˜ˆì¸¡ ë°ì´í„° (ì°¨íŠ¸ìš©)
                'half_month_averages': half_month_data,  # ë°˜ì›” í‰ê·  ë°ì´í„° (VarmaxResult ì»´í¬ë„ŒíŠ¸ìš©)
                'metrics': metrics,
                'ma_results': ma_results,
                'selected_features': self.selected_vars[:var_num],
                'current_date': format_date(current_date),
                'model_info': {
                    'model_type': 'VARMAX',
                    'variables_used': var_num,
                    'prediction_days': self.pred_days,
                    'r2_train': self.r2_train,
                    'r2_test': self.r2_test
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ [VARMAX_GEN] VARMAX prediction failed: {str(e)}")
            logger.error(f"âŒ [VARMAX_GEN] Full traceback: {traceback.format_exc()}")
            return {
                'success': False,
                'error': str(e)
            }

def background_prediction_simple_compatible(file_path, current_date, save_to_csv=True, use_cache=True):
    """í˜¸í™˜ì„±ì„ ìœ ì§€í•˜ëŠ” ë°±ê·¸ë¼ìš´ë“œ ì˜ˆì¸¡ í•¨ìˆ˜ - ìºì‹œ ìš°ì„  ì‚¬ìš©, JSON ì•ˆì „ì„± ë³´ì¥"""
    global prediction_state
    
    try:
        # ì¼ê´€ëœ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ìœ„í•œ ì‹œë“œ ê³ ì •
        set_seed()
        prediction_state['is_predicting'] = True
        prediction_state['prediction_progress'] = 10
        prediction_state['prediction_start_time'] = time.time()  # ì‹œì‘ ì‹œê°„ ê¸°ë¡
        prediction_state['error'] = None
        prediction_state['latest_file_path'] = file_path  # íŒŒì¼ ê²½ë¡œ ì €ì¥
        prediction_state['current_file'] = file_path  # ìºì‹œ ì—°ë™ìš© íŒŒì¼ ê²½ë¡œ
        
        logger.info(f"ğŸ¯ Starting compatible prediction for {current_date}")
        logger.info(f"  ğŸ”„ Cache enabled: {use_cache}")
        
        # ë°ì´í„° ë¡œë“œ (ë‹¨ì¼ ë‚ ì§œ ì˜ˆì¸¡ìš© - LSTM ëª¨ë¸, 2022ë…„ ì´ì „ ë°ì´í„° ì œê±°)
        df = load_data(file_path, model_type='lstm')
        prediction_state['current_data'] = df
        prediction_state['prediction_progress'] = 20
        
        # í˜„ì¬ ë‚ ì§œ ì²˜ë¦¬ ë° ì˜ì—…ì¼ ì¡°ì •
        if current_date is None:
            current_date = df.index.max()
        else:
            current_date = pd.to_datetime(current_date)
        
        # ğŸ¯ íœ´ì¼ì´ë©´ ë‹¤ìŒ ì˜ì—…ì¼ë¡œ ì¡°ì •
        original_date = current_date
        adjusted_date = current_date
        
        # ì£¼ë§ì´ë‚˜ íœ´ì¼ì´ë©´ ë‹¤ìŒ ì˜ì—…ì¼ë¡œ ì´ë™
        while adjusted_date.weekday() >= 5 or is_holiday(adjusted_date):
            adjusted_date += pd.Timedelta(days=1)
        
        if adjusted_date != original_date:
            logger.info(f"ğŸ“… Date adjusted for business day: {original_date.strftime('%Y-%m-%d')} -> {adjusted_date.strftime('%Y-%m-%d')}")
            logger.info(f"  ğŸ“‹ Reason: {'Weekend' if original_date.weekday() >= 5 else 'Holiday'}")
        
        current_date = adjusted_date
        
        # ìºì‹œ í™•ì¸
        if use_cache:
            logger.info("ğŸ” Checking for existing prediction cache...")
            prediction_state['prediction_progress'] = 30
            
            try:
                cached_result = check_existing_prediction(current_date, file_path)
                logger.info(f"  ğŸ“‹ Cache check result: {cached_result is not None}")
                if cached_result:
                    logger.info(f"  ğŸ“‹ Cache success status: {cached_result.get('success', False)}")
                else:
                    logger.info("  âŒ No cache result returned")
            except Exception as cache_check_error:
                logger.error(f"  âŒ Cache check failed with error: {str(cache_check_error)}")
                logger.error(f"  ğŸ“ Error traceback: {traceback.format_exc()}")
                cached_result = None
        else:
            logger.info("ğŸ†• Cache disabled - running new prediction...")
            cached_result = None
            
        if cached_result and cached_result.get('success'):
            logger.info("ğŸ‰ Found existing prediction! Loading from cache...")
            prediction_state['prediction_progress'] = 50
            
            try:
                    # ìºì‹œëœ ë°ì´í„° ë¡œë“œ ë° ì •ë¦¬
                    predictions = cached_result['predictions']
                    metadata = cached_result['metadata']
                    attention_data = cached_result.get('attention_data')
                    
                    # ë°ì´í„° ì •ë¦¬ (JSON ì•ˆì „ì„± ë³´ì¥)
                    cleaned_predictions = clean_cached_predictions(predictions)
                    
                    # í˜¸í™˜ì„± ìœ ì§€ëœ í˜•íƒœë¡œ ë³€í™˜
                    compatible_predictions = convert_to_legacy_format(cleaned_predictions)
                    
                    # JSON ì§ë ¬í™” í…ŒìŠ¤íŠ¸
                    try:
                        test_json = json.dumps(compatible_predictions[:1] if compatible_predictions else [])
                        logger.info("âœ… JSON serialization test passed for cached data")
                    except Exception as json_error:
                        logger.error(f"âŒ JSON serialization failed for cached data: {str(json_error)}")
                        raise Exception("Cached data serialization failed")
                    
                    # êµ¬ê°„ ì ìˆ˜ ì²˜ë¦¬ (JSON ì•ˆì „)
                    interval_scores = metadata.get('interval_scores', {})
                    cleaned_interval_scores = {}
                    for key, value in interval_scores.items():
                        if isinstance(value, dict):
                            cleaned_score = {}
                            for k, v in value.items():
                                cleaned_score[k] = safe_serialize_value(v)
                            cleaned_interval_scores[key] = cleaned_score
                        else:
                            cleaned_interval_scores[key] = safe_serialize_value(value)
                    
                    # ì´ë™í‰ê·  ì¬ê³„ì‚°
                    prediction_state['prediction_progress'] = 60
                    logger.info("Recalculating moving averages from cached data...")
                    historical_data = df[df.index <= current_date].copy()
                    ma_results = calculate_moving_averages_with_history(
                        cleaned_predictions, historical_data, target_col='MOPJ'
                    )
                    
                    # ì‹œê°í™” ì¬ìƒì„±
                    prediction_state['prediction_progress'] = 70
                    logger.info("Regenerating visualizations from cached data...")
                    plots = regenerate_visualizations_from_cache(
                        cleaned_predictions, df, current_date, metadata
                    )
                    
                    # ë©”íŠ¸ë¦­ ì •ë¦¬
                    metrics = metadata.get('metrics')
                    cleaned_metrics = {}
                    if metrics:
                        for key, value in metrics.items():
                            cleaned_metrics[key] = safe_serialize_value(value)
                    
                    # ì–´í…ì…˜ ë°ì´í„° ì •ë¦¬
                    cleaned_attention = None
                    logger.info(f"ğŸ“Š [CACHE_ATTENTION] Processing attention data: available={bool(attention_data)}")
                    if attention_data:
                        logger.info(f"ğŸ“Š [CACHE_ATTENTION] Original keys: {list(attention_data.keys())}")
                        
                        cleaned_attention = {}
                        for key, value in attention_data.items():
                            if key == 'image' and value:
                                cleaned_attention[key] = value  # base64 ì´ë¯¸ì§€ëŠ” ê·¸ëŒ€ë¡œ
                                logger.info(f"ğŸ“Š [CACHE_ATTENTION] Image preserved (length: {len(value)})")
                            elif isinstance(value, dict):
                                cleaned_attention[key] = {}
                                for k, v in value.items():
                                    cleaned_attention[key][k] = safe_serialize_value(v)
                                logger.info(f"ğŸ“Š [CACHE_ATTENTION] Dict '{key}' processed: {len(cleaned_attention[key])} items")
                            else:
                                cleaned_attention[key] = safe_serialize_value(value)
                                logger.info(f"ğŸ“Š [CACHE_ATTENTION] Value '{key}' processed: {type(value)}")
                        
                        logger.info(f"ğŸ“Š [CACHE_ATTENTION] Final cleaned keys: {list(cleaned_attention.keys())}")
                    else:
                        logger.warning(f"ğŸ“Š [CACHE_ATTENTION] No attention data in cache result")
                    
                    # ìƒíƒœ ì„¤ì •
                    prediction_state['latest_predictions'] = compatible_predictions
                    prediction_state['latest_attention_data'] = cleaned_attention
                    prediction_state['current_date'] = safe_serialize_value(metadata.get('prediction_start_date'))
                    prediction_state['selected_features'] = metadata.get('selected_features', [])
                    prediction_state['semimonthly_period'] = safe_serialize_value(metadata.get('semimonthly_period'))
                    prediction_state['next_semimonthly_period'] = safe_serialize_value(metadata.get('next_semimonthly_period'))
                    prediction_state['latest_interval_scores'] = cleaned_interval_scores
                    prediction_state['latest_metrics'] = cleaned_metrics
                    prediction_state['latest_plots'] = plots
                    prediction_state['latest_ma_results'] = ma_results
                    
                    # feature_importance ì„¤ì •
                    if cleaned_attention and 'feature_importance' in cleaned_attention:
                        prediction_state['feature_importance'] = cleaned_attention['feature_importance']
                    else:
                        prediction_state['feature_importance'] = None
                    
                    prediction_state['prediction_progress'] = 100
                    prediction_state['is_predicting'] = False
                    logger.info("âœ… Cache prediction completed successfully!")
                    return
                    
            except Exception as cache_error:
                logger.warning(f"âš ï¸  Cache processing failed: {str(cache_error)}")
                logger.info("ğŸ”„ Falling back to new prediction...")
        else:
            logger.info("  ğŸ“‹ No usable cache found - proceeding with new prediction")
        
        # ìƒˆë¡œìš´ ì˜ˆì¸¡ ìˆ˜í–‰
        logger.info(f"ğŸ¤– Running new prediction...")
        prediction_state['prediction_progress'] = 40
        
        # ì˜ˆì¸¡ ìˆ˜í–‰
        results = generate_predictions_with_save(df, current_date, save_to_csv=save_to_csv, file_path=file_path)
        prediction_state['prediction_progress'] = 80
        
        # ìƒˆë¡œìš´ ì˜ˆì¸¡ ê²°ê³¼ ì •ë¦¬ (JSON ì•ˆì „ì„± ë³´ì¥)
        if isinstance(results.get('predictions'), list):
            raw_predictions = results['predictions']
        else:
            raw_predictions = results.get('predictions_flat', [])
        
        # í˜¸í™˜ì„± ìœ ì§€ëœ í˜•íƒœë¡œ ë³€í™˜
        compatible_predictions = convert_to_legacy_format(raw_predictions)
        
        # JSON ì§ë ¬í™” í…ŒìŠ¤íŠ¸
        try:
            test_json = json.dumps(compatible_predictions[:1] if compatible_predictions else [])
            logger.info("âœ… JSON serialization test passed for new prediction")
        except Exception as json_error:
            logger.error(f"âŒ JSON serialization failed for new prediction: {str(json_error)}")
            # ë°ì´í„° ì¶”ê°€ ì •ë¦¬ ì‹œë„
            for pred in compatible_predictions:
                for key, value in pred.items():
                    pred[key] = safe_serialize_value(value)
        
        # ìƒíƒœ ì„¤ì •
        prediction_state['latest_predictions'] = compatible_predictions
        prediction_state['latest_attention_data'] = results.get('attention_data')
        prediction_state['current_date'] = safe_serialize_value(results.get('current_date'))
        prediction_state['selected_features'] = results.get('selected_features', [])
        prediction_state['semimonthly_period'] = safe_serialize_value(results.get('semimonthly_period'))
        prediction_state['next_semimonthly_period'] = safe_serialize_value(results.get('next_semimonthly_period'))
        prediction_state['latest_interval_scores'] = results.get('interval_scores', {})
        prediction_state['latest_metrics'] = results.get('metrics')
        prediction_state['latest_plots'] = results.get('plots', {})
        prediction_state['latest_ma_results'] = results.get('ma_results', {})
        
        # feature_importance ì„¤ì •
        if results.get('attention_data') and 'feature_importance' in results['attention_data']:
            prediction_state['feature_importance'] = results['attention_data']['feature_importance']
        else:
            prediction_state['feature_importance'] = None
        
        # ì €ì¥
        if save_to_csv:
            logger.info("ğŸ’¾ Saving prediction to cache...")
            save_result = save_prediction_simple(results, current_date)
            if save_result['success']:
                logger.info(f"âœ… Cache saved successfully: {save_result.get('prediction_start_date')}")
            else:
                logger.warning(f"âš ï¸  Cache save failed: {save_result.get('error')}")
        
        prediction_state['prediction_progress'] = 100
        prediction_state['is_predicting'] = False
        logger.info("âœ… New prediction completed successfully")
        
    except Exception as e:
        logger.error(f"âŒ Error in compatible prediction: {str(e)}")
        logger.error(traceback.format_exc())
        prediction_state['error'] = str(e)
        prediction_state['is_predicting'] = False
        prediction_state['prediction_progress'] = 0


def safe_serialize_value(value):
    """ê°’ì„ JSON ì•ˆì „í•˜ê²Œ ì§ë ¬í™” (NaN/Infinity ì²˜ë¦¬ ê°•í™”)"""
    if value is None:
        return None
    
    # numpy/pandas ë°°ì—´ íƒ€ì… ë¨¼ì € ì²´í¬
    if isinstance(value, (np.ndarray, pd.Series, list)):
        if len(value) == 0:
            return []
        elif len(value) == 1:
            # ë‹¨ì¼ ì›ì†Œ ë°°ì—´ì¸ ê²½ìš° ìŠ¤ì¹¼ë¼ë¡œ ì²˜ë¦¬
            return safe_serialize_value(value[0])
        else:
            # ë‹¤ì¤‘ ì›ì†Œ ë°°ì—´ì¸ ê²½ìš° ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            try:
                return [safe_serialize_value(item) for item in value]
            except:
                return [str(item) for item in value]
    
    # ğŸ”§ ê°•í™”ëœ NaN/Infinity ì²˜ë¦¬
    try:
        # pandas isna ì²´í¬ (ê°€ì¥ í¬ê´„ì )
        if pd.isna(value):
            return None
    except (TypeError, ValueError):
        pass
    
    # ğŸ”§ NumPy NaN/Infinity ì²´í¬
    try:
        if isinstance(value, (int, float, np.number)):
            if np.isnan(value) or np.isinf(value):
                return None
            # ì •ìƒ ìˆ«ìê°’ì¸ ê²½ìš°
            if isinstance(value, (np.floating, float)):
                return float(value)
            elif isinstance(value, (np.integer, int)):
                return int(value)
    except (TypeError, ValueError, OverflowError):
        pass
    
    # ğŸ”§ ë¬¸ìì—´ ì²´í¬ (NaNì´ ë¬¸ìì—´ë¡œ ë³€í™˜ëœ ê²½ìš°)
    if isinstance(value, str):
        value_lower = value.lower().strip()
        if value_lower in ['nan', 'inf', '-inf', 'infinity', '-infinity', 'null', 'none']:
            return None
        return value
    
    # ë‚ ì§œ ê°ì²´ ì²˜ë¦¬
    if hasattr(value, 'isoformat'):  # datetime/Timestamp
        try:
            return value.strftime('%Y-%m-%d')
        except:
            return str(value)
    elif hasattr(value, 'strftime'):  # ê¸°íƒ€ ë‚ ì§œ ê°ì²´
        try:
            return value.strftime('%Y-%m-%d')
        except:
            return str(value)
    
    # ğŸ”§ ìµœì¢… JSON ì§ë ¬í™” í…ŒìŠ¤íŠ¸ (ë” ì•ˆì „í•˜ê²Œ)
    try:
        # JSON ì§ë ¬í™” ê°€ëŠ¥í•œì§€ í™•ì¸
        json_str = json.dumps(value)
        # ì§ë ¬í™”ëœ ë¬¸ìì—´ì— NaNì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        if 'NaN' in json_str or 'Infinity' in json_str:
            return None
        return value
    except (TypeError, ValueError, OverflowError):
        # ì§ë ¬í™” ì‹¤íŒ¨ ì‹œ ë¬¸ìì—´ë¡œ
        try:
            str_value = str(value)
            # ë¬¸ìì—´ì—ë„ NaNì´ í¬í•¨ëœ ê²½ìš° ì²˜ë¦¬
            if any(nan_str in str_value.lower() for nan_str in ['nan', 'inf', 'infinity']):
                return None
            return str_value
        except:
            return None

def clean_predictions_data(predictions):
    """ì˜ˆì¸¡ ë°ì´í„°ë¥¼ JSON ì•ˆì „í•˜ê²Œ ì •ë¦¬"""
    if not predictions:
        return []
    
    cleaned = []
    for pred in predictions:
        cleaned_pred = {}
        for key, value in pred.items():
            if key in ['date', 'prediction_from']:
                # ë‚ ì§œ í•„ë“œ
                if hasattr(value, 'strftime'):
                    cleaned_pred[key] = value.strftime('%Y-%m-%d')
                else:
                    cleaned_pred[key] = str(value)
            elif key in ['prediction', 'actual', 'error', 'error_pct']:
                # ìˆ«ì í•„ë“œ
                cleaned_pred[key] = safe_serialize_value(value)
            else:
                # ê¸°íƒ€ í•„ë“œ
                cleaned_pred[key] = safe_serialize_value(value)
        cleaned.append(cleaned_pred)
    
    return cleaned

def clean_cached_predictions(predictions):
    """ìºì‹œì—ì„œ ë¡œë“œëœ ì˜ˆì¸¡ ë°ì´í„°ë¥¼ ì •ë¦¬í•˜ëŠ” í•¨ìˆ˜"""
    cleaned_predictions = []
    
    for pred in predictions:
        try:
            # ëª¨ë“  í•„ë“œë¥¼ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
            cleaned_pred = {}
            for key, value in pred.items():
                if key in ['Date', 'date']:
                    # ë‚ ì§œ í•„ë“œ íŠ¹ë³„ ì²˜ë¦¬
                    if pd.notna(value):
                        if hasattr(value, 'strftime'):
                            cleaned_pred[key] = value.strftime('%Y-%m-%d')
                        else:
                            cleaned_pred[key] = str(value)[:10]
                    else:
                        cleaned_pred[key] = None
                elif key in ['Prediction', 'prediction', 'Actual', 'actual']:
                    # ìˆ«ì í•„ë“œ ì²˜ë¦¬
                    cleaned_pred[key] = safe_serialize_value(value)
                else:
                    # ê¸°íƒ€ í•„ë“œ
                    cleaned_pred[key] = safe_serialize_value(value)
            
            cleaned_predictions.append(cleaned_pred)
            
        except Exception as e:
            logger.warning(f"Error cleaning prediction item: {str(e)}")
            continue
    
    return cleaned_predictions

def clean_interval_scores_safe(interval_scores):
    """êµ¬ê°„ ì ìˆ˜ë¥¼ ì•ˆì „í•˜ê²Œ ì •ë¦¬í•˜ëŠ” í•¨ìˆ˜ - ê°•í™”ëœ ì˜¤ë¥˜ ì²˜ë¦¬"""
    cleaned_interval_scores = []
    
    try:
        # ì…ë ¥ê°’ ê²€ì¦
        if interval_scores is None:
            logger.info("ğŸ“‹ interval_scores is None, returning empty list")
            return []
        
        if not isinstance(interval_scores, (dict, list)):
            logger.warning(f"âš ï¸ interval_scores is not dict or list: {type(interval_scores)}")
            return []
        
        if isinstance(interval_scores, dict):
            if not interval_scores:  # ë¹ˆ dict
                logger.info("ğŸ“‹ interval_scores is empty dict, returning empty list")
                return []
                
            for key, value in interval_scores.items():
                try:
                    if isinstance(value, dict):
                        cleaned_score = {}
                        for k, v in value.items():
                            try:
                                # ë°°ì—´ì´ë‚˜ ë³µì¡í•œ íƒ€ì…ì€ íŠ¹ë³„ ì²˜ë¦¬
                                if isinstance(v, (np.ndarray, pd.Series, list)):
                                    if hasattr(v, '__len__') and len(v) == 1:
                                        cleaned_score[k] = safe_serialize_value(v[0])
                                    elif hasattr(v, '__len__') and len(v) == 0:
                                        cleaned_score[k] = None
                                    else:
                                        # ë‹¤ì¤‘ ì›ì†Œ ë°°ì—´ì€ ë¬¸ìì—´ë¡œ ë³€í™˜
                                        cleaned_score[k] = str(v)
                                else:
                                    cleaned_score[k] = safe_serialize_value(v)
                            except Exception as inner_e:
                                logger.warning(f"âš ï¸ Error processing key {k}: {str(inner_e)}")
                                cleaned_score[k] = None
                        cleaned_interval_scores.append(cleaned_score)
                    else:
                        # dictê°€ ì•„ë‹Œ ê²½ìš° ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
                        cleaned_interval_scores.append(safe_serialize_value(value))
                except Exception as value_e:
                    logger.warning(f"âš ï¸ Error processing interval_scores key {key}: {str(value_e)}")
                    continue
                    
        elif isinstance(interval_scores, list):
            if not interval_scores:  # ë¹ˆ list
                logger.info("ğŸ“‹ interval_scores is empty list, returning empty list")
                return []
                
            for i, score in enumerate(interval_scores):
                try:
                    if isinstance(score, dict):
                        cleaned_score = {}
                        for k, v in score.items():
                            try:
                                # ë°°ì—´ì´ë‚˜ ë³µì¡í•œ íƒ€ì…ì€ íŠ¹ë³„ ì²˜ë¦¬
                                if isinstance(v, (np.ndarray, pd.Series, list)):
                                    if hasattr(v, '__len__') and len(v) == 1:
                                        cleaned_score[k] = safe_serialize_value(v[0])
                                    elif hasattr(v, '__len__') and len(v) == 0:
                                        cleaned_score[k] = None
                                    else:
                                        cleaned_score[k] = str(v)
                                else:
                                    cleaned_score[k] = safe_serialize_value(v)
                            except Exception as inner_e:
                                logger.warning(f"âš ï¸ Error processing score[{i}].{k}: {str(inner_e)}")
                                cleaned_score[k] = None
                        cleaned_interval_scores.append(cleaned_score)
                    else:
                        cleaned_interval_scores.append(safe_serialize_value(score))
                except Exception as score_e:
                    logger.warning(f"âš ï¸ Error processing interval_scores[{i}]: {str(score_e)}")
                    continue
        
        logger.info(f"âœ… Successfully cleaned {len(cleaned_interval_scores)} interval scores")
        return cleaned_interval_scores
        
    except Exception as e:
        logger.error(f"âŒ Critical error cleaning interval scores: {str(e)}")
        logger.error(traceback.format_exc())
        return []

def convert_to_legacy_format(predictions_data):
    """
    ìƒˆÂ·ì˜› êµ¬ì¡°ë¥¼ ëª¨ë‘ ë°›ì•„ í”„ë¡ íŠ¸ì—”ë“œ(ëŒ€ë¬¸ì) + ë°±ì—”ë“œ(ì†Œë¬¸ì) í‚¤ë¥¼ ë™ì‹œ ë³´ì¡´.
    JSON ì§ë ¬í™” ì•ˆì „ì„± ë³´ì¥
    """
    if not predictions_data:
        return []
    
    legacy_out = []
    actual_values_found = 0  # ì‹¤ì œê°’ì´ ë°œê²¬ëœ ìˆ˜ ì¹´ìš´íŠ¸
    
    for i, pred in enumerate(predictions_data):
        try:
            # ë‚ ì§œ í•„ë“œ ì•ˆì „ ì²˜ë¦¬
            date_value = pred.get("date") or pred.get("Date")
            if hasattr(date_value, 'strftime'):
                date_str = date_value.strftime('%Y-%m-%d')
            elif isinstance(date_value, str):
                date_str = date_value[:10] if len(date_value) > 10 else date_value
            else:
                date_str = str(date_value) if date_value is not None else None
            
            # ì˜ˆì¸¡ê°’ ì•ˆì „ ì²˜ë¦¬
            prediction_value = pred.get("prediction") or pred.get("Prediction")
            prediction_safe = safe_serialize_value(prediction_value)
            
            # ì‹¤ì œê°’ ì•ˆì „ ì²˜ë¦¬ - ë‹¤ì–‘í•œ í•„ë“œëª… í™•ì¸
            actual_value = (pred.get("actual") or 
                          pred.get("Actual") or 
                          pred.get("actual_value") or 
                          pred.get("Actual_Value"))
            
            # ì‹¤ì œê°’ì´ ìˆëŠ”ì§€ í™•ì¸
            if actual_value is not None and actual_value != 'None' and not (
                isinstance(actual_value, float) and (np.isnan(actual_value) or np.isinf(actual_value))
            ):
                actual_safe = safe_serialize_value(actual_value)
                actual_values_found += 1
                if i < 5:  # ì²˜ìŒ 5ê°œë§Œ ë¡œê¹…
                    logger.debug(f"  ğŸ“Š [LEGACY_FORMAT] Found actual value for {date_str}: {actual_safe}")
            else:
                actual_safe = None
            
            # ê¸°íƒ€ í•„ë“œë“¤ ì•ˆì „ ì²˜ë¦¬
            prediction_from = pred.get("prediction_from") or pred.get("Prediction_From")
            if hasattr(prediction_from, 'strftime'):
                prediction_from = prediction_from.strftime('%Y-%m-%d')
            elif prediction_from:
                prediction_from = str(prediction_from)
            
            legacy_item = {
                # â”€â”€ í”„ë¡ íŠ¸ì—”ë“œ í˜¸í™˜ ëŒ€ë¬¸ì í‚¤ (JSON ì•ˆì „) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                "Date": date_str,
                "Prediction": prediction_safe,
                "Actual": actual_safe,

                # â”€â”€ ë°±ì—”ë“œ í›„ì† í•¨ìˆ˜(ì†Œë¬¸ì 'date' ì°¸ì¡°)ìš© â”€â”€
                "date": date_str,
                "prediction": prediction_safe,
                "actual": actual_safe,

                # ê¸°íƒ€ í•„ë“œ ì•ˆì „ ì²˜ë¦¬
                "Prediction_From": prediction_from,
                "SemimonthlyPeriod": safe_serialize_value(pred.get("semimonthly_period")),
                "NextSemimonthlyPeriod": safe_serialize_value(pred.get("next_semimonthly_period")),
                "is_synthetic": bool(pred.get("is_synthetic", False)),
                
                # ì¶”ê°€ ë©”íƒ€ë°ì´í„° (ìˆëŠ” ê²½ìš°)
                "day_offset": safe_serialize_value(pred.get("day_offset")),
                "is_business_day": bool(pred.get("is_business_day", True)),
                "error": safe_serialize_value(pred.get("error")),
                "error_pct": safe_serialize_value(pred.get("error_pct"))
            }
            
            legacy_out.append(legacy_item)
            
        except Exception as e:
            logger.warning(f"Error converting prediction item {i}: {str(e)}")
            continue
    
    # ì‹¤ì œê°’ í†µê³„ ë¡œê¹…
    total_predictions = len(legacy_out)
    logger.info(f"  ğŸ“Š [LEGACY_FORMAT] Converted {total_predictions} predictions, {actual_values_found} with actual values")
    
    return legacy_out

#######################################################################
# API ì—”ë“œí¬ì¸íŠ¸
#######################################################################

@app.route('/api/health', methods=['GET'])
def health_check():
    """ì„œë²„ ìƒíƒœ í™•ì¸ API"""
    return jsonify({
        'status': 'ok',
        'timestamp': datetime.now().isoformat(),
        'attention_endpoint_available': True
    })

@app.route('/api/test-attention', methods=['GET'])
def test_attention():
    """ì–´í…ì…˜ ë§µ ì—”ë“œí¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸ìš©"""
    return jsonify({
        'success': True,
        'message': 'Test attention endpoint is working',
        'timestamp': datetime.now().isoformat()
    })

@app.route('/api/test', methods=['GET'])
def test_api():
    """API ì—°ê²° í…ŒìŠ¤íŠ¸"""
    return jsonify({
        'status': 'ok',
        'message': 'API is working!',
        'timestamp': datetime.now().isoformat()
    })

@app.route('/api/test/cache-dirs', methods=['GET'])
def test_cache_dirs():
    """ìºì‹œ ë””ë ‰í† ë¦¬ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    try:
        # í˜„ì¬ ìƒíƒœ í™•ì¸
        current_file = prediction_state.get('current_file', None)
        
        # íŒŒì¼ ê²½ë¡œê°€ ìˆìœ¼ë©´ í•´ë‹¹ íŒŒì¼ë¡œ, ì—†ìœ¼ë©´ ê¸°ë³¸ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
        test_file = request.args.get('file_path', current_file)
        
        if test_file and not os.path.exists(test_file):
            return jsonify({
                'error': f'File does not exist: {test_file}',
                'current_file': current_file
            }), 400
        
        # ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„± í…ŒìŠ¤íŠ¸
        cache_dirs = get_file_cache_dirs(test_file)
        
        # ë””ë ‰í† ë¦¬ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        dir_status = {}
        for name, path in cache_dirs.items():
            dir_status[name] = {
                'path': str(path),
                'exists': path.exists(),
                'is_dir': path.is_dir() if path.exists() else False
            }
        
        return jsonify({
            'success': True,
            'test_file': test_file,
            'current_file': current_file,
            'cache_dirs': dir_status,
            'cache_root_exists': Path(CACHE_ROOT_DIR).exists()
        })
        
    except Exception as e:
        logger.error(f"Cache directory test failed: {str(e)}")
        return jsonify({
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

def detect_file_type_by_content(file_path):
    """
    íŒŒì¼ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ì‹¤ì œ íŒŒì¼ íƒ€ì…ì„ ê°ì§€í•˜ëŠ” í•¨ìˆ˜
    íšŒì‚¬ ë³´ì•ˆìœ¼ë¡œ ì¸í•´ í™•ì¥ìê°€ ë³€ê²½ëœ íŒŒì¼ë“¤ì„ ì²˜ë¦¬
    """
    try:
        # íŒŒì¼ì˜ ì²« ëª‡ ë°”ì´íŠ¸ë¥¼ ì½ì–´ì„œ íŒŒì¼ íƒ€ì… ê°ì§€
        with open(file_path, 'rb') as f:
            header = f.read(8)
        
        # Excel íŒŒì¼ ì‹œê·¸ë‹ˆì²˜ í™•ì¸
        if header[:4] == b'PK\x03\x04':  # ZIP ê¸°ë°˜ íŒŒì¼ (xlsx)
            return 'xlsx'
        elif header[:8] == b'\xd0\xcf\x11\xe0\xa1\xb1\x1a\xe1':  # OLE2 ê¸°ë°˜ íŒŒì¼ (xls)
            return 'xls'
        
        # CSV íŒŒì¼ì¸ì§€ í™•ì¸ (í…ìŠ¤íŠ¸ ê¸°ë°˜)
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                first_line = f.readline()
                # CSV íŠ¹ì„± í™•ì¸: ì‰¼í‘œë‚˜ íƒ­ì´ í¬í•¨ë˜ì–´ ìˆê³ , Date ì»¬ëŸ¼ì´ ìˆëŠ”ì§€
                if (',' in first_line or '\t' in first_line) and ('date' in first_line.lower() or 'Date' in first_line):
                    return 'csv'
        except:
            # UTF-8ë¡œ ì½ê¸° ì‹¤íŒ¨ì‹œ ë‹¤ë¥¸ ì¸ì½”ë”© ì‹œë„
            try:
                with open(file_path, 'r', encoding='cp949') as f:
                    first_line = f.readline()
                    if (',' in first_line or '\t' in first_line) and ('date' in first_line.lower() or 'Date' in first_line):
                        return 'csv'
            except:
                pass
        
        # ê¸°ë³¸ê°’ ë°˜í™˜
        return None
        
    except Exception as e:
        logger.warning(f"File type detection failed: {str(e)}")
        return None

def normalize_security_extension(filename):
    """
    íšŒì‚¬ ë³´ì•ˆì •ì±…ìœ¼ë¡œ ë³€ê²½ëœ í™•ì¥ìë¥¼ ì›ë˜ í™•ì¥ìë¡œ ë³µì›
    
    Args:
        filename (str): ì›ë³¸ íŒŒì¼ëª…
    
    Returns:
        tuple: (ì •ê·œí™”ëœ íŒŒì¼ëª…, ì›ë³¸ í™•ì¥ì, ë³´ì•ˆ í™•ì¥ìì¸ì§€ ì—¬ë¶€)
    """
    # ë³´ì•ˆ í™•ì¥ì ë§¤í•‘
    security_extensions = {
        '.cs': '.csv',     # csv -> cs
        '.xl': '.xlsx',    # xlsx -> xl  
        '.xls': '.xlsx',   # ê¸°ì¡´ xlsë„ xlsxë¡œ í†µì¼
        '.log': '.xlsx',   # log -> xlsx (ë³´ì•ˆ ì •ì±…ìœ¼ë¡œ Excel íŒŒì¼ì„ logë¡œ ìœ„ì¥)
        '.dat': None,      # ë‚´ìš© ë¶„ì„ í•„ìš”
        '.txt': None,      # ë‚´ìš© ë¶„ì„ í•„ìš”
    }
    
    filename_lower = filename.lower()
    original_ext = os.path.splitext(filename_lower)[1]
    
    # ë³´ì•ˆ í™•ì¥ìì¸ì§€ í™•ì¸
    if original_ext in security_extensions:
        if security_extensions[original_ext]:
            # ì§ì ‘ ë§¤í•‘ì´ ìˆëŠ” ê²½ìš°
            normalized_ext = security_extensions[original_ext]
            base_name = os.path.splitext(filename)[0]
            normalized_filename = f"{base_name}{normalized_ext}"
            
            logger.info(f"ğŸ”’ [SECURITY] Extension normalization: {filename} -> {normalized_filename}")
            return normalized_filename, normalized_ext, True
        else:
            # ë‚´ìš© ë¶„ì„ì´ í•„ìš”í•œ ê²½ìš°
            return filename, original_ext, True
    
    # ì¼ë°˜ í™•ì¥ìì¸ ê²½ìš°
    return filename, original_ext, False

def process_security_file(temp_filepath, original_filename):
    """
    ë³´ì•ˆ ì •ì±…ìœ¼ë¡œ í™•ì¥ìê°€ ë³€ê²½ëœ íŒŒì¼ì„ ì²˜ë¦¬
    
    Args:
        temp_filepath (str): ì„ì‹œ íŒŒì¼ ê²½ë¡œ
        original_filename (str): ì›ë³¸ íŒŒì¼ëª…
    
    Returns:
        tuple: (ì²˜ë¦¬ëœ íŒŒì¼ ê²½ë¡œ, ì •ê·œí™”ëœ íŒŒì¼ëª…, ì‹¤ì œ í™•ì¥ì)
    """
    # í™•ì¥ì ì •ê·œí™”
    normalized_filename, detected_ext, is_security_ext = normalize_security_extension(original_filename)
    
    if is_security_ext:
        logger.info(f"ğŸ”’ [SECURITY] Processing security file: {original_filename}")
        
        # íŒŒì¼ ë‚´ìš©ìœ¼ë¡œ ì‹¤ì œ íƒ€ì… ê°ì§€
        if detected_ext is None or detected_ext in ['.dat', '.txt']:
            content_type = detect_file_type_by_content(temp_filepath)
            if content_type:
                detected_ext = f'.{content_type}'
                base_name = os.path.splitext(normalized_filename)[0]
                normalized_filename = f"{base_name}{detected_ext}"
                logger.info(f"ğŸ“Š [CONTENT_DETECTION] Detected file type: {content_type}")
        
        # ìƒˆë¡œìš´ íŒŒì¼ ê²½ë¡œ ìƒì„±
        new_filepath = temp_filepath.replace(os.path.splitext(temp_filepath)[1], detected_ext)
        
        # íŒŒì¼ ì´ë¦„ ë³€ê²½ (í™•ì¥ì ìˆ˜ì •)
        if new_filepath != temp_filepath:
            try:
                shutil.move(temp_filepath, new_filepath)
                logger.info(f"ğŸ“ [SECURITY] File extension corrected: {os.path.basename(temp_filepath)} -> {os.path.basename(new_filepath)}")
                return new_filepath, normalized_filename, detected_ext
            except Exception as e:
                logger.warning(f"âš ï¸ [SECURITY] Failed to rename file: {str(e)}")
                return temp_filepath, normalized_filename, detected_ext
    
    return temp_filepath, normalized_filename, detected_ext

@app.route('/api/upload', methods=['POST'])
def upload_file():
    """ìŠ¤ë§ˆíŠ¸ ìºì‹œ ê¸°ëŠ¥ì´ ìˆëŠ” ë°ì´í„° íŒŒì¼ ì—…ë¡œë“œ API (CSV, Excel ì§€ì›)"""
    if 'file' not in request.files:
        return jsonify({'error': 'No file part'}), 400
        
    file = request.files['file']
    if file.filename == '':
        return jsonify({'error': 'No selected file'}), 400
    
    # ğŸ”’ ë³´ì•ˆ í™•ì¥ì ì •ê·œí™” ì²˜ë¦¬
    normalized_filename, normalized_ext, is_security_file = normalize_security_extension(file.filename)
    
    # ì§€ì›ë˜ëŠ” íŒŒì¼ í˜•ì‹ í™•ì¸ (ë³´ì•ˆ í™•ì¥ì í¬í•¨)
    allowed_extensions = ['.csv', '.xlsx', '.xls']
    security_extensions = ['.cs', '.xl', '.log', '.dat', '.txt']  # ë³´ì•ˆ í™•ì¥ì ì¶”ê°€
    
    file_ext = os.path.splitext(file.filename.lower())[1]
    
    if file and (file_ext in allowed_extensions or file_ext in security_extensions):
        try:
            # ì„ì‹œ íŒŒì¼ëª… ìƒì„± (ì›ë³¸ í™•ì¥ì ìœ ì§€)
            original_filename = secure_filename(file.filename)
            temp_filename = secure_filename(f"temp_{int(time.time())}{file_ext}")
            temp_filepath = os.path.join(app.config['UPLOAD_FOLDER'], temp_filename)
            
            # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
            file.save(temp_filepath)
            logger.info(f"ğŸ“¤ [UPLOAD] File saved temporarily: {temp_filename}")
            
            # ğŸ”’ 1ë‹¨ê³„: ë³´ì•ˆ íŒŒì¼ ì²˜ë¦¬ (í™•ì¥ì ë³µì›) - ìºì‹œ ë¹„êµ ì „ì— ë¨¼ì € ì²˜ë¦¬
            if is_security_file:
                temp_filepath, normalized_filename, actual_ext = process_security_file(temp_filepath, original_filename)
                file_ext = actual_ext  # ì‹¤ì œ í™•ì¥ìë¡œ ì—…ë°ì´íŠ¸
                logger.info(f"ğŸ”’ [SECURITY] File processed: {original_filename} -> {normalized_filename}")
                
                # ì²˜ë¦¬ëœ íŒŒì¼ì´ ì§€ì›ë˜ëŠ” í˜•ì‹ì¸ì§€ ì¬í™•ì¸
                if file_ext not in allowed_extensions:
                    try:
                        os.remove(temp_filepath)
                    except:
                        pass
                    return jsonify({'error': f'ë³´ì•ˆ íŒŒì¼ ì²˜ë¦¬ í›„ ì§€ì›ë˜ì§€ ì•ŠëŠ” í˜•ì‹ì…ë‹ˆë‹¤: {file_ext}'}), 400
            
            # ğŸ“Š 2ë‹¨ê³„: ë°ì´í„° ë¶„ì„ - ë‚ ì§œ ë²”ìœ„ í™•ì¸ (ë³´ì•ˆ ì²˜ë¦¬ ì™„ë£Œëœ íŒŒì¼ë¡œ)
            # ğŸ”§ ë°ì´í„° ë¡œë”© ìºì‹±ì„ ìœ„í•œ ë³€ìˆ˜ ì´ˆê¸°í™”
            df_analysis = None
            
            try:
                if file_ext == '.csv':
                    df_analysis = pd.read_csv(temp_filepath)
                else:  # Excel íŒŒì¼
                    # Excel íŒŒì¼ì€ load_data í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ê³ ê¸‰ ì²˜ë¦¬ (ğŸ”§ ìºì‹œ í™œì„±í™”)
                    logger.info(f"ğŸ” [UPLOAD] Starting data analysis for {temp_filename}")
                    df_analysis = load_data(temp_filepath, use_cache=True)
                    # ì¸ë±ìŠ¤ê°€ Dateì¸ ê²½ìš° ì»¬ëŸ¼ìœ¼ë¡œ ë³µì›
                    if df_analysis.index.name == 'Date':
                        df_analysis = df_analysis.reset_index()
                if 'Date' in df_analysis.columns:
                    df_analysis['Date'] = pd.to_datetime(df_analysis['Date'])
                    start_date = df_analysis['Date'].min()
                    end_date = df_analysis['Date'].max()
                    total_records = len(df_analysis)
                    
                    # 2022ë…„ ì´í›„ ë°ì´í„° í™•ì¸
                    cutoff_2022 = pd.to_datetime('2022-01-01')
                    recent_data = df_analysis[df_analysis['Date'] >= cutoff_2022]
                    recent_records = len(recent_data)
                    
                    logger.info(f"ğŸ“Š [DATA_ANALYSIS] Full range: {start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')} ({total_records} records)")
                    logger.info(f"ğŸ“Š [DATA_ANALYSIS] 2022+ range: {recent_records} records")
                    
                    data_info = {
                        'start_date': start_date.strftime('%Y-%m-%d'),
                        'end_date': end_date.strftime('%Y-%m-%d'),
                        'total_records': total_records,
                        'recent_records_2022plus': recent_records,
                        'has_historical_data': start_date < cutoff_2022,
                        'lstm_recommended_cutoff': '2022-01-01'
                    }
                else:
                    # Date ì»¬ëŸ¼ì´ ì—†ëŠ” íŒŒì¼ì˜ ê²½ìš° (ì˜ˆ: holidays.csv)
                    file_type_hint = None
                    if 'holiday' in original_filename.lower():
                        file_type_hint = "íœ´ì¼ íŒŒì¼ë¡œ ë³´ì…ë‹ˆë‹¤. /api/holidays/upload ì—”ë“œí¬ì¸íŠ¸ ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤."
                    data_info = {
                        'warning': 'No Date column found',
                        'file_type_hint': file_type_hint
                    }
            except Exception as e:
                logger.warning(f"Data analysis failed: {str(e)}")
                data_info = {'warning': f'Data analysis failed: {str(e)}'}
            
            # ğŸ”§ Excel íŒŒì¼ ì½ê¸° ì™„ë£Œ í›„ íŒŒì¼ í•¸ë“¤ ê°•ì œ í•´ì œ
            import gc
            gc.collect()  # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ìœ¼ë¡œ pandasê°€ ì—´ì–´ë‘” íŒŒì¼ í•¸ë“¤ í•´ì œ
            
            # ğŸ” 3ë‹¨ê³„: ìºì‹œ í˜¸í™˜ì„± í™•ì¸ (ë³´ì•ˆ ì²˜ë¦¬ ë° ë°ì´í„° ë¶„ì„ ì™„ë£Œ í›„)
            # ì‚¬ìš©ìì˜ ì˜ë„ëœ ë°ì´í„° ë²”ìœ„ ì¶”ì • (ê¸°ë³¸ê°’: 2022ë…„ë¶€í„° LSTM, ì „ì²´ ë°ì´í„° VARMAX)
            # end_dateê°€ ì •ì˜ë˜ì§€ ì•Šì€ ê²½ìš°ë¥¼ ìœ„í•œ ì•ˆì „í•œ fallback
            default_end_date = datetime.now().strftime('%Y-%m-%d')
            intended_range = {
                'start_date': '2022-01-01',  # LSTM ê¶Œì¥ ì‹œì‘ì 
                'cutoff_date': data_info.get('end_date', default_end_date)
            }
            
            logger.info(f"ğŸ” [UPLOAD_CACHE] Starting cache compatibility check:")
            logger.info(f"  ğŸ“ New file: {temp_filename}")
            logger.info(f"  ğŸ“… Data range: {data_info.get('start_date')} ~ {data_info.get('end_date')}")
            logger.info(f"  ğŸ“Š Total records: {data_info.get('total_records')}")
            logger.info(f"  ğŸ¯ Intended range: {intended_range}")
            
            # ğŸ”§ ì´ë¯¸ ë¡œë”©ëœ ë°ì´í„°ë¥¼ ì „ë‹¬í•˜ì—¬ ì¤‘ë³µ ë¡œë”© ë°©ì§€
            cache_result = find_compatible_cache_file(temp_filepath, intended_range, cached_df=df_analysis)
            
            logger.info(f"ğŸ¯ [UPLOAD_CACHE] Cache check result:")
            logger.info(f"  âœ… Found: {cache_result['found']}")
            logger.info(f"  ğŸ·ï¸ Type: {cache_result.get('cache_type')}")
            if cache_result.get('cache_files'):
                logger.info(f"  ğŸ“ Cache files: {[os.path.basename(f) for f in cache_result['cache_files']]}")
            if cache_result.get('compatibility_info'):
                logger.info(f"  â„¹ï¸ Compatibility info: {cache_result['compatibility_info']}")
            
            response_data = {
                'success': True,
                'filepath': temp_filepath,
                'filename': os.path.basename(temp_filepath),
                'original_filename': original_filename,
                'normalized_filename': normalized_filename if is_security_file else original_filename,
                'data_info': data_info,
                'model_recommendations': {
                    'varmax': 'ì „ì²´ ë°ì´í„° ì‚¬ìš© ê¶Œì¥ (ì¥ê¸° íŠ¸ë Œë“œ ë¶„ì„)',
                    'lstm': '2022ë…„ ì´í›„ ë°ì´í„° ì‚¬ìš© ê¶Œì¥ (ë‹¨ê¸° ì •í™•ë„ í–¥ìƒ)'
                },
                'security_info': {
                    'is_security_file': is_security_file,
                    'original_extension': os.path.splitext(file.filename.lower())[1] if is_security_file else None,
                    'detected_extension': file_ext if is_security_file else None,
                    'message': f"ë³´ì•ˆ íŒŒì¼ì´ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤: {os.path.splitext(file.filename)[1]} -> {file_ext}" if is_security_file else None
                },
                'cache_info': {
                    'found': cache_result['found'],
                    'cache_type': cache_result.get('cache_type'),
                    'message': None
                }
            }
            
            if cache_result['found']:
                cache_type = cache_result['cache_type']
                cache_files = cache_result.get('cache_files', [])
                compatibility_info = cache_result.get('compatibility_info', {})
                
                if cache_type == 'exact':
                    cache_file = cache_files[0] if cache_files else None
                    response_data['cache_info']['message'] = f"ë™ì¼í•œ ë°ì´í„° ë°œê²¬! ê¸°ì¡´ ìºì‹œë¥¼ í™œìš©í•©ë‹ˆë‹¤. ({os.path.basename(cache_file) if cache_file else 'Unknown'})"
                    response_data['cache_info']['compatible_file'] = cache_file
                    logger.info(f"âœ… [CACHE] Exact match found: {cache_file}")
                    
                elif cache_type == 'extension':
                    cache_file = cache_files[0] if cache_files else None
                    extension_details = compatibility_info.get('extension_details', {})
                    new_rows = extension_details.get('new_rows_count', compatibility_info.get('new_rows_count', 0))
                    extension_type = extension_details.get('validation_details', {}).get('extension_type', ['ë°ì´í„° í™•ì¥'])
                    
                    if isinstance(extension_type, list):
                        extension_desc = ' + '.join(extension_type)
                    else:
                        extension_desc = str(extension_type)
                    
                    response_data['cache_info']['message'] = f"ğŸ“ˆ ë°ì´í„° í™•ì¥ ê°ì§€! {extension_desc} (+{new_rows}ê°œ ìƒˆ í–‰). ê¸°ì¡´ í•˜ì´í¼íŒŒë¼ë¯¸í„°ì™€ ìºì‹œë¥¼ ì¬ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                    response_data['cache_info']['compatible_file'] = cache_file
                    response_data['cache_info']['extension_info'] = compatibility_info
                    response_data['cache_info']['hyperparams_reusable'] = True  # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¬ì‚¬ìš© ê°€ëŠ¥ í‘œì‹œ
                    logger.info(f"ğŸ“ˆ [CACHE] Extension detected from {cache_file}: {extension_desc} (+{new_rows} rows)")
                    
                elif cache_type in ['partial', 'near_complete', 'multi_cache']:
                    best_coverage = compatibility_info.get('best_coverage', 0)
                    total_caches = compatibility_info.get('total_compatible_caches', len(cache_files))
                    
                    if cache_type == 'near_complete':
                        response_data['cache_info']['message'] = f"ğŸ¯ ê±°ì˜ ì™„ì „í•œ ìºì‹œ ë§¤ì¹˜! ({best_coverage:.1%} ì»¤ë²„ë¦¬ì§€) ê¸°ì¡´ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ìµœëŒ€í•œ í™œìš©í•©ë‹ˆë‹¤."
                    elif cache_type == 'multi_cache':
                        response_data['cache_info']['message'] = f"ğŸ”— ë‹¤ì¤‘ ìºì‹œ ë°œê²¬! {total_caches}ê°œ ìºì‹œì—ì„œ {best_coverage:.1%} ì»¤ë²„ë¦¬ì§€ë¡œ ì˜ˆì¸¡ì„ ê°€ì†í™”í•©ë‹ˆë‹¤."
                    else:  # partial
                        response_data['cache_info']['message'] = f"ğŸ“Š ë¶€ë¶„ ìºì‹œ ë§¤ì¹˜! ({best_coverage:.1%} ì»¤ë²„ë¦¬ì§€) ì¼ë¶€ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì¬í™œìš©í•©ë‹ˆë‹¤."
                    
                    response_data['cache_info']['compatible_files'] = cache_files
                    response_data['cache_info']['compatibility_info'] = compatibility_info
                    logger.info(f"ğŸ¯ [ENHANCED_CACHE] {cache_type} cache found: {total_caches} caches, {best_coverage:.1%} coverage")
                
                # ğŸ”§ íŒŒì¼ ì²˜ë¦¬ ë¡œì§ ê°œì„ : ë°ì´í„° í™•ì¥ ì‹œ ìƒˆ íŒŒì¼ ì‚¬ìš©
                if cache_type == 'exact' and cache_files:
                    # ì •í™•íˆ ë™ì¼í•œ íŒŒì¼ì¸ ê²½ìš°ì—ë§Œ ê¸°ì¡´ íŒŒì¼ ì‚¬ìš©
                    cache_file = cache_files[0]
                    response_data['filepath'] = cache_file
                    response_data['filename'] = os.path.basename(cache_file)
                    
                    # ì„ì‹œ íŒŒì¼ ì‚­ì œ (ì™„ì „íˆ ë™ì¼í•œ ê²½ìš°ë§Œ)
                    if temp_filepath != cache_file:
                        try:
                            os.remove(temp_filepath)
                            logger.info(f"ğŸ—‘ï¸ [CLEANUP] Temporary file removed (exact match): {temp_filename}")
                        except Exception as e:
                            logger.warning(f"âš ï¸ [CLEANUP] Failed to remove temp file {temp_filename}: {str(e)}")
                            # ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
                            
                elif cache_type == 'extension' and cache_files:
                    # ğŸ”„ ë°ì´í„° í™•ì¥ì˜ ê²½ìš°: ìƒˆ íŒŒì¼ì„ ì‚¬ìš©í•˜ë˜, ìºì‹œ ì •ë³´ëŠ” ìœ ì§€
                    logger.info(f"ğŸ“ˆ [EXTENSION] Data extension detected - using NEW file with cache info")
                    
                    # ìƒˆ íŒŒì¼ì„ ì •ì‹ íŒŒì¼ëª…ìœ¼ë¡œ ì €ì¥ (ì›ë³¸ í™•ì¥ì ìœ ì§€)
                    try:
                        content_hash = get_data_content_hash(temp_filepath)
                        final_filename = f"data_{content_hash}{file_ext}" if content_hash else temp_filename
                    except Exception as hash_error:
                        logger.warning(f"âš ï¸ Hash calculation failed for extended file, using timestamp-based filename: {str(hash_error)}")
                        final_filename = temp_filename  # í•´ì‹œ ì‹¤íŒ¨ ì‹œ ì„ì‹œ íŒŒì¼ëª… ìœ ì§€
                    
                    final_filepath = os.path.join(app.config['UPLOAD_FOLDER'], final_filename)
                    
                    if temp_filepath != final_filepath:
                        # ğŸ”§ ê°•í™”ëœ íŒŒì¼ ì´ë™ ë¡œì§ (Excel íŒŒì¼ ë½ í•´ì œ ëŒ€ê¸°)
                        moved_successfully = False
                        for attempt in range(3):  # ìµœëŒ€ 3ë²ˆ ì‹œë„
                            try:
                                # Excel íŒŒì¼ ì½ê¸° í›„ íŒŒì¼ ë½ í•´ì œë¥¼ ìœ„í•œ ì¶©ë¶„í•œ ëŒ€ê¸°
                                import gc
                                gc.collect()  # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ìœ¼ë¡œ íŒŒì¼ í•¸ë“¤ í•´ì œ
                                time.sleep(0.5 + attempt * 0.5)  # ì ì§„ì ìœ¼ë¡œ ëŒ€ê¸° ì‹œê°„ ì¦ê°€
                                
                                shutil.move(temp_filepath, final_filepath)
                                logger.info(f"ğŸ“ [UPLOAD] Extended file renamed: {final_filename} (attempt {attempt + 1})")
                                moved_successfully = True
                                break
                            except OSError as move_error:
                                logger.warning(f"âš ï¸ Extended file move attempt {attempt + 1} failed: {str(move_error)}")
                                if attempt == 2:  # ë§ˆì§€ë§‰ ì‹œë„
                                    logger.warning(f"âš ï¸ All move attempts failed, keeping original filename: {str(move_error)}")
                                    final_filepath = temp_filepath
                                    final_filename = temp_filename
                        
                        if not moved_successfully:
                            final_filepath = temp_filepath
                            final_filename = temp_filename
                    else:
                        logger.info(f"ğŸ“ [UPLOAD] Extended file already has correct name: {final_filename}")
                        
                    response_data['filepath'] = final_filepath
                    response_data['filename'] = final_filename
                    
                    # í™•ì¥ ì •ë³´ì— ìƒˆ íŒŒì¼ ì •ë³´ ì¶”ê°€
                    response_data['cache_info']['new_file_used'] = True
                    response_data['cache_info']['original_cache_file'] = cache_files[0]
                    
                    # ğŸ”‘ ë°ì´í„° í™•ì¥ í‘œì‹œ - í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¬ì‚¬ìš© ê°€ëŠ¥
                    response_data['data_extended'] = True
                    response_data['hyperparams_inheritance'] = {
                        'available': True,
                        'source_file': os.path.basename(cache_files[0]),
                        'extension_type': extension_desc if 'extension_desc' in locals() else 'ë°ì´í„° í™•ì¥',
                        'new_rows_added': new_rows if 'new_rows' in locals() else compatibility_info.get('new_rows_count', 0)
                    }
                    
                else:
                    # ìƒˆ íŒŒì¼ì€ ìœ ì§€ (ë¶€ë¶„/ë‹¤ì¤‘ ìºì‹œì˜ ê²½ìš°, ì›ë³¸ í™•ì¥ì ìœ ì§€)
                    try:
                        content_hash = get_data_content_hash(temp_filepath)
                        final_filename = f"data_{content_hash}{file_ext}" if content_hash else temp_filename
                    except Exception as hash_error:
                        logger.warning(f"âš ï¸ Hash calculation failed, using timestamp-based filename: {str(hash_error)}")
                        final_filename = temp_filename  # í•´ì‹œ ì‹¤íŒ¨ ì‹œ ì„ì‹œ íŒŒì¼ëª… ìœ ì§€
                    
                    final_filepath = os.path.join(app.config['UPLOAD_FOLDER'], final_filename)
                    
                    if temp_filepath != final_filepath:
                        # ğŸ”§ ê°•í™”ëœ íŒŒì¼ ì´ë™ ë¡œì§ (Excel íŒŒì¼ ë½ í•´ì œ ëŒ€ê¸°)
                        moved_successfully = False
                        for attempt in range(3):  # ìµœëŒ€ 3ë²ˆ ì‹œë„
                            try:
                                # Excel íŒŒì¼ ì½ê¸° í›„ íŒŒì¼ ë½ í•´ì œë¥¼ ìœ„í•œ ì¶©ë¶„í•œ ëŒ€ê¸°
                                import gc
                                gc.collect()  # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ìœ¼ë¡œ íŒŒì¼ í•¸ë“¤ í•´ì œ
                                time.sleep(0.5 + attempt * 0.5)  # ì ì§„ì ìœ¼ë¡œ ëŒ€ê¸° ì‹œê°„ ì¦ê°€
                                
                                shutil.move(temp_filepath, final_filepath)
                                logger.info(f"ğŸ“ [UPLOAD] File renamed: {final_filename} (attempt {attempt + 1})")
                                moved_successfully = True
                                break
                            except OSError as move_error:
                                logger.warning(f"âš ï¸ File move attempt {attempt + 1} failed: {str(move_error)}")
                                if attempt == 2:  # ë§ˆì§€ë§‰ ì‹œë„
                                    logger.warning(f"âš ï¸ All move attempts failed, keeping original filename: {str(move_error)}")
                                    final_filepath = temp_filepath
                                    final_filename = temp_filename
                        
                        if not moved_successfully:
                            final_filepath = temp_filepath
                            final_filename = temp_filename
                    else:
                        logger.info(f"ğŸ“ [UPLOAD] File already has correct name: {final_filename}")
                        
                    response_data['filepath'] = final_filepath
                    response_data['filename'] = final_filename
                response_data['cache_info']['message'] = "ìƒˆë¡œìš´ ë°ì´í„°ì…ë‹ˆë‹¤. ëª¨ë¸ë³„ë¡œ ì ì ˆí•œ ë°ì´í„° ë²”ìœ„ë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜ˆì¸¡í•©ë‹ˆë‹¤."
            
            # ğŸ”‘ ì—…ë¡œë“œëœ íŒŒì¼ ê²½ë¡œë¥¼ ì „ì—­ ìƒíƒœì— ì €ì¥
            prediction_state['current_file'] = response_data['filepath']
            logger.info(f"ğŸ“ Set current_file in prediction_state: {response_data['filepath']}")
            
            # ğŸ”§ ì„±ê³µ ì‹œ temp íŒŒì¼ ì •ë¦¬ (final_filepathì™€ ë‹¤ë¥¸ ê²½ìš°ì—ë§Œ)
            if 'temp_filepath' in locals() and os.path.exists(temp_filepath):
                final_filepath = response_data.get('filepath')
                if final_filepath and temp_filepath != final_filepath:
                    try:
                        os.remove(temp_filepath)
                        logger.info(f"ğŸ—‘ï¸ [CLEANUP] Success - temp file removed: {os.path.basename(temp_filepath)}")
                    except Exception as cleanup_error:
                        logger.warning(f"âš ï¸ [CLEANUP] Failed to remove temp file after success: {str(cleanup_error)}")
                else:
                    logger.info(f"ğŸ“ [CLEANUP] Temp file kept as final file: {os.path.basename(temp_filepath)}")
            
            return jsonify(response_data)
            
        except Exception as e:
            logger.error(f"Error during file upload: {str(e)}")
            # ğŸ”§ ê°•í™”ëœ temp íŒŒì¼ ì •ë¦¬
            try:
                if 'temp_filepath' in locals() and os.path.exists(temp_filepath):
                    os.remove(temp_filepath)
                    logger.info(f"ğŸ—‘ï¸ [CLEANUP] Temp file removed on error: {os.path.basename(temp_filepath)}")
            except Exception as cleanup_error:
                logger.warning(f"âš ï¸ [CLEANUP] Failed to remove temp file on error: {str(cleanup_error)}")
            return jsonify({'error': f'Error processing file: {str(e)}'}), 500
    
    return jsonify({'error': 'Invalid file type. Only CSV and Excel files (.csv, .xlsx, .xls) are allowed'}), 400

@app.route('/api/holidays', methods=['GET'])
def get_holidays():
    """íœ´ì¼ ëª©ë¡ ì¡°íšŒ API"""
    try:
        # íœ´ì¼ì„ ë‚ ì§œì™€ ì„¤ëª…ì´ í¬í•¨ëœ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        holidays_list = []
        file_holidays = load_holidays_from_file()  # íŒŒì¼ì—ì„œ ë¡œë“œ
        
        # í˜„ì¬ ì „ì—­ íœ´ì¼ì—ì„œ íŒŒì¼ íœ´ì¼ê³¼ ìë™ ê°ì§€ íœ´ì¼ êµ¬ë¶„
        auto_detected = holidays - file_holidays
        
        for holiday_date in file_holidays:
            holidays_list.append({
                'date': holiday_date,
                'description': 'Holiday (from file)',
                'source': 'file'
            })
        
        for holiday_date in auto_detected:
            holidays_list.append({
                'date': holiday_date,
                'description': 'Holiday (detected from missing data)',
                'source': 'auto_detected'
            })
        
        # ë‚ ì§œìˆœìœ¼ë¡œ ì •ë ¬
        holidays_list.sort(key=lambda x: x['date'])
        
        return jsonify({
            'success': True,
            'holidays': holidays_list,
            'count': len(holidays_list),
            'file_holidays': len(file_holidays),
            'auto_detected_holidays': len(auto_detected)
        })
    except Exception as e:
        logger.error(f"Error getting holidays: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e),
            'holidays': [],
            'count': 0
        }), 500

@app.route('/api/holidays/upload', methods=['POST'])
def upload_holidays():
    """íœ´ì¼ ëª©ë¡ íŒŒì¼ ì—…ë¡œë“œ API"""
    if 'file' not in request.files:
        return jsonify({'error': 'No file part'}), 400
        
    file = request.files['file']
    if file.filename == '':
        return jsonify({'error': 'No selected file'}), 400
        
    if file and (file.filename.endswith('.csv') or file.filename.endswith('.xlsx')):
        try:
            # ì„ì‹œ íŒŒì¼ëª… ìƒì„±
            filename = secure_filename(f"holidays_{int(time.time())}{os.path.splitext(file.filename)[1]}")
            filepath = os.path.join(app.config['UPLOAD_FOLDER'], filename)
            
            # íŒŒì¼ ì €ì¥
            file.save(filepath)
            
            # íœ´ì¼ ì •ë³´ ì—…ë°ì´íŠ¸ - ë³´ì•ˆ ìš°íšŒ ê¸°ëŠ¥ ì‚¬ìš©
            logger.info(f"ğŸ–ï¸ [HOLIDAY_UPLOAD] Processing uploaded holiday file: {filename}")
            new_holidays = update_holidays_safe(filepath)
            
            # ì›ë³¸ íŒŒì¼ì„ holidays ë””ë ‰í† ë¦¬ë¡œ ë³µì‚¬
            holidays_dir = 'holidays'
            if not os.path.exists(holidays_dir):
                os.makedirs(holidays_dir)
                logger.info(f"ğŸ“ Created holidays directory: {holidays_dir}")
            
            permanent_path = os.path.join(holidays_dir, 'holidays' + os.path.splitext(file.filename)[1])
            shutil.copy2(filepath, permanent_path)
            logger.info(f"ğŸ“ Holiday file copied to: {permanent_path}")
            
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            try:
                os.remove(filepath)
                logger.info(f"ğŸ—‘ï¸ Temporary file removed: {filepath}")
            except:
                pass
            
            return jsonify({
                'success': True,
                'message': f'Successfully uploaded and loaded {len(new_holidays)} holidays',
                'filepath': permanent_path,
                'filename': os.path.basename(permanent_path),
                'holidays': list(new_holidays)
            })
        except Exception as e:
            logger.error(f"Error during holiday file upload: {str(e)}")
            logger.error(traceback.format_exc())
            return jsonify({'error': f'Error processing file: {str(e)}'}), 500
    
    return jsonify({
        'error': 'Invalid file type. Only CSV and Excel files are allowed',
        'supported_extensions': {
            'standard': ['.csv', '.xlsx', '.xls'],
            'security': ['.cs (csv)', '.xl (xlsx)', '.log (xlsx)', '.dat (auto-detect)', '.txt (auto-detect)']
        }
    }), 400

@app.route('/api/holidays/reload', methods=['POST'])
def reload_holidays():
    """íœ´ì¼ ëª©ë¡ ì¬ë¡œë“œ API - ë³´ì•ˆ ìš°íšŒ ê¸°ëŠ¥ í¬í•¨"""
    try:
        filepath = request.json.get('filepath') if request.json else None
        
        logger.info(f"ğŸ”„ [HOLIDAY_RELOAD] Reloading holidays from: {filepath or 'default file'}")
        
        # ë³´ì•ˆ ìš°íšŒ ê¸°ëŠ¥ì„ í¬í•¨í•œ ì•ˆì „í•œ ì¬ë¡œë“œ
        new_holidays = update_holidays_safe(filepath)
        
        return jsonify({
            'success': True,
            'message': f'Successfully reloaded {len(new_holidays)} holidays',
            'holidays': list(new_holidays),
            'security_bypass_used': XLWINGS_AVAILABLE
        })
        
    except Exception as e:
        logger.error(f"âŒ [HOLIDAY_RELOAD] Error reloading holidays: {str(e)}")
        return jsonify({
            'success': False,
            'error': f'Failed to reload holidays: {str(e)}'
        }), 500

@app.route('/api/file/metadata', methods=['GET'])
def get_file_metadata():
    """íŒŒì¼ ë©”íƒ€ë°ì´í„° ì¡°íšŒ API"""
    filepath = request.args.get('filepath')
    if not filepath or not os.path.exists(filepath):
        return jsonify({'error': 'File not found'}), 404
    
    try:
        # íŒŒì¼ í™•ì¥ìì— ë”°ë¼ ì½ê¸° ë°©ì‹ ê²°ì •
        file_ext = os.path.splitext(filepath.lower())[1]
        
        if file_ext == '.csv':
            # CSV íŒŒì¼ ì²˜ë¦¬
            df = pd.read_csv(filepath, nrows=5)  # ì²˜ìŒ 5í–‰ë§Œ ì½ê¸°
            columns = df.columns.tolist()
            latest_date = None
            
            if 'Date' in df.columns:
                # ë‚ ì§œ ì •ë³´ë¥¼ ë³„ë„ë¡œ ì½ì–´ì„œ ìµœì‹  ë‚ ì§œ í™•ì¸
                dates_df = pd.read_csv(filepath, usecols=['Date'])
                dates_df['Date'] = pd.to_datetime(dates_df['Date'])
                latest_date = dates_df['Date'].max().strftime('%Y-%m-%d')
        else:
            # Excel íŒŒì¼ ì²˜ë¦¬ (ê³ ê¸‰ ì²˜ë¦¬ ì‚¬ìš©) - ğŸ”§ ì¤‘ë³µ ë¡œë”© ë°©ì§€
            logger.info(f"ğŸ” [METADATA] Loading Excel data for metadata extraction...")
            df = load_data(filepath)
            # ì¸ë±ìŠ¤ê°€ Dateì¸ ê²½ìš° ì»¬ëŸ¼ìœ¼ë¡œ ë³µì›
            if df.index.name == 'Date':
                full_df = df.copy()  # ğŸ”§ ì „ì²´ ë°ì´í„° ì €ì¥ (ì¤‘ë³µ ë¡œë”© ë°©ì§€)
                df = df.reset_index()
            else:
                full_df = df.copy()  # ğŸ”§ ì „ì²´ ë°ì´í„° ì €ì¥ (ì¤‘ë³µ ë¡œë”© ë°©ì§€)
            
            # ì²˜ìŒ 5í–‰ë§Œ ì„ íƒ
            df_sample = df.head(5)
            columns = df.columns.tolist()
            latest_date = None
            
            if 'Date' in df.columns:
                # ğŸ”§ ì´ë¯¸ ë¡œë”©ëœ ë°ì´í„°ì—ì„œ ìµœì‹  ë‚ ì§œ í™•ì¸ (ì¤‘ë³µ ë¡œë”© ë°©ì§€)
                if full_df.index.name == 'Date':
                    latest_date = pd.to_datetime(full_df.index).max().strftime('%Y-%m-%d')
                else:
                    latest_date = pd.to_datetime(full_df['Date']).max().strftime('%Y-%m-%d')
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            df = df_sample
        
        return jsonify({
            'success': True,
            'rows': len(df),
            'columns': columns,
            'latest_date': latest_date,
            'sample': df.head().to_dict(orient='records')
        })
    except Exception as e:
        logger.error(f"Error reading file metadata: {str(e)}")
        return jsonify({'error': f'Error reading file: {str(e)}'}), 500
    
@app.route('/api/data/dates', methods=['GET'])
def get_available_dates():
    filepath = request.args.get('filepath')
    days_limit = int(request.args.get('limit', 999999))  # ê¸°ë³¸ê°’ì„ ë§¤ìš° í° ìˆ˜ë¡œ ì„¤ì • (ëª¨ë“  ë‚ ì§œ)
    force_refresh = request.args.get('force_refresh', 'false').lower() == 'true'  # ê°•ì œ ìƒˆë¡œê³ ì¹¨ ì˜µì…˜
    
    if not filepath or not os.path.exists(filepath):
        return jsonify({'error': 'File not found'}), 404
    
    try:
        # ğŸ”„ íŒŒì¼ì˜ ìµœì‹  í•´ì‹œì™€ ìˆ˜ì • ì‹œê°„ í™•ì¸í•˜ì—¬ ë³€ê²½ ê°ì§€
        current_file_hash = get_data_content_hash(filepath)
        current_file_mtime = os.path.getmtime(filepath)
        
        logger.info(f"ğŸ” [DATE_REFRESH] Checking file status:")
        logger.info(f"  ğŸ“ File: {os.path.basename(filepath)}")
        logger.info(f"  ğŸ”‘ Current hash: {current_file_hash[:12] if current_file_hash else 'None'}...")
        logger.info(f"  â° Modified time: {datetime.fromtimestamp(current_file_mtime).strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info(f"  ğŸ”„ Force refresh: {force_refresh}")
        
        # íŒŒì¼ ë°ì´í„° ë¡œë“œ ë° ë¶„ì„ (íŒŒì¼ í˜•ì‹ì— ë§ê²Œ, í•­ìƒ ìµœì‹  íŒŒì¼ ë‚´ìš© í™•ì¸)
        # ğŸ”‘ ë‹¨ì¼ ë‚ ì§œ ì˜ˆì¸¡ìš©: LSTM ëª¨ë¸ íƒ€ì… ì§€ì •í•˜ì—¬ 2022ë…„ ì´í›„ ë°ì´í„°ë§Œ ë¡œë“œ
        file_ext = os.path.splitext(filepath.lower())[1]
        if file_ext == '.csv':
            df = pd.read_csv(filepath)
        else:
            # Excel íŒŒì¼ì¸ ê²½ìš° load_data í•¨ìˆ˜ ì‚¬ìš© (LSTM ëª¨ë¸ íƒ€ì… ì§€ì •)
            df = load_data(filepath, model_type='lstm')
            # ì¸ë±ìŠ¤ê°€ Dateì¸ ê²½ìš° ì»¬ëŸ¼ìœ¼ë¡œ ë³µì›
            if df.index.name == 'Date':
                df = df.reset_index()
        
        df['Date'] = pd.to_datetime(df['Date'])
        df = df.sort_values('Date')
        
        # ğŸ–ï¸ ë°ì´í„°ë¥¼ ë¡œë“œí•œ í›„ íœ´ì¼ ì •ë³´ ìë™ ì—…ë°ì´íŠ¸ (ë¹ˆ í‰ì¼ ê°ì§€) - ì„ì‹œ ë¹„í™œì„±í™”
        logger.info(f"ğŸ–ï¸ [HOLIDAYS] Auto-detection temporarily disabled to show more dates...")
        # updated_holidays = update_holidays(df=df)
        updated_holidays = load_holidays_from_file()  # íŒŒì¼ íœ´ì¼ë§Œ ì‚¬ìš©
        logger.info(f"ğŸ–ï¸ [HOLIDAYS] Total holidays (file only): {len(updated_holidays)}")
        
        # ğŸ“Š ì‹¤ì œ íŒŒì¼ ë°ì´í„° ë²”ìœ„ í™•ì¸ (ìºì‹œ ë¬´ì‹œ)
        total_rows = len(df)
        data_start_date = df.iloc[0]['Date']
        data_end_date = df.iloc[-1]['Date']
        
        logger.info(f"ğŸ“Š [ACTUAL_DATA] File analysis results:")
        logger.info(f"  ğŸ“ˆ Total data rows: {total_rows}")
        logger.info(f"  ğŸ“… Actual date range: {data_start_date.strftime('%Y-%m-%d')} ~ {data_end_date.strftime('%Y-%m-%d')}")
        
        # ğŸ” ê¸°ì¡´ ìºì‹œì™€ ë¹„êµ (ìˆëŠ” ê²½ìš°)
        existing_cache_range = find_existing_cache_range(filepath)
        if existing_cache_range and not force_refresh:
            cache_start = pd.to_datetime(existing_cache_range['start_date'])
            cache_cutoff = pd.to_datetime(existing_cache_range['cutoff_date'])
            
            logger.info(f"ğŸ’¾ [CACHE_COMPARISON] Found existing cache range:")
            logger.info(f"  ğŸ“… Cached range: {cache_start.strftime('%Y-%m-%d')} ~ {cache_cutoff.strftime('%Y-%m-%d')}")
            
            # ì‹¤ì œ ë°ì´í„°ê°€ ìºì‹œëœ ë²”ìœ„ë³´ë‹¤ í™•ì¥ë˜ì—ˆëŠ”ì§€ í™•ì¸
            data_extended = (
                data_start_date < cache_start or 
                data_end_date > cache_cutoff
            )
            
            if data_extended:
                logger.info(f"ğŸ“ˆ [DATA_EXTENSION] Data has been extended!")
                logger.info(f"  â¬…ï¸ Start extension: {data_start_date.strftime('%Y-%m-%d')} vs cached {cache_start.strftime('%Y-%m-%d')}")
                logger.info(f"  â¡ï¸ End extension: {data_end_date.strftime('%Y-%m-%d')} vs cached {cache_cutoff.strftime('%Y-%m-%d')}")
                logger.info(f"  ğŸ”„ Using extended data range for date calculation")
            else:
                logger.info(f"âœ… [NO_EXTENSION] Data range matches cached range, proceeding with current data")
        else:
            if force_refresh:
                logger.info(f"ğŸ”„ [FORCE_REFRESH] Ignoring cache due to force refresh")
            else:
                logger.info(f"ğŸ“­ [NO_CACHE] No existing cache found, using full data range")
        
        # ë°ì´í„° ë§ˆì§€ë§‰ ë‚ ì§œì˜ ë‹¤ìŒ ì˜ì—…ì¼ì„ ê³„ì‚°í•˜ì—¬ ì˜ˆì¸¡ ì‹œì‘ì  ì„¤ì • (ì‹¤ì œ ë°ì´í„° ê¸°ì¤€)
        # ìµœì†Œ 100ê°œ í–‰ ì´ìƒì˜ íˆìŠ¤í† ë¦¬ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ì˜ˆì¸¡ ê°€ëŠ¥
        min_history_rows = 100
        prediction_start_index = max(min_history_rows, total_rows // 4)  # 25% ì§€ì  ë˜ëŠ” ìµœì†Œ 100í–‰ ì¤‘ í° ê°’
        
        # ì‹¤ì œ ì˜ˆì¸¡ì— ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ëª¨ë“  ë‚ ì§œ (ì¶©ë¶„í•œ íˆìŠ¤í† ë¦¬ê°€ ìˆëŠ” ë‚ ì§œë¶€í„°)
        predictable_dates = df.iloc[prediction_start_index:]['Date']
        
        # ì˜ˆì¸¡ ì‹œì‘ ì„ê³„ê°’ ê³„ì‚° (ì°¸ê³ ìš©)
        if prediction_start_index < total_rows:
            prediction_threshold_date = df.iloc[prediction_start_index]['Date']
        else:
            prediction_threshold_date = data_end_date
        
        logger.info(f"ğŸ¯ [PREDICTION_CALC] Prediction calculation:")
        logger.info(f"  ğŸ“Š Min history rows: {min_history_rows}")
        logger.info(f"  ğŸ“ Start index: {prediction_start_index} (date: {prediction_threshold_date.strftime('%Y-%m-%d')})")
        logger.info(f"  ğŸ“… Predictable dates: {len(predictable_dates)} dates available")
        
        # ì˜ˆì¸¡ ê°€ëŠ¥í•œ ëª¨ë“  ë‚ ì§œë¥¼ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ë°˜í™˜ (ìµœì‹  ë‚ ì§œë¶€í„°)
        # days_limitë³´ë‹¤ ì‘ì€ ê²½ìš°ì—ë§Œ ì œí•œ ì ìš©
        if len(predictable_dates) <= days_limit:
            dates = predictable_dates.sort_values(ascending=False).dt.strftime('%Y-%m-%d').tolist()
        else:
            dates = predictable_dates.sort_values(ascending=False).head(days_limit).dt.strftime('%Y-%m-%d').tolist()
        
        logger.info(f"ğŸ”¢ [FINAL_RESULT] Final date calculation:")
        logger.info(f"  ğŸ“Š Available predictable dates: {len(predictable_dates)}")
        logger.info(f"  ğŸ“‹ Returned dates: {len(dates)}")
        logger.info(f"  ğŸ“… Latest available date: {dates[0] if dates else 'None'}")
        
        response_data = {
            'success': True,
            'dates': dates,
            'latest_date': dates[0] if dates else None,  # ì²« ë²ˆì§¸ ìš”ì†Œê°€ ìµœì‹  ë‚ ì§œ (ë‚´ë¦¼ì°¨ìˆœ)
            'data_start_date': data_start_date.strftime('%Y-%m-%d'),
            'data_end_date': data_end_date.strftime('%Y-%m-%d'),
            'prediction_threshold': prediction_threshold_date.strftime('%Y-%m-%d'),
            'min_history_rows': min_history_rows,
            'total_rows': total_rows,
            'file_hash': current_file_hash[:12] if current_file_hash else None,  # ì¶”ê°€: íŒŒì¼ í•´ì‹œ ì •ë³´
            'file_modified': datetime.fromtimestamp(current_file_mtime).strftime('%Y-%m-%d %H:%M:%S')  # ì¶”ê°€: íŒŒì¼ ìˆ˜ì • ì‹œê°„
        }
        
        logger.info(f"ğŸ“¡ [API_RESPONSE] Sending enhanced dates response:")
        logger.info(f"  ğŸ“… Data range: {response_data['data_start_date']} ~ {response_data['data_end_date']}")
        logger.info(f"  ğŸ¯ Prediction threshold: {response_data['prediction_threshold']}")
        logger.info(f"  ğŸ“… Available date range: {dates[-1] if dates else 'None'} ~ {dates[0] if dates else 'None'} (ìµœì‹ ë¶€í„°)")
        logger.info(f"  ğŸ”‘ File signature: {response_data['file_hash']} @ {response_data['file_modified']}")
        
        return jsonify(response_data)
    except Exception as e:
        logger.error(f"Error reading dates: {str(e)}")
        return jsonify({'error': f'Error reading dates: {str(e)}'}), 500

@app.route('/api/data/refresh', methods=['POST'])
def refresh_file_data():
    """íŒŒì¼ ë°ì´í„° ìƒˆë¡œê³ ì¹¨ ë° ìºì‹œ ê°±ì‹  API"""
    try:
        filepath = request.json.get('filepath') if request.json else request.args.get('filepath')
        if not filepath or not os.path.exists(filepath):
            return jsonify({'error': 'File not found'}), 404
        
        # íŒŒì¼ í•´ì‹œì™€ ìˆ˜ì • ì‹œê°„ í™•ì¸
        current_file_hash = get_data_content_hash(filepath)
        current_file_mtime = os.path.getmtime(filepath)
        
        logger.info(f"ğŸ”„ [FILE_REFRESH] Starting file data refresh:")
        logger.info(f"  ğŸ“ File: {os.path.basename(filepath)}")
        logger.info(f"  ğŸ”‘ Hash: {current_file_hash[:12] if current_file_hash else 'None'}...")
        
        # ê¸°ì¡´ ìºì‹œ í™•ì¸
        existing_cache_range = find_existing_cache_range(filepath)
        refresh_needed = False
        refresh_reason = []
        
        if existing_cache_range:
            # ìºì‹œëœ ë©”íƒ€ë°ì´í„°ì™€ ë¹„êµ
            meta_file = existing_cache_range.get('meta_file')
            if meta_file and os.path.exists(meta_file):
                try:
                    with open(meta_file, 'r', encoding='utf-8') as f:
                        meta_data = json.load(f)
                    
                    cached_hash = meta_data.get('file_hash')
                    cached_mtime = meta_data.get('file_modified_time')
                    
                    if cached_hash != current_file_hash:
                        refresh_needed = True
                        refresh_reason.append("File content changed")
                        
                    if cached_mtime and cached_mtime != current_file_mtime:
                        refresh_needed = True
                        refresh_reason.append("File modification time changed")
                        
                except Exception as e:
                    logger.warning(f"Error reading cache metadata: {str(e)}")
                    refresh_needed = True
                    refresh_reason.append("Cache metadata error")
            else:
                refresh_needed = True
                refresh_reason.append("No cache metadata found")
        else:
            refresh_needed = True
            refresh_reason.append("No existing cache")
        
        # íŒŒì¼ ë°ì´í„° ë¶„ì„ (íŒŒì¼ í˜•ì‹ì— ë§ê²Œ)
        file_ext = os.path.splitext(filepath.lower())[1]
        if file_ext == '.csv':
            df = pd.read_csv(filepath)
        else:
            # Excel íŒŒì¼ì¸ ê²½ìš° load_data í•¨ìˆ˜ ì‚¬ìš©
            df = load_data(filepath)
            # ì¸ë±ìŠ¤ê°€ Dateì¸ ê²½ìš° ì»¬ëŸ¼ìœ¼ë¡œ ë³µì›
            if df.index.name == 'Date':
                df = df.reset_index()
        
        df['Date'] = pd.to_datetime(df['Date'])
        df = df.sort_values('Date')
        
        current_data_range = {
            'start_date': df.iloc[0]['Date'],
            'end_date': df.iloc[-1]['Date'],
            'total_rows': len(df)
        }
        
        # ìºì‹œì™€ ì‹¤ì œ ë°ì´í„° ë²”ìœ„ ë¹„êµ
        if existing_cache_range and not refresh_needed:
            cache_start = pd.to_datetime(existing_cache_range['start_date'])
            cache_cutoff = pd.to_datetime(existing_cache_range['cutoff_date'])
            
            if (current_data_range['start_date'] < cache_start or 
                current_data_range['end_date'] > cache_cutoff):
                refresh_needed = True
                refresh_reason.append("Data range extended")
        
        response_data = {
            'success': True,
            'refresh_needed': refresh_needed,
            'refresh_reasons': refresh_reason,
            'file_info': {
                'hash': current_file_hash[:12] if current_file_hash else None,
                'modified_time': datetime.fromtimestamp(current_file_mtime).strftime('%Y-%m-%d %H:%M:%S'),
                'total_rows': current_data_range['total_rows'],
                'date_range': {
                    'start': current_data_range['start_date'].strftime('%Y-%m-%d'),
                    'end': current_data_range['end_date'].strftime('%Y-%m-%d')
                }
            }
        }
        
        if existing_cache_range:
            response_data['cache_info'] = {
                'date_range': {
                    'start': existing_cache_range['start_date'],
                    'end': existing_cache_range['cutoff_date']
                },
                'meta_file': existing_cache_range.get('meta_file')
            }
        
        logger.info(f"ğŸ“Š [REFRESH_ANALYSIS] File refresh analysis:")
        logger.info(f"  ğŸ”„ Refresh needed: {refresh_needed}")
        logger.info(f"  ğŸ“ Reasons: {', '.join(refresh_reason) if refresh_reason else 'None'}")
        logger.info(f"  ğŸ“… Current range: {response_data['file_info']['date_range']['start']} ~ {response_data['file_info']['date_range']['end']}")
        
        return jsonify(response_data)
        
    except Exception as e:
        logger.error(f"Error in file refresh check: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/debug/compare-files', methods=['POST'])
def debug_compare_files():
    """ë‘ íŒŒì¼ì„ ì§ì ‘ ë¹„êµí•˜ì—¬ ì°¨ì´ì ì„ ë¶„ì„í•˜ëŠ” ë””ë²„ê¹… API"""
    try:
        data = request.json
        file1_path = data.get('file1_path')
        file2_path = data.get('file2_path')
        
        if not file1_path or not file2_path:
            return jsonify({'error': 'Both file paths are required'}), 400
            
        if not os.path.exists(file1_path) or not os.path.exists(file2_path):
            return jsonify({'error': 'One or both files do not exist'}), 404
        
        logger.info(f"ğŸ” [DEBUG_COMPARE] Comparing files:")
        logger.info(f"  ğŸ“ File 1: {file1_path}")
        logger.info(f"  ğŸ“ File 2: {file2_path}")
        
        # íŒŒì¼ ê¸°ë³¸ ì •ë³´
        file1_hash = get_data_content_hash(file1_path)
        file2_hash = get_data_content_hash(file2_path)
        file1_size = os.path.getsize(file1_path)
        file2_size = os.path.getsize(file2_path)
        file1_mtime = os.path.getmtime(file1_path)
        file2_mtime = os.path.getmtime(file2_path)
        
        # ë°ì´í„° ë¶„ì„ (íŒŒì¼ í˜•ì‹ì— ë§ê²Œ)
        def load_file_safely(filepath):
            file_ext = os.path.splitext(filepath.lower())[1]
            if file_ext == '.csv':
                return pd.read_csv(filepath)
            else:
                # Excel íŒŒì¼ì¸ ê²½ìš° load_data í•¨ìˆ˜ ì‚¬ìš©
                df = load_data(filepath)
                # ì¸ë±ìŠ¤ê°€ Dateì¸ ê²½ìš° ì»¬ëŸ¼ìœ¼ë¡œ ë³µì›
                if df.index.name == 'Date':
                    df = df.reset_index()
                return df
        
        df1 = load_file_safely(file1_path)
        df2 = load_file_safely(file2_path)
        
        if 'Date' in df1.columns and 'Date' in df2.columns:
            df1['Date'] = pd.to_datetime(df1['Date'])
            df2['Date'] = pd.to_datetime(df2['Date'])
            df1 = df1.sort_values('Date')
            df2 = df2.sort_values('Date')
            
            file1_dates = {
                'start': df1['Date'].min(),
                'end': df1['Date'].max(),
                'count': len(df1)
            }
            
            file2_dates = {
                'start': df2['Date'].min(),
                'end': df2['Date'].max(),
                'count': len(df2)
            }
        else:
            file1_dates = {'error': 'No Date column'}
            file2_dates = {'error': 'No Date column'}
        
        # í™•ì¥ ì²´í¬
        extension_result = check_data_extension(file1_path, file2_path)
        
        # ìºì‹œ í˜¸í™˜ì„± ì²´í¬
        cache_result = find_compatible_cache_file(file2_path)
        
        response_data = {
            'success': True,
            'comparison': {
                'file1': {
                    'path': file1_path,
                    'hash': file1_hash[:12] if file1_hash else None,
                    'size': file1_size,
                    'modified': datetime.fromtimestamp(file1_mtime).strftime('%Y-%m-%d %H:%M:%S'),
                    'dates': {
                        'start': file1_dates['start'].strftime('%Y-%m-%d') if isinstance(file1_dates.get('start'), pd.Timestamp) else str(file1_dates.get('start')),
                        'end': file1_dates['end'].strftime('%Y-%m-%d') if isinstance(file1_dates.get('end'), pd.Timestamp) else str(file1_dates.get('end')),
                        'count': file1_dates.get('count')
                    } if 'error' not in file1_dates else file1_dates
                },
                'file2': {
                    'path': file2_path,
                    'hash': file2_hash[:12] if file2_hash else None,
                    'size': file2_size,
                    'modified': datetime.fromtimestamp(file2_mtime).strftime('%Y-%m-%d %H:%M:%S'),
                    'dates': {
                        'start': file2_dates['start'].strftime('%Y-%m-%d') if isinstance(file2_dates.get('start'), pd.Timestamp) else str(file2_dates.get('start')),
                        'end': file2_dates['end'].strftime('%Y-%m-%d') if isinstance(file2_dates.get('end'), pd.Timestamp) else str(file2_dates.get('end')),
                        'count': file2_dates.get('count')
                    } if 'error' not in file2_dates else file2_dates
                },
                'identical_hash': file1_hash == file2_hash,
                'size_difference': file2_size - file1_size,
                'extension_analysis': extension_result,
                'cache_analysis': cache_result
            }
        }
        
        logger.info(f"ğŸ“Š [DEBUG_COMPARE] Comparison results:")
        logger.info(f"  ğŸ”‘ Identical hash: {file1_hash == file2_hash}")
        logger.info(f"  ğŸ“ Size difference: {file2_size - file1_size} bytes")
        logger.info(f"  ğŸ“ˆ Is extension: {extension_result.get('is_extension', False)}")
        logger.info(f"  ğŸ’¾ Cache found: {cache_result.get('found', False)}")
        
        return jsonify(response_data)
        
    except Exception as e:
        logger.error(f"Error in file comparison: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/predictions/saved', methods=['GET'])
def get_saved_predictions():
    """ì €ì¥ëœ ì˜ˆì¸¡ ê²°ê³¼ ëª©ë¡ ì¡°íšŒ API"""
    try:
        limit = int(request.args.get('limit', 100))
        predictions_list = get_saved_predictions_list(limit)
        
        return jsonify({
            'success': True,
            'predictions': predictions_list,
            'count': len(predictions_list)
        })
    except Exception as e:
        logger.error(f"Error getting saved predictions: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/predictions/saved/<date>', methods=['GET'])
def get_saved_prediction_by_date(date):
    """íŠ¹ì • ë‚ ì§œì˜ ì €ì¥ëœ ì˜ˆì¸¡ ê²°ê³¼ ì¡°íšŒ API"""
    try:
        result = load_prediction_from_csv(date)
        
        if result['success']:
            return jsonify(result)
        else:
            return jsonify({'error': result['error']}), 404
    except Exception as e:
        logger.error(f"Error loading saved prediction: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/predictions/saved/<date>', methods=['DELETE'])
def delete_saved_prediction_api(date):
    """ì €ì¥ëœ ì˜ˆì¸¡ ê²°ê³¼ ì‚­ì œ API"""
    try:
        result = delete_saved_prediction(date)
        
        if result['success']:
            return jsonify(result)
        else:
            return jsonify({'error': result['error']}), 500
    except Exception as e:
        logger.error(f"Error deleting saved prediction: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/predictions/saved/<date>/update-actual', methods=['POST'])
def update_prediction_actual_values_api(date):
    """ìºì‹œëœ ì˜ˆì¸¡ì˜ ì‹¤ì œê°’ë§Œ ì—…ë°ì´íŠ¸í•˜ëŠ” API - ì„±ëŠ¥ ìµœì í™”"""
    try:
        # ìš”ì²­ íŒŒë¼ë¯¸í„°
        data = request.json or {}
        update_latest_only = data.get('update_latest_only', True)
        
        logger.info(f"ğŸ”„ [API] Updating actual values for prediction {date}")
        logger.info(f"  ğŸ“Š Update latest only: {update_latest_only}")
        
        # ì‹¤ì œê°’ ì—…ë°ì´íŠ¸ ì‹¤í–‰
        result = update_cached_prediction_actual_values(date, update_latest_only)
        
        if result['success']:
            logger.info(f"âœ… [API] Successfully updated {result.get('updated_count', 0)} actual values")
            return jsonify({
                'success': True,
                'updated_count': result.get('updated_count', 0),
                'message': f'Updated {result.get("updated_count", 0)} actual values',
                'predictions': result['predictions']
            })
        else:
            logger.error(f"âŒ [API] Failed to update actual values: {result.get('error')}")
            return jsonify({'error': result['error']}), 500
            
    except Exception as e:
        logger.error(f"âŒ [API] Error updating actual values: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/predictions/export', methods=['GET'])
def export_predictions():
    """ì €ì¥ëœ ì˜ˆì¸¡ ê²°ê³¼ë“¤ì„ í•˜ë‚˜ì˜ CSV íŒŒì¼ë¡œ ë‚´ë³´ë‚´ê¸° API"""
    try:
        start_date = request.args.get('start_date')
        end_date = request.args.get('end_date')
        
        # ë‚ ì§œ ë²”ìœ„ì— ë”°ë¥¸ ì˜ˆì¸¡ ë¡œë“œ
        if start_date:
            predictions = load_accumulated_predictions_from_csv(start_date, end_date)
        else:
            # ëª¨ë“  ì €ì¥ëœ ì˜ˆì¸¡ ë¡œë“œ
            predictions_list = get_saved_predictions_list(limit=1000)
            predictions = []
            for pred_info in predictions_list:
                loaded = load_prediction_from_csv(pred_info['prediction_date'])
                if loaded['success']:
                    predictions.extend(loaded['predictions'])
        
        if not predictions:
            return jsonify({'error': 'No predictions found for export'}), 404
        
        # DataFrameìœ¼ë¡œ ë³€í™˜
        if isinstance(predictions[0], dict) and 'predictions' in predictions[0]:
            # ëˆ„ì  ì˜ˆì¸¡ í˜•ì‹ì¸ ê²½ìš°
            all_predictions = []
            for pred_group in predictions:
                all_predictions.extend(pred_group['predictions'])
            export_df = pd.DataFrame(all_predictions)
        else:
            # ë‹¨ìˆœ ì˜ˆì¸¡ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš°
            export_df = pd.DataFrame(predictions)
        
        # ì„ì‹œ íŒŒì¼ ìƒì„±
        temp_file = tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False)
        export_df.to_csv(temp_file.name, index=False)
        temp_file.close()
        
        # íŒŒì¼ ì „ì†¡
        return send_file(
            temp_file.name,
            as_attachment=True,
            download_name=f'predictions_export_{datetime.now().strftime("%Y%m%d_%H%M%S")}.csv',
            mimetype='text/csv'
        )
        
    except Exception as e:
        logger.error(f"Error exporting predictions: {str(e)}")
        return jsonify({'error': str(e)}), 500

# 7. API ì—”ë“œí¬ì¸íŠ¸ ìˆ˜ì • - ìŠ¤ë§ˆíŠ¸ ìºì‹œ ì‚¬ìš©
@app.route('/api/predict', methods=['POST'])
def start_prediction_compatible():
    """í˜¸í™˜ì„±ì„ ìœ ì§€í•˜ëŠ” ì˜ˆì¸¡ ì‹œì‘ API - ìºì‹œ ìš°ì„  ì‚¬ìš© (ë¡œê·¸ ê°•í™”)"""
    global prediction_state
    
    if prediction_state['is_predicting']:
        return jsonify({
            'success': False,
            'error': 'Prediction already in progress',
            'progress': prediction_state['prediction_progress']
        }), 409
    
    data = request.json
    filepath = data.get('filepath')
    current_date = data.get('date')
    save_to_csv = data.get('save_to_csv', True)
    use_cache = data.get('use_cache', True)  # ê¸°ë³¸ê°’ True
    
    if not filepath or not os.path.exists(filepath):
        return jsonify({'error': 'Invalid file path'}), 400
    
    # ğŸ”‘ íŒŒì¼ ê²½ë¡œë¥¼ ì „ì—­ ìƒíƒœì— ì €ì¥ (ìºì‹œ ì—°ë™ìš©)
    prediction_state['current_file'] = filepath
    
    # âœ… ë¡œê·¸ ê°•í™”
    logger.info(f"ğŸš€ Prediction API called:")
    logger.info(f"  ğŸ“… Target date: {current_date}")
    logger.info(f"  ğŸ“ Data file: {filepath}")
    logger.info(f"  ğŸ’¾ Save to CSV: {save_to_csv}")
    logger.info(f"  ğŸ”„ Use cache: {use_cache}")
    
    # í˜¸í™˜ì„± ìœ ì§€ ë°±ê·¸ë¼ìš´ë“œ í•¨ìˆ˜ ì‹¤í–‰ (ìºì‹œ ìš°ì„  ì‚¬ìš©, ë‹¨ì¼ ì˜ˆì¸¡ë§Œ)
    thread = Thread(target=background_prediction_simple_compatible, 
                   args=(filepath, current_date, save_to_csv, use_cache))
    thread.daemon = True
    thread.start()
    
    return jsonify({
        'success': True,
        'message': 'Compatible prediction started (cache-first)',
        'use_cache': use_cache,
        'cache_priority': 'high',
        'features': ['Cache-first loading', 'Unified file naming', 'Enhanced logging', 'Past/Future visualization split']
    })

@app.route('/api/predict/status', methods=['GET'])
def prediction_status():
    """ì˜ˆì¸¡ ìƒíƒœ í™•ì¸ API (ë‚¨ì€ ì‹œê°„ ì¶”ê°€)"""
    global prediction_state
    
    status = {
        'is_predicting': prediction_state['is_predicting'],
        'progress': prediction_state['prediction_progress'],
        'error': prediction_state['error']
    }
    
    # ì˜ˆì¸¡ ì¤‘ì¸ ê²½ìš° ë‚¨ì€ ì‹œê°„ ê³„ì‚°
    if prediction_state['is_predicting'] and prediction_state['prediction_start_time']:
        time_info = calculate_estimated_time_remaining(
            prediction_state['prediction_start_time'], 
            prediction_state['prediction_progress']
        )
        status.update(time_info)
    
    # ì˜ˆì¸¡ì´ ì™„ë£Œëœ ê²½ìš° ë‚ ì§œ ì •ë³´ë„ ë°˜í™˜
    if not prediction_state['is_predicting'] and prediction_state['current_date']:
        status['current_date'] = prediction_state['current_date']
    
    return jsonify(status)

@app.route('/api/results', methods=['GET'])
def get_prediction_results_compatible():
    """í˜¸í™˜ì„±ì„ ìœ ì§€í•˜ëŠ” ì˜ˆì¸¡ ê²°ê³¼ ì¡°íšŒ API (ì˜¤ë¥˜ ìˆ˜ì •)"""
    global prediction_state
    
    logger.info(f"=== API /results called (compatible version) ===")
    
    if prediction_state['is_predicting']:
        return jsonify({
            'success': False,
            'error': 'Prediction in progress',
            'progress': prediction_state['prediction_progress']
        }), 409
    
    if prediction_state['latest_predictions'] is None:
        return jsonify({'error': 'No prediction results available'}), 404

    try:
        # ì˜ˆì¸¡ ë°ì´í„°ë¥¼ ê¸°ì¡´ í˜•íƒœë¡œ ë³€í™˜
        if isinstance(prediction_state['latest_predictions'], list):
            raw_predictions = prediction_state['latest_predictions']
        else:
            raw_predictions = prediction_state['latest_predictions']
        
        # ê¸°ì¡´ í˜•íƒœë¡œ ë³€í™˜
        compatible_predictions = convert_to_legacy_format(raw_predictions)
        
        logger.info(f"Converted {len(raw_predictions)} predictions to legacy format")
        logger.info(f"Sample converted prediction: {compatible_predictions[0] if compatible_predictions else 'None'}")
        
        # ë©”íŠ¸ë¦­ ì •ë¦¬
        metrics = prediction_state['latest_metrics']
        cleaned_metrics = {}
        if metrics:
            for key, value in metrics.items():
                cleaned_metrics[key] = safe_serialize_value(value)
        
        # êµ¬ê°„ ì ìˆ˜ ì•ˆì „ ì •ë¦¬ - ì˜¤ë¥˜ ë°©ì§€ ê°•í™”
        interval_scores = prediction_state['latest_interval_scores'] or []
        
        # interval_scores ë°ì´í„° íƒ€ì… ê²€ì¦ ë° ì•ˆì „ ì²˜ë¦¬
        if interval_scores is None:
            interval_scores = []
        elif not isinstance(interval_scores, (list, dict)):
            logger.warning(f"âš ï¸ Unexpected interval_scores type: {type(interval_scores)}, converting to empty list")
            interval_scores = []
        elif isinstance(interval_scores, dict) and not interval_scores:
            interval_scores = []
        
        try:
            cleaned_interval_scores = clean_interval_scores_safe(interval_scores)
        except Exception as interval_error:
            logger.error(f"âŒ Error cleaning interval_scores: {str(interval_error)}")
            cleaned_interval_scores = []
        
        # MA ê²°ê³¼ ì •ë¦¬ ë° í•„ìš”ì‹œ ì¬ê³„ì‚°
        ma_results = prediction_state['latest_ma_results'] or {}
        cleaned_ma_results = {}
        
        # ì´ë™í‰ê·  ê²°ê³¼ê°€ ì—†ê±°ë‚˜ ë¹„ì–´ìˆë‹¤ë©´ ì¬ê³„ì‚° ì‹œë„
        if not ma_results or len(ma_results) == 0:
            logger.info("ğŸ”„ MA results missing, attempting to recalculate...")
            try:
                # í˜„ì¬ ë°ì´í„°ì™€ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë™í‰ê·  ì¬ê³„ì‚°
                current_date = prediction_state.get('current_date')
                if current_date and prediction_state.get('latest_file_path'):
                    # ì›ë³¸ ë°ì´í„° ë¡œë“œ
                    df = load_data(prediction_state['latest_file_path'])
                    if df is not None and not df.empty:
                        # í˜„ì¬ ë‚ ì§œë¥¼ datetimeìœ¼ë¡œ ë³€í™˜
                        if isinstance(current_date, str):
                            current_date_dt = pd.to_datetime(current_date)
                        else:
                            current_date_dt = current_date
                        
                        # ê³¼ê±° ë°ì´í„° ì¶”ì¶œ
                        historical_data = df[df.index <= current_date_dt].copy()
                        
                        # ì˜ˆì¸¡ ë°ì´í„°ë¥¼ ì´ë™í‰ê·  ê³„ì‚°ìš©ìœ¼ë¡œ ë³€í™˜
                        ma_input_data = []
                        for pred in raw_predictions:
                            try:
                                ma_item = {
                                    'Date': pd.to_datetime(pred.get('Date') or pred.get('date')),
                                    'Prediction': safe_serialize_value(pred.get('Prediction') or pred.get('prediction')),
                                    'Actual': safe_serialize_value(pred.get('Actual') or pred.get('actual'))
                                }
                                ma_input_data.append(ma_item)
                            except Exception as e:
                                logger.warning(f"âš ï¸ Error processing MA data item: {str(e)}")
                                continue
                        
                        # ì´ë™í‰ê·  ê³„ì‚°
                        if ma_input_data:
                            ma_results = calculate_moving_averages_with_history(
                                ma_input_data, historical_data, target_col='MOPJ'
                            )
                            if ma_results:
                                logger.info(f"âœ… MA recalculated successfully with {len(ma_results)} windows")
                                prediction_state['latest_ma_results'] = ma_results
                            else:
                                logger.warning("âš ï¸ MA recalculation returned empty results")
                        else:
                            logger.warning("âš ï¸ No valid input data for MA calculation")
                    else:
                        logger.warning("âš ï¸ Unable to load original data for MA calculation")
                else:
                    logger.warning("âš ï¸ Missing current_date or file_path for MA calculation")
            except Exception as e:
                logger.error(f"âŒ Error recalculating MA: {str(e)}")
        
        # MA ê²°ê³¼ ì •ë¦¬
        for key, value in ma_results.items():
            if isinstance(value, list):
                cleaned_ma_results[key] = []
                for item in value:
                    if isinstance(item, dict):
                        cleaned_item = {}
                        for k, v in item.items():
                            cleaned_item[k] = safe_serialize_value(v)
                        cleaned_ma_results[key].append(cleaned_item)
                    else:
                        cleaned_ma_results[key].append(safe_serialize_value(item))
            else:
                cleaned_ma_results[key] = safe_serialize_value(value)
        
        # ì–´í…ì…˜ ë°ì´í„° ì •ë¦¬
        attention_data = prediction_state['latest_attention_data']
        cleaned_attention = None
        
        logger.info(f"ğŸ“Š [ATTENTION] Processing attention data: available={bool(attention_data)}")
        if attention_data:
            logger.info(f"ğŸ“Š [ATTENTION] Original keys: {list(attention_data.keys())}")
            
            cleaned_attention = {}
            for key, value in attention_data.items():
                if key == 'image' and value:
                    cleaned_attention[key] = value  # base64 ì´ë¯¸ì§€ëŠ” ê·¸ëŒ€ë¡œ
                    logger.info(f"ğŸ“Š [ATTENTION] Image data preserved (length: {len(value) if isinstance(value, str) else 'N/A'})")
                elif isinstance(value, dict):
                    cleaned_attention[key] = {}
                    for k, v in value.items():
                        cleaned_attention[key][k] = safe_serialize_value(v)
                    logger.info(f"ğŸ“Š [ATTENTION] Dict processed for key '{key}': {len(cleaned_attention[key])} items")
                else:
                    cleaned_attention[key] = safe_serialize_value(value)
                    logger.info(f"ğŸ“Š [ATTENTION] Value processed for key '{key}': {type(value)}")
            
            logger.info(f"ğŸ“Š [ATTENTION] Final cleaned keys: {list(cleaned_attention.keys())}")
        else:
            logger.warning(f"ğŸ“Š [ATTENTION] No attention data available in prediction_state")
        
        # í”Œë¡¯ ë°ì´í„° ì •ë¦¬
        plots = prediction_state['latest_plots'] or {}
        cleaned_plots = {}
        for key, value in plots.items():
            if isinstance(value, dict):
                cleaned_plots[key] = {}
                for k, v in value.items():
                    if k == 'image' and v:
                        cleaned_plots[key][k] = v  # base64 ì´ë¯¸ì§€ëŠ” ê·¸ëŒ€ë¡œ
                    else:
                        cleaned_plots[key][k] = safe_serialize_value(v)
            else:
                cleaned_plots[key] = safe_serialize_value(value)
        
        response_data = {
            'success': True,
            'current_date': safe_serialize_value(prediction_state['current_date']),
            'predictions': compatible_predictions,  # í˜¸í™˜ì„± ìœ ì§€ëœ í˜•íƒœ
            'interval_scores': cleaned_interval_scores,
            'ma_results': cleaned_ma_results,
            'attention_data': cleaned_attention,
            'plots': cleaned_plots,
            'metrics': cleaned_metrics if cleaned_metrics else None,
            'selected_features': prediction_state['selected_features'] or [],
            'feature_importance': safe_serialize_value(prediction_state.get('feature_importance')),
            'semimonthly_period': safe_serialize_value(prediction_state['semimonthly_period']),
            'next_semimonthly_period': safe_serialize_value(prediction_state['next_semimonthly_period'])
        }
        
        # ğŸ”§ ê°•í™”ëœ JSON ì§ë ¬í™” í…ŒìŠ¤íŠ¸
        try:
            test_json = json.dumps(response_data)
            # ì§ë ¬í™”ëœ JSONì— NaNì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ ì¶”ê°€ í™•ì¸
            if 'NaN' in test_json or 'Infinity' in test_json:
                logger.error(f"JSON contains NaN/Infinity values")
                # NaN ê°’ë“¤ì„ ëª¨ë‘ nullë¡œ êµì²´
                test_json_cleaned = test_json.replace('NaN', 'null').replace('Infinity', 'null').replace('-Infinity', 'null')
                response_data = json.loads(test_json_cleaned)
            logger.info(f"JSON serialization test: SUCCESS (length: {len(test_json)})")
        except Exception as json_error:
            logger.error(f"JSON serialization test: FAILED - {str(json_error)}")
            logger.error(f"Error details: {traceback.format_exc()}")
            
            # ì‘ê¸‰ ì²˜ì¹˜: ëª¨ë“  ìˆ«ì í•„ë“œë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜ ì‹œë„
            try:
                logger.info("Attempting emergency data cleaning...")
                cleaned_predictions = []
                for pred in compatible_predictions:
                    cleaned_pred = {}
                    for k, v in pred.items():
                        if isinstance(v, (int, float)):
                            if pd.isna(v) or np.isnan(v) or np.isinf(v):
                                cleaned_pred[k] = None
                            else:
                                cleaned_pred[k] = float(v)
                        else:
                            cleaned_pred[k] = safe_serialize_value(v)
                    cleaned_predictions.append(cleaned_pred)
                
                response_data['predictions'] = cleaned_predictions
                
                # ì¬ì‹œë„
                test_json = json.dumps(response_data)
                logger.info("Emergency cleaning successful")
            except Exception as emergency_error:
                logger.error(f"Emergency cleaning failed: {str(emergency_error)}")
                return jsonify({
                    'success': False,
                    'error': f'Data serialization error: {str(json_error)}'
                }), 500
        
        logger.info(f"=== Compatible Response Summary ===")
        logger.info(f"Total predictions: {len(compatible_predictions)}")
        logger.info(f"Has metrics: {cleaned_metrics is not None}")
        logger.info(f"Sample prediction fields: {list(compatible_predictions[0].keys()) if compatible_predictions else 'None'}")
        
        return jsonify(response_data)
        
    except Exception as e:
        logger.error(f"Error creating compatible response: {str(e)}")
        logger.error(traceback.format_exc())
        return jsonify({'error': f'Error creating response: {str(e)}'}), 500

@app.route('/api/results/attention-map', methods=['GET'])
def get_attention_map():
    """ì–´í…ì…˜ ë§µ ë°ì´í„° ì¡°íšŒ API"""
    global prediction_state
    
    logger.info("ğŸ” [ATTENTION_MAP] API call received - FINAL UPDATE")
    
    # ì–´í…ì…˜ ë°ì´í„° í™•ì¸
    attention_data = prediction_state.get('latest_attention_data')
    
    # í…ŒìŠ¤íŠ¸ìš©: ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ë”ë¯¸ ë°ì´í„° ìƒì„±
    test_mode = request.args.get('test', '').lower() == 'true'
    
    if not attention_data:
        if test_mode:
            logger.info("ğŸ§ª [ATTENTION_MAP] Creating test data")
            attention_data = {
                'image': 'data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mNkYPhfDwAChwGA60e6kgAAAABJRU5ErkJggg==',
                'feature_importance': {
                    'Feature_1': 0.35,
                    'Feature_2': 0.25,
                    'Feature_3': 0.20,
                    'Feature_4': 0.15,
                    'Feature_5': 0.05
                },
                'temporal_importance': {
                    '2024-01-01': 0.1,
                    '2024-01-02': 0.2,
                    '2024-01-03': 0.3,
                    '2024-01-04': 0.4
                }
            }
        else:
            logger.warning("âš ï¸ [ATTENTION_MAP] No attention data available")
            return jsonify({
                'error': 'No attention map data available',
                'message': 'ì˜ˆì¸¡ì„ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”. ì˜ˆì¸¡ ì™„ë£Œ í›„ ì–´í…ì…˜ ë§µ ë°ì´í„°ê°€ ìƒì„±ë©ë‹ˆë‹¤.',
                'suggestion': 'CSV íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ì˜ˆì¸¡ì„ ì‹¤í–‰í•œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.',
                'test_url': '/api/results/attention-map?test=true'
            }), 404
    
    logger.info(f"ğŸ“Š [ATTENTION_MAP] Available keys: {list(attention_data.keys())}")
    
    # ì–´í…ì…˜ ë°ì´í„° ì •ë¦¬ ë° ì§ë ¬í™”
    cleaned_attention = {}
    
    try:
        for key, value in attention_data.items():
            if key == 'image' and value:
                cleaned_attention[key] = value  # base64 ì´ë¯¸ì§€ëŠ” ê·¸ëŒ€ë¡œ
                logger.info(f"ğŸ“Š [ATTENTION_MAP] Image data preserved (length: {len(value) if isinstance(value, str) else 'N/A'})")
            elif isinstance(value, dict):
                cleaned_attention[key] = {}
                for k, v in value.items():
                    cleaned_attention[key][k] = safe_serialize_value(v)
                logger.info(f"ğŸ“Š [ATTENTION_MAP] Dict processed for key '{key}': {len(cleaned_attention[key])} items")
            else:
                cleaned_attention[key] = safe_serialize_value(value)
                logger.info(f"ğŸ“Š [ATTENTION_MAP] Value processed for key '{key}': {type(value)}")
        
        response_data = {
            'success': True,
            'attention_data': cleaned_attention,
            'current_date': safe_serialize_value(prediction_state.get('current_date')),
            'feature_importance': safe_serialize_value(prediction_state.get('feature_importance'))
        }
        
        # JSON ì§ë ¬í™” í…ŒìŠ¤íŠ¸
        json.dumps(response_data)
        
        logger.info(f"âœ… [ATTENTION_MAP] Response ready with keys: {list(cleaned_attention.keys())}")
        
        return jsonify(response_data)
        
    except Exception as e:
        logger.error(f"ğŸ’¥ [ATTENTION_MAP] Error processing attention data: {str(e)}")
        logger.error(traceback.format_exc())
        return jsonify({'error': f'Error processing attention map: {str(e)}'}), 500

@app.route('/api/features', methods=['GET'])
def get_features():
    """ì„ íƒëœ íŠ¹ì„± ì¡°íšŒ API"""
    global prediction_state
    
    if prediction_state['selected_features'] is None:
        return jsonify({'error': 'No feature information available'}), 404
    
    return jsonify({
        'success': True,
        'current_date': prediction_state['current_date'],
        'selected_features': prediction_state['selected_features'],
        'feature_importance': prediction_state['feature_importance']
    })

# ì •ì  íŒŒì¼ ì œê³µ
@app.route('/static/<path:path>')
def serve_static(path):
    return send_file(os.path.join('static', path))

# ê¸°ë³¸ ë¼ìš°íŠ¸
@app.route('/')
def index():
    return jsonify({
        'app': 'MOPJ Prediction API',
        'version': '1.0.0',
        'status': 'running',
        'endpoints': [
            '/api/health',
            '/api/upload',
            '/api/holidays',
            '/api/holidays/upload',
            '/api/holidays/reload',
            '/api/file/metadata',
            '/api/data/dates',
            '/api/predict',
            '/api/predict/accumulated',
            '/api/predict/status',
            '/api/results',
            '/api/results/predictions',
            '/api/results/interval-scores',
            '/api/results/moving-averages',
            '/api/results/attention-map',
            '/api/results/accumulated',
            '/api/results/accumulated/interval-scores',
            '/api/results/accumulated/<date>',
            '/api/results/accumulated/report',
            '/api/results/accumulated/visualization',
            '/api/results/reliability',  # ìƒˆë¡œ ì¶”ê°€ëœ ì‹ ë¢°ë„ API
            '/api/features'
        ],
        'new_features': [
            'Prediction consistency scoring (ì˜ˆì¸¡ ì‹ ë¢°ë„)',
            'Purchase reliability percentage (êµ¬ë§¤ ì‹ ë¢°ë„)',
            'Holiday management system',
            'Accumulated predictions analysis'
        ]
    })

# 4. API ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€ - ëˆ„ì  ì˜ˆì¸¡ ì‹œì‘
@app.route('/api/predict/accumulated', methods=['POST'])
def start_accumulated_prediction():
    """ì—¬ëŸ¬ ë‚ ì§œì— ëŒ€í•œ ëˆ„ì  ì˜ˆì¸¡ ì‹œì‘ API (ì €ì¥/ë¡œë“œ ê¸°ëŠ¥ í¬í•¨)"""
    global prediction_state
    
    if prediction_state['is_predicting']:
        return jsonify({
            'success': False,
            'error': 'Prediction already in progress',
            'progress': prediction_state['prediction_progress']
        }), 409
    
    data = request.json
    filepath = data.get('filepath')
    start_date = data.get('start_date')
    end_date = data.get('end_date')
    save_to_csv = data.get('save_to_csv', True)
    use_saved_data = data.get('use_saved_data', True)  # ì €ì¥ëœ ë°ì´í„° í™œìš© ì—¬ë¶€
    
    if not filepath or not os.path.exists(filepath):
        return jsonify({'error': 'Invalid file path'}), 400
    
    if not start_date:
        return jsonify({'error': 'Start date is required'}), 400
    
    # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ëˆ„ì  ì˜ˆì¸¡ ì‹¤í–‰
    thread = Thread(target=run_accumulated_predictions_with_save, 
                   args=(filepath, start_date, end_date, save_to_csv, use_saved_data))
    thread.daemon = True
    thread.start()
    
    return jsonify({
        'success': True,
        'message': 'Accumulated prediction started',
        'save_to_csv': save_to_csv,
        'use_saved_data': use_saved_data,
        'status_url': '/api/predict/status'
    })

# 5. API ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€ - ëˆ„ì  ì˜ˆì¸¡ ê²°ê³¼ ì¡°íšŒ
@app.route('/api/results/accumulated', methods=['GET'])
def get_accumulated_results():
    global prediction_state
    
    logger.info("ğŸ” [ACCUMULATED] API call received")
    
    if prediction_state['is_predicting']:
        logger.warning("âš ï¸ [ACCUMULATED] Prediction still in progress")
        return jsonify({
            'success': False,
            'error': 'Prediction in progress',
            'progress': prediction_state['prediction_progress']
        }), 409

    if not prediction_state['accumulated_predictions']:
        logger.error("âŒ [ACCUMULATED] No accumulated prediction results available")
        return jsonify({'error': 'No accumulated prediction results available'}), 404

    logger.info("âœ… [ACCUMULATED] Processing accumulated predictions...")
    
    # ëˆ„ì  êµ¬ë§¤ ì‹ ë¢°ë„ ê³„ì‚° - ì˜¬ë°”ë¥¸ ë°©ì‹ ì‚¬ìš©
    accumulated_purchase_reliability, _ = calculate_accumulated_purchase_reliability(
        prediction_state['accumulated_predictions']
    )
    
    logger.info(f"ğŸ’° [ACCUMULATED] Purchase reliability calculated: {accumulated_purchase_reliability}")
    
    # âœ… ìƒì„¸ ë””ë²„ê¹… ë¡œê¹… ì¶”ê°€
    logger.info(f"ğŸ” [ACCUMULATED] Purchase reliability debugging:")
    logger.info(f"   - Type: {type(accumulated_purchase_reliability)}")
    logger.info(f"   - Value: {accumulated_purchase_reliability}")
    logger.info(f"   - Repr: {repr(accumulated_purchase_reliability)}")
    if accumulated_purchase_reliability == 100.0:
        logger.warning(f"âš ï¸ [ACCUMULATED] 100% reliability detected! Detailed analysis:")
        logger.warning(f"   - Total predictions: {len(prediction_state['accumulated_predictions'])}")
        for i, pred in enumerate(prediction_state['accumulated_predictions'][:3]):  # ì²˜ìŒ 3ê°œë§Œ
            logger.warning(f"   - Prediction {i+1}: date={pred.get('date')}, interval_scores_keys={list(pred.get('interval_scores', {}).keys())}")
    
    # ë°ì´í„° ì•ˆì „ì„± ê²€ì‚¬
    safe_interval_scores = []
    if prediction_state.get('accumulated_interval_scores'):
        safe_interval_scores = [
            item for item in prediction_state['accumulated_interval_scores'] 
            if item is not None and isinstance(item, dict) and 'days' in item and item['days'] is not None
        ]
        logger.info(f"ğŸ“Š [ACCUMULATED] Safe interval scores count: {len(safe_interval_scores)}")
    else:
        logger.warning("âš ï¸ [ACCUMULATED] No accumulated_interval_scores found")
    
    consistency_scores = prediction_state.get('accumulated_consistency_scores', {})
    logger.info(f"ğŸ¯ [ACCUMULATED] Consistency scores keys: {list(consistency_scores.keys())}")
    
    # âœ… ìºì‹œ í†µê³„ ì •ë³´ ì¶”ê°€
    cache_stats = prediction_state.get('cache_statistics', {
        'total_dates': 0,
        'cached_dates': 0,
        'new_predictions': 0,
        'cache_hit_rate': 0.0
    })
    
    response_data = {
        'success': True,
        'prediction_dates': prediction_state.get('prediction_dates', []),
        'accumulated_metrics': prediction_state.get('accumulated_metrics', {}),
        'predictions': prediction_state['accumulated_predictions'],
        'accumulated_interval_scores': safe_interval_scores,
        'accumulated_consistency_scores': consistency_scores,
        'accumulated_purchase_reliability': accumulated_purchase_reliability,
        'cache_statistics': cache_stats  # âœ… ìºì‹œ í†µê³„ ì¶”ê°€
    }
    
    # âœ… ìµœì¢… ì‘ë‹µ ë°ì´í„° ê²€ì¦ ë¡œê¹…
    logger.info(f"ğŸ“¤ [ACCUMULATED] Final response validation:")
    logger.info(f"   - accumulated_purchase_reliability in response: {response_data['accumulated_purchase_reliability']}")
    logger.info(f"   - Type in response: {type(response_data['accumulated_purchase_reliability'])}")
    
    logger.info(f"ğŸ“¤ [ACCUMULATED] Response summary: predictions={len(response_data['predictions'])}, metrics_keys={list(response_data['accumulated_metrics'].keys())}, reliability={response_data['accumulated_purchase_reliability']}")
    
    return jsonify(response_data)

@app.route('/api/results/accumulated/interval-scores', methods=['GET'])
def get_accumulated_interval_scores():
    global prediction_state
    scores = prediction_state.get('accumulated_interval_scores', [])
    
    # 'days' ì†ì„±ì´ ì—†ëŠ” í•­ëª© í•„í„°ë§
    safe_scores = [
        item for item in scores 
        if item is not None and isinstance(item, dict) and 'days' in item and item['days'] is not None
    ]
    
    return jsonify(safe_scores)

# 7. ëˆ„ì  ë³´ê³ ì„œ API ì—”ë“œí¬ì¸íŠ¸
@app.route('/api/results/accumulated/report', methods=['GET'])
def get_accumulated_report():
    """ëˆ„ì  ì˜ˆì¸¡ ê²°ê³¼ ë³´ê³ ì„œ ìƒì„± ë° ë‹¤ìš´ë¡œë“œ API"""
    global prediction_state
    
    # ì˜ˆì¸¡ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš°
    if not prediction_state['accumulated_predictions']:
        return jsonify({'error': 'No accumulated prediction results available'}), 404
    
    report_file = generate_accumulated_report()
    if not report_file:
        return jsonify({'error': 'Failed to generate report'}), 500
    
    return send_file(report_file, as_attachment=True)

def return_prediction_result(pred, date, match_type):
    """
    ì˜ˆì¸¡ ê²°ê³¼ë¥¼ API ì‘ë‹µ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•˜ëŠ” í—¬í¼ í•¨ìˆ˜
    
    Parameters:
    -----------
    pred : dict
        ì˜ˆì¸¡ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    date : str
        ìš”ì²­ëœ ë‚ ì§œ
    match_type : str
        ë§¤ì¹­ ë°©ì‹ ì„¤ëª…
    
    Returns:
    --------
    JSON response
    """
    try:
        logger.info(f"ğŸ”„ [API] Returning prediction result for date={date}, match_type={match_type}")
        
        # ì˜ˆì¸¡ ë°ì´í„° ì•ˆì „í•˜ê²Œ ì¶”ì¶œ
        predictions = pred.get('predictions', [])
        if not isinstance(predictions, list):
            logger.warning(f"âš ï¸ [API] predictions is not a list: {type(predictions)}")
            predictions = []
        
        # êµ¬ê°„ ì ìˆ˜ ì•ˆì „í•˜ê²Œ ì¶”ì¶œ ë° ë³€í™˜
        interval_scores = pred.get('interval_scores', {})
        if isinstance(interval_scores, dict):
            # ë”•ì…”ë„ˆë¦¬ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            interval_scores_list = []
            for key, interval in interval_scores.items():
                if interval and isinstance(interval, dict) and 'days' in interval:
                    interval_scores_list.append(interval)
            interval_scores = interval_scores_list
        elif not isinstance(interval_scores, list):
            logger.warning(f"âš ï¸ [API] interval_scores is neither dict nor list: {type(interval_scores)}")
            interval_scores = []
        
        # ë©”íŠ¸ë¦­ ì•ˆì „í•˜ê²Œ ì¶”ì¶œ
        metrics = pred.get('metrics', {})
        if not isinstance(metrics, dict):
            logger.warning(f"âš ï¸ [API] metrics is not a dict: {type(metrics)}")
            metrics = {}
        
        # ğŸ”„ ì´ë™í‰ê·  ë°ì´í„° ì¶”ì¶œ (ìºì‹œëœ ë°ì´í„° ë˜ëŠ” íŒŒì¼ì—ì„œ ë¡œë“œ)
        ma_results = pred.get('ma_results', {})
        if not ma_results:
            # íŒŒì¼ë³„ ìºì‹œì—ì„œ MA íŒŒì¼ ë¡œë“œ ì‹œë„
            try:
                current_file = prediction_state.get('current_file')
                if current_file:
                    cache_dirs = get_file_cache_dirs(current_file)
                    predictions_dir = cache_dirs['predictions']
                    
                    pred_start_date = pred.get('prediction_start_date') or date
                    ma_file_path = predictions_dir / f"prediction_start_{pd.to_datetime(pred_start_date).strftime('%Y%m%d')}_ma.json"
                else:
                    # ë°±ì—…: ê¸€ë¡œë²Œ ìºì‹œ ì‚¬ìš©
                    pred_start_date = pred.get('prediction_start_date') or date
                    ma_file_path = Path(PREDICTIONS_DIR) / f"prediction_start_{pd.to_datetime(pred_start_date).strftime('%Y%m%d')}_ma.json"
                
                if ma_file_path.exists():
                    with open(ma_file_path, 'r', encoding='utf-8') as f:
                        ma_results = json.load(f)
                    logger.info(f"ğŸ“Š [API] MA results loaded from file for {date}: {len(ma_results)} windows")
                else:
                    logger.info(f"âš ï¸ [API] No MA file found for {date}: {ma_file_path}")
                    
                    # íŒŒì¼ì´ ì—†ìœ¼ë©´ ì˜ˆì¸¡ ë°ì´í„°ì—ì„œ ì¬ê³„ì‚° (íˆìŠ¤í† ë¦¬ì»¬ ë°ì´í„° ì—†ì´ ì œí•œì ìœ¼ë¡œ)
                    if predictions:
                        ma_results = calculate_moving_averages_with_history(
                            predictions, None, target_col='MOPJ', windows=[5, 10, 23]
                        )
                        logger.info(f"ğŸ“Š [API] MA results recalculated for {date}: {len(ma_results)} windows")
            except Exception as e:
                logger.warning(f"âš ï¸ [API] Error loading/calculating MA for {date}: {str(e)}")
                ma_results = {}
        
        # ğŸ¯ Attention ë°ì´í„° ì¶”ì¶œ
        attention_data = pred.get('attention_data', {})
        if not attention_data:
            # íŒŒì¼ë³„ ìºì‹œì—ì„œ Attention íŒŒì¼ ë¡œë“œ ì‹œë„
            try:
                current_file = prediction_state.get('current_file')
                if current_file:
                    cache_dirs = get_file_cache_dirs(current_file)
                    predictions_dir = cache_dirs['predictions']
                    
                    pred_start_date = pred.get('prediction_start_date') or date
                    attention_file_path = predictions_dir / f"prediction_start_{pd.to_datetime(pred_start_date).strftime('%Y%m%d')}_attention.json"
                else:
                    # ë°±ì—…: ê¸€ë¡œë²Œ ìºì‹œ ì‚¬ìš©
                    pred_start_date = pred.get('prediction_start_date') or date
                    attention_file_path = Path(PREDICTIONS_DIR) / f"prediction_start_{pd.to_datetime(pred_start_date).strftime('%Y%m%d')}_attention.json"
                
                if attention_file_path.exists():
                    with open(attention_file_path, 'r', encoding='utf-8') as f:
                        attention_data = json.load(f)
                    logger.info(f"ğŸ“Š [API] Attention data loaded from file for {date}")
                else:
                    logger.info(f"âš ï¸ [API] No attention file found for {date}: {attention_file_path}")
            except Exception as e:
                logger.warning(f"âš ï¸ [API] Error loading attention data for {date}: {str(e)}")
        
        # ê¸°ë³¸ ì‘ë‹µ ë°ì´í„° êµ¬ì„±
        response_data = {
            'success': True,
            'date': date,
            'predictions': predictions,
            'interval_scores': interval_scores,
            'metrics': metrics,
            'ma_results': ma_results,
            'attention_data': attention_data,
            'next_semimonthly_period': pred.get('next_semimonthly_period'),
            'actual_business_days': pred.get('actual_business_days'),
            'match_type': match_type,
            'data_end_date': pred.get('date'),
            'prediction_start_date': pred.get('prediction_start_date')
        }
        
        # ê° í•„ë“œë¥¼ ê°œë³„ì ìœ¼ë¡œ ì•ˆì „í•˜ê²Œ ì§ë ¬í™”
        safe_response = {}
        for key, value in response_data.items():
            safe_value = safe_serialize_value(value)
            if safe_value is not None:  # Noneì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ì¶”ê°€
                safe_response[key] = safe_value
        
        # successì™€ dateëŠ” í•­ìƒ í¬í•¨
        safe_response['success'] = True
        safe_response['date'] = date
        
        logger.info(f"âœ… [API] Successfully prepared response for {date}: predictions={len(safe_response.get('predictions', []))}, interval_scores={len(safe_response.get('interval_scores', []))}, ma_windows={len(safe_response.get('ma_results', {}))}, attention_data={bool(safe_response.get('attention_data'))}")
        
        return jsonify(safe_response)
        
    except Exception as e:
        logger.error(f"ğŸ’¥ [API] Error in return_prediction_result for {date}: {str(e)}")
        logger.error(traceback.format_exc())
        return jsonify({
            'success': False,
            'error': f'Error processing prediction result: {str(e)}',
            'date': date
        }), 500

# 8. API ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€ - íŠ¹ì • ë‚ ì§œ ì˜ˆì¸¡ ê²°ê³¼ ì¡°íšŒ

@app.route('/api/results/accumulated/<date>', methods=['GET'])
def get_accumulated_result_by_date(date):
    """íŠ¹ì • ë‚ ì§œì˜ ëˆ„ì  ì˜ˆì¸¡ ê²°ê³¼ ì¡°íšŒ API"""
    global prediction_state
    
    logger.info(f"ğŸ” [API] Searching for accumulated result by date: {date}")
    
    if not prediction_state['accumulated_predictions']:
        logger.warning("âŒ [API] No accumulated prediction results available")
        return jsonify({'error': 'No accumulated prediction results available'}), 404
    
    logger.info(f"ğŸ“Š [API] Available prediction dates (data_end_date): {[p['date'] for p in prediction_state['accumulated_predictions']]}")
    
    # âœ… 1ë‹¨ê³„: ì •í™•í•œ ë°ì´í„° ê¸°ì¤€ì¼ ë§¤ì¹­ ìš°ì„  í™•ì¸
    logger.info(f"ğŸ” [API] Step 1: Looking for EXACT data_end_date match for {date}")
    for i, pred in enumerate(prediction_state['accumulated_predictions']):
        data_end_date = pred.get('date')  # ë°ì´í„° ê¸°ì¤€ì¼
        
        logger.info(f"ğŸ” [API] Checking prediction {i+1}: data_end_date={data_end_date}")
        
        if data_end_date == date:
            logger.info(f"âœ… [API] Found prediction by EXACT DATA END DATE match: {date}")
            logger.info(f"ğŸ“Š [API] Prediction data preview: predictions={len(pred.get('predictions', []))}, interval_scores={len(pred.get('interval_scores', {}))}")
            return return_prediction_result(pred, date, "exact data end date")
    
    # âœ… 2ë‹¨ê³„: ì •í™•í•œ ë§¤ì¹­ì´ ì—†ìœ¼ë©´ ê³„ì‚°ëœ ì˜ˆì¸¡ ì‹œì‘ì¼ë¡œ ë§¤ì¹­
    logger.info(f"ğŸ” [API] Step 2: No exact match found. Looking for calculated prediction start date match for {date}")
    for i, pred in enumerate(prediction_state['accumulated_predictions']):
        data_end_date = pred.get('date')  # ë°ì´í„° ê¸°ì¤€ì¼
        prediction_start_date = pred.get('prediction_start_date')  # ì˜ˆì¸¡ ì‹œì‘ì¼
        
        logger.info(f"ğŸ” [API] Checking prediction {i+1}: data_end_date={data_end_date}, prediction_start_date={prediction_start_date}")
        
        if data_end_date:
            try:
                data_end_dt = pd.to_datetime(data_end_date)
                calculated_start_date = data_end_dt + pd.Timedelta(days=1)
                
                # ì£¼ë§ê³¼ íœ´ì¼ ê±´ë„ˆë›°ê¸°
                while calculated_start_date.weekday() >= 5 or is_holiday(calculated_start_date):
                    calculated_start_date += pd.Timedelta(days=1)
                
                calculated_start_str = calculated_start_date.strftime('%Y-%m-%d')
                
                if calculated_start_str == date:
                    logger.info(f"âœ… [API] Found prediction by CALCULATED PREDICTION START DATE: {date} (from data end date: {data_end_date})")
                    return return_prediction_result(pred, date, "calculated prediction start date from data end date")
                    
            except Exception as e:
                logger.warning(f"âš ï¸ [API] Error calculating prediction start date for {data_end_date}: {str(e)}")
                continue
    
    logger.error(f"âŒ [API] No prediction results found for date {date}")
    return jsonify({'error': f'No prediction results for date {date}'}), 404

# 10. ëˆ„ì  ì§€í‘œ ì‹œê°í™” API ì—”ë“œí¬ì¸íŠ¸
@app.route('/api/results/accumulated/visualization', methods=['GET'])
def get_accumulated_visualization():
    """ëˆ„ì  ì˜ˆì¸¡ ì§€í‘œ ì‹œê°í™” API"""
    global prediction_state
    
    if not prediction_state['accumulated_predictions']:
        return jsonify({'error': 'No accumulated prediction results available'}), 404
    
    filename, img_str = visualize_accumulated_metrics()
    if not filename:
        return jsonify({'error': 'Failed to generate visualization'}), 500
    
    return jsonify({
        'success': True,
        'file_path': filename,
        'image': img_str
    })

# ìƒˆë¡œìš´ API ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€
@app.route('/api/results/reliability', methods=['GET'])
def get_reliability_scores():
    """ì‹ ë¢°ë„ ì ìˆ˜ ì¡°íšŒ API"""
    global prediction_state
    
    # ë‹¨ì¼ ì˜ˆì¸¡ ì‹ ë¢°ë„
    single_reliability = {}
    if prediction_state.get('latest_interval_scores') and prediction_state.get('latest_predictions'):
        try:
            # ì‹¤ì œ ì˜ì—…ì¼ ìˆ˜ ê³„ì‚°
            actual_business_days = len([p for p in prediction_state['latest_predictions'] 
                                       if p.get('Date') and not p.get('is_synthetic', False)])
            
            single_reliability = {
                'period': prediction_state['next_semimonthly_period']
            }
        except Exception as e:
            logger.error(f"Error calculating single prediction reliability: {str(e)}")
            single_reliability = {'error': 'Unable to calculate single prediction reliability'}
    
    # ëˆ„ì  ì˜ˆì¸¡ ì‹ ë¢°ë„ (ì•ˆì „í•œ ì ‘ê·¼)
    accumulated_reliability = prediction_state.get('accumulated_consistency_scores', {})
    
    return jsonify({
        'success': True,
        'single_prediction_reliability': single_reliability,
        'accumulated_prediction_reliability': accumulated_reliability
    })

@app.route('/api/cache/clear/accumulated', methods=['POST'])
def clear_accumulated_cache():
    """ëˆ„ì  ì˜ˆì¸¡ ìºì‹œ í´ë¦¬ì–´"""
    global prediction_state
    
    try:
        # ëˆ„ì  ì˜ˆì¸¡ ê´€ë ¨ ìƒíƒœ í´ë¦¬ì–´
        prediction_state['accumulated_predictions'] = []
        prediction_state['accumulated_metrics'] = {}
        prediction_state['accumulated_interval_scores'] = []
        prediction_state['accumulated_consistency_scores'] = {}
        prediction_state['accumulated_purchase_reliability'] = 0
        prediction_state['prediction_dates'] = []
        
        logger.info("ğŸ§¹ [CACHE] Accumulated prediction cache cleared")
        
        return jsonify({
            'success': True,
            'message': 'Accumulated prediction cache cleared successfully'
        })
        
    except Exception as e:
        logger.error(f"âŒ [CACHE] Error clearing accumulated cache: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/debug/reliability', methods=['GET'])
def debug_reliability_calculation():
    """êµ¬ë§¤ ì‹ ë¢°ë„ ê³„ì‚° ë””ë²„ê¹… API"""
    global prediction_state
    
    if not prediction_state['accumulated_predictions']:
        return jsonify({'error': 'No accumulated prediction results available'}), 404
    
    predictions = prediction_state['accumulated_predictions']
    print(f"ğŸ” [DEBUG] Total predictions: {len(predictions)}")
    
    debug_data = {
        'prediction_count': len(predictions),
        'predictions_details': []
    }
    
    total_score = 0
    
    for i, pred in enumerate(predictions):
        pred_date = pred.get('date')
        interval_scores = pred.get('interval_scores', {})
        
        print(f"ğŸ“Š [DEBUG] Prediction {i+1} ({pred_date}):")
        print(f"   - interval_scores type: {type(interval_scores)}")
        print(f"   - interval_scores keys: {list(interval_scores.keys()) if isinstance(interval_scores, dict) else 'N/A'}")
        
        pred_detail = {
            'date': pred_date,
            'interval_scores_type': str(type(interval_scores)),
            'interval_scores_keys': list(interval_scores.keys()) if isinstance(interval_scores, dict) else [],
            'individual_scores': [],
            'best_score': 0
        }
        
        if isinstance(interval_scores, dict):
            for key, score_data in interval_scores.items():
                print(f"   - {key}: {score_data}")
                if isinstance(score_data, dict) and 'score' in score_data:
                    score_value = score_data.get('score', 0)
                    pred_detail['individual_scores'].append({
                        'key': key,
                        'score': score_value,
                        'full_data': score_data
                    })
                    print(f"     -> score: {score_value}")
        
        if pred_detail['individual_scores']:
            best_score = max([s['score'] for s in pred_detail['individual_scores']])
            # ì ìˆ˜ë¥¼ 3ì ìœ¼ë¡œ ì œí•œ
            capped_score = min(best_score, 3.0)
            pred_detail['best_score'] = best_score
            pred_detail['capped_score'] = capped_score
            total_score += capped_score
            print(f"   - Best score: {best_score:.1f}, Capped score: {capped_score:.1f}")
        
        debug_data['predictions_details'].append(pred_detail)
    
    max_possible_score = len(predictions) * 3
    reliability = (total_score / max_possible_score) * 100 if max_possible_score > 0 else 0
    
    debug_data.update({
        'total_score': total_score,
        'max_possible_score': max_possible_score,
        'reliability_percentage': reliability
    })
    
    print(f"ğŸ¯ [DEBUG] CALCULATION SUMMARY:")
    print(f"   - Total predictions: {len(predictions)}")
    print(f"   - Total score: {total_score}")
    print(f"   - Max possible score: {max_possible_score}")
    print(f"   - Reliability: {reliability:.1f}%")
    
    return jsonify(debug_data)

@app.route('/api/cache/check', methods=['POST'])
def check_cached_predictions():
    """ëˆ„ì  ì˜ˆì¸¡ ë²”ìœ„ì—ì„œ ìºì‹œëœ ì˜ˆì¸¡ì´ ì–¼ë§ˆë‚˜ ìˆëŠ”ì§€ í™•ì¸"""
    data = request.get_json()
    start_date = data.get('start_date')
    end_date = data.get('end_date')
    
    if not start_date or not end_date:
        return jsonify({'error': 'start_date and end_date are required'}), 400
    
    try:
        logger.info(f"ğŸ” [CACHE_CHECK] Checking cache availability for {start_date} to {end_date}")
        
        # ì €ì¥ëœ ì˜ˆì¸¡ í™•ì¸
        cached_predictions = load_accumulated_predictions_from_csv(start_date, end_date)
        
        # ì „ì²´ ë²”ìœ„ ê³„ì‚° (ë°ì´í„° ê¸°ì¤€ì¼ ê¸°ì¤€)
        start_dt = pd.to_datetime(start_date)
        end_dt = pd.to_datetime(end_date)
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ë‚ ì§œ ê³„ì‚° (ë°ì´í„° ê¸°ì¤€ì¼)
        available_dates = []
        current_dt = start_dt
        while current_dt <= end_dt:
            # ì˜ì—…ì¼ë§Œ í¬í•¨ (ì£¼ë§ê³¼ íœ´ì¼ ì œì™¸)
            if current_dt.weekday() < 5 and not is_holiday(current_dt):
                available_dates.append(current_dt.strftime('%Y-%m-%d'))
            current_dt += pd.Timedelta(days=1)
        
        # ìºì‹œëœ ë‚ ì§œ ëª©ë¡
        cached_dates = [pred['date'] for pred in cached_predictions]
        missing_dates = [date for date in available_dates if date not in cached_dates]
        
        cache_percentage = round(len(cached_predictions) / max(len(available_dates), 1) * 100, 1)
        
        logger.info(f"ğŸ“Š [CACHE_CHECK] Cache status: {len(cached_predictions)}/{len(available_dates)} ({cache_percentage}%)")
        
        return jsonify({
            'success': True,
            'total_dates_in_range': len(available_dates),
            'cached_predictions': len(cached_predictions),
            'cached_dates': cached_dates,
            'missing_dates': missing_dates,
            'cache_percentage': cache_percentage,
            'will_use_cache': len(cached_predictions) > 0,
            'estimated_time_savings': f"ì•½ {len(cached_predictions) * 3}ë¶„ ì ˆì•½ ì˜ˆìƒ" if len(cached_predictions) > 0 else "ì—†ìŒ"
        })
        
    except Exception as e:
        logger.error(f"âŒ [CACHE_CHECK] Error checking cached predictions: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/results/accumulated/recent', methods=['GET'])
def get_recent_accumulated_results():
    """
    í˜ì´ì§€ ë¡œë“œ ì‹œ ìµœê·¼ ëˆ„ì  ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ìë™ìœ¼ë¡œ ë³µì›í•˜ëŠ” API
    """
    try:
        # ì €ì¥ëœ ì˜ˆì¸¡ ëª©ë¡ ì¡°íšŒ (ìµœê·¼ ê²ƒë¶€í„°)
        predictions_list = get_saved_predictions_list(limit=50)
        
        if not predictions_list:
            return jsonify({
                'success': False, 
                'message': 'No saved predictions found',
                'has_recent_results': False
            })
        
        # ë‚ ì§œë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ ì—°ì†ëœ ë²”ìœ„ ì°¾ê¸°
        dates_by_groups = {}
        for pred in predictions_list:
            data_end_date = pred.get('data_end_date', pred.get('prediction_date'))
            if data_end_date:
                date_obj = pd.to_datetime(data_end_date)
                # ì£¼ì°¨ë³„ë¡œ ê·¸ë£¹í™” (ê°™ì€ ì£¼ì˜ ì˜ˆì¸¡ë“¤ì„ í•˜ë‚˜ì˜ ë²”ìœ„ë¡œ ê°„ì£¼)
                week_key = date_obj.strftime('%Y-W%U')
                if week_key not in dates_by_groups:
                    dates_by_groups[week_key] = []
                dates_by_groups[week_key].append({
                    'date': data_end_date,
                    'date_obj': date_obj,
                    'pred_info': pred
                })
        
        # ê°€ì¥ ìµœê·¼ ê·¸ë£¹ ì„ íƒ
        if not dates_by_groups:
            return jsonify({
                'success': False, 
                'message': 'No valid date groups found',
                'has_recent_results': False
            })
        
        # ìµœê·¼ ì£¼ì˜ ì˜ˆì¸¡ë“¤ ê°€ì ¸ì˜¤ê¸°
        latest_week = max(dates_by_groups.keys())
        latest_group = dates_by_groups[latest_week]
        latest_group.sort(key=lambda x: x['date_obj'])
        
        # ì—°ì†ëœ ë‚ ì§œ ë²”ìœ„ ì°¾ê¸°
        start_date = latest_group[0]['date_obj']
        end_date = latest_group[-1]['date_obj']
        
        logger.info(f"ğŸ”„ [AUTO_RESTORE] Found recent accumulated predictions: {start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}")
        
        # ê¸°ì¡´ ìºì‹œì—ì„œ ëˆ„ì  ê²°ê³¼ ë¡œë“œ
        loaded_predictions = load_accumulated_predictions_from_csv(start_date, end_date)
        
        if not loaded_predictions:
            return jsonify({
                'success': False, 
                'message': 'Failed to load cached predictions',
                'has_recent_results': False
            })
        
        # ëˆ„ì  ë©”íŠ¸ë¦­ ê³„ì‚°
        accumulated_metrics = {
            'f1': 0.0,
            'accuracy': 0.0,
            'mape': 0.0,
            'weighted_score': 0.0,
            'total_predictions': 0
        }
        
        for pred in loaded_predictions:
            metrics = pred.get('metrics', {})
            if isinstance(metrics, dict):
                accumulated_metrics['f1'] += metrics.get('f1', 0.0)
                accumulated_metrics['accuracy'] += metrics.get('accuracy', 0.0)
                accumulated_metrics['mape'] += metrics.get('mape', 0.0)
                accumulated_metrics['weighted_score'] += metrics.get('weighted_score', 0.0)
                accumulated_metrics['total_predictions'] += 1
        
        if accumulated_metrics['total_predictions'] > 0:
            count = accumulated_metrics['total_predictions']
            accumulated_metrics['f1'] /= count
            accumulated_metrics['accuracy'] /= count
            accumulated_metrics['mape'] /= count
            accumulated_metrics['weighted_score'] /= count
        
        # êµ¬ê°„ ì ìˆ˜ ê³„ì‚°
        accumulated_interval_scores = {}
        for pred in loaded_predictions:
            interval_scores = pred.get('interval_scores', {})
            if isinstance(interval_scores, dict):
                for interval in interval_scores.values():
                    if not interval or not isinstance(interval, dict) or 'days' not in interval or interval['days'] is None:
                        continue
                    interval_key = f"{format_date(interval['start_date'])}-{format_date(interval['end_date'])}"
                    if interval_key in accumulated_interval_scores:
                        accumulated_interval_scores[interval_key]['score'] += interval['score']
                        accumulated_interval_scores[interval_key]['count'] += 1
                    else:
                        accumulated_interval_scores[interval_key] = interval.copy()
                        accumulated_interval_scores[interval_key]['count'] = 1
        
        # ì •ë ¬ëœ êµ¬ê°„ ì ìˆ˜ ë¦¬ìŠ¤íŠ¸
        accumulated_scores_list = [
            v for v in accumulated_interval_scores.values()
            if v is not None and isinstance(v, dict) and 'days' in v and v['days'] is not None
        ]
        accumulated_scores_list.sort(key=lambda x: x['score'], reverse=True)
        
        # êµ¬ë§¤ ì‹ ë¢°ë„ ê³„ì‚°
        accumulated_purchase_reliability, _ = calculate_accumulated_purchase_reliability(loaded_predictions)
        
        # ì¼ê´€ì„± ì ìˆ˜ ê³„ì‚°
        unique_periods = set()
        for pred in loaded_predictions:
            if 'next_semimonthly_period' in pred and pred['next_semimonthly_period']:
                unique_periods.add(pred['next_semimonthly_period'])
        
        accumulated_consistency_scores = {}
        for period in unique_periods:
            try:
                consistency_data = calculate_prediction_consistency(loaded_predictions, period)
                accumulated_consistency_scores[period] = consistency_data
            except Exception as e:
                logger.error(f"Error calculating consistency for period {period}: {str(e)}")
        
        # ìºì‹œ í†µê³„
        cache_statistics = {
            'total_dates': len(loaded_predictions),
            'cached_dates': len(loaded_predictions),
            'new_predictions': 0,
            'cache_hit_rate': 100.0
        }
        
        # ì „ì—­ ìƒíƒœ ì—…ë°ì´íŠ¸ (ì„ íƒì )
        global prediction_state
        prediction_state['accumulated_predictions'] = loaded_predictions
        prediction_state['accumulated_metrics'] = accumulated_metrics
        prediction_state['prediction_dates'] = [p['date'] for p in loaded_predictions]
        prediction_state['accumulated_interval_scores'] = accumulated_scores_list
        prediction_state['accumulated_consistency_scores'] = accumulated_consistency_scores
        prediction_state['accumulated_purchase_reliability'] = accumulated_purchase_reliability
        prediction_state['cache_statistics'] = cache_statistics
        
        logger.info(f"âœ… [AUTO_RESTORE] Successfully restored {len(loaded_predictions)} accumulated predictions")
        
        return jsonify({
            'success': True,
            'has_recent_results': True,
            'restored_range': {
                'start_date': start_date.strftime('%Y-%m-%d'),
                'end_date': end_date.strftime('%Y-%m-%d'),
                'prediction_count': len(loaded_predictions)
            },
            'prediction_dates': [p['date'] for p in loaded_predictions],
            'accumulated_metrics': accumulated_metrics,
            'predictions': loaded_predictions,
            'accumulated_interval_scores': accumulated_scores_list,
            'accumulated_consistency_scores': accumulated_consistency_scores,
            'accumulated_purchase_reliability': accumulated_purchase_reliability,
            'cache_statistics': cache_statistics,
            'message': f"ìµœê·¼ ëˆ„ì  ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ìë™ìœ¼ë¡œ ë³µì›í–ˆìŠµë‹ˆë‹¤ ({start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')})"
        })
        
    except Exception as e:
        logger.error(f"âŒ [AUTO_RESTORE] Error restoring recent accumulated results: {str(e)}")
        logger.error(traceback.format_exc())
        return jsonify({
            'success': False, 
            'error': str(e),
            'has_recent_results': False
        }), 500

@app.route('/api/cache/rebuild-index', methods=['POST'])
def rebuild_predictions_index_api():
    """ì˜ˆì¸¡ ì¸ë±ìŠ¤ ì¬ìƒì„± API (rebuild_index.py ê¸°ëŠ¥ì„ í†µí•©)"""
    try:
        # í˜„ì¬ íŒŒì¼ì˜ ìºì‹œ ë””ë ‰í† ë¦¬ ê°€ì ¸ì˜¤ê¸°
        current_file = prediction_state.get('current_file')
        if not current_file:
            return jsonify({'success': False, 'error': 'í˜„ì¬ ì—…ë¡œë“œëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.'})
        
        # ğŸ”§ ìƒˆë¡œìš´ rebuild í•¨ìˆ˜ ì‚¬ìš©
        success = rebuild_predictions_index_from_existing_files()
        
        if success:
            cache_dirs = get_file_cache_dirs(current_file)
            index_file = cache_dirs['predictions'] / 'predictions_index.csv'
            
            # ê²°ê³¼ ë°ì´í„° ì½ê¸°
            index_data = []
            if index_file.exists():
                with open(index_file, 'r', encoding='utf-8') as f:
                    reader = csv.DictReader(f)
                    index_data = list(reader)
            
            return jsonify({
                'success': True,
                'message': f'ì¸ë±ìŠ¤ íŒŒì¼ì„ ì„±ê³µì ìœ¼ë¡œ ì¬ìƒì„±í–ˆìŠµë‹ˆë‹¤. ({len(index_data)}ê°œ í•­ëª©)',
                'file_location': str(index_file),
                'entries_count': len(index_data),
                'rebuilt_entries': [{'date': row.get('prediction_start_date', ''), 'data_end': row.get('data_end_date', '')} for row in index_data]
            })
        else:
            return jsonify({
                'success': False,
                'error': 'ì¸ë±ìŠ¤ ì¬ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë¡œê·¸ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.'
            })
        
    except Exception as e:
        logger.error(f"âŒ Error rebuilding predictions index: {str(e)}")
        logger.error(traceback.format_exc())
        return jsonify({'success': False, 'error': f'ì¸ë±ìŠ¤ ì¬ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}'})

@app.route('/api/cache/clear/semimonthly', methods=['POST'])
def clear_semimonthly_cache():
    """íŠ¹ì • ë°˜ì›” ê¸°ê°„ì˜ ìºì‹œë§Œ ì‚­ì œí•˜ëŠ” API"""
    try:
        data = request.json
        target_date = data.get('date')
        
        if not target_date:
            return jsonify({'error': 'Date parameter is required'}), 400
        
        target_date = pd.to_datetime(target_date)
        target_semimonthly = get_semimonthly_period(target_date)
        
        logger.info(f"ğŸ—‘ï¸ [API] Clearing cache for semimonthly period: {target_semimonthly}")
        
        # í˜„ì¬ íŒŒì¼ì˜ ìºì‹œ ë””ë ‰í† ë¦¬ì—ì„œ í•´ë‹¹ ë°˜ì›” ìºì‹œ ì‚­ì œ
        cache_dirs = get_file_cache_dirs()
        predictions_dir = cache_dirs['predictions']
        
        deleted_files = []
        
        if predictions_dir.exists():
            # ë©”íƒ€ íŒŒì¼ í™•ì¸í•˜ì—¬ ë°˜ì›” ê¸°ê°„ì´ ì¼ì¹˜í•˜ëŠ” ìºì‹œ ì‚­ì œ
            for meta_file in predictions_dir.glob("*_meta.json"):
                try:
                    with open(meta_file, 'r', encoding='utf-8') as f:
                        meta_data = json.load(f)
                    
                    cached_data_end_date = meta_data.get('data_end_date')
                    if cached_data_end_date:
                        cached_data_end_date = pd.to_datetime(cached_data_end_date)
                        cached_semimonthly = get_semimonthly_period(cached_data_end_date)
                        
                        if cached_semimonthly == target_semimonthly:
                            # ê´€ë ¨ íŒŒì¼ë“¤ ì‚­ì œ
                            base_name = meta_file.stem.replace('_meta', '')
                            files_to_delete = [
                                meta_file,
                                meta_file.parent / f"{base_name}.csv",
                                meta_file.parent / f"{base_name}_attention.json",
                                meta_file.parent / f"{base_name}_ma.json"
                            ]
                            
                            for file_path in files_to_delete:
                                if file_path.exists():
                                    file_path.unlink()
                                    deleted_files.append(str(file_path.name))
                                    logger.info(f"  ğŸ—‘ï¸ Deleted: {file_path.name}")
                            
                except Exception as e:
                    logger.warning(f"âš ï¸ Error processing meta file {meta_file}: {str(e)}")
                    continue
        
        return jsonify({
            'success': True,
            'message': f'Cache cleared for semimonthly period: {target_semimonthly}',
            'target_semimonthly': target_semimonthly,
            'target_date': target_date.strftime('%Y-%m-%d'),
            'deleted_files': deleted_files,
            'deleted_count': len(deleted_files)
        })
        
    except Exception as e:
        logger.error(f"âŒ [API] Error clearing semimonthly cache: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

#######################################################################
# VARMAX ì˜ˆì¸¡ ì €ì¥/ë¡œë“œ ì‹œìŠ¤í…œ
#######################################################################

def save_varmax_prediction(prediction_results: dict, prediction_date):
    """
    VARMAX ì˜ˆì¸¡ ê²°ê³¼ë¥¼ íŒŒì¼ì— ì €ì¥í•˜ëŠ” í•¨ìˆ˜
    """
    try:
        file_path = prediction_state.get('current_file', None)
        if not file_path:
            logger.warning("No current file path for VARMAX prediction save")
            return False
            
        # íŒŒì¼ë³„ ìºì‹œ ë””ë ‰í† ë¦¬ ê°€ì ¸ì˜¤ê¸°
        cache_dirs = get_file_cache_dirs(file_path)
        varmax_dir = cache_dirs['root'] / 'varmax'
        varmax_dir.mkdir(exist_ok=True)
        
        # ì €ì¥í•  íŒŒì¼ ê²½ë¡œ
        prediction_file = varmax_dir / f"varmax_prediction_{prediction_date}.json"
        
        # JSONìœ¼ë¡œ ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
        clean_results = {}
        for key, value in prediction_results.items():
            try:
                clean_results[key] = safe_serialize_value(value)
            except Exception as e:
                logger.warning(f"Failed to serialize {key}: {e}")
                continue
        
        # ë©”íƒ€ë°ì´í„° ì¶”ê°€
        clean_results['metadata'] = {
            'prediction_date': prediction_date,
            'created_at': datetime.now().isoformat(),
            'file_path': file_path,
            'model_type': 'VARMAX'
        }
        
        # íŒŒì¼ì— ì €ì¥
        with open(prediction_file, 'w', encoding='utf-8') as f:
            json.dump(clean_results, f, ensure_ascii=False, indent=2)
        
        # ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸
        update_varmax_predictions_index({
            'prediction_date': prediction_date,
            'file_path': str(prediction_file),
            'created_at': datetime.now().isoformat(),
            'original_file': file_path
        })
        
        logger.info(f"âœ… VARMAX prediction saved: {prediction_file}")
        return True
        
    except Exception as e:
        logger.error(f"âŒ Failed to save VARMAX prediction: {e}")
        logger.error(traceback.format_exc())
        return False

def load_varmax_prediction(prediction_date):
    """
    ì €ì¥ëœ VARMAX ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜
    """
    try:
        file_path = prediction_state.get('current_file', None)
        if not file_path:
            logger.warning("No current file path for VARMAX prediction load")
            return None
            
        # íŒŒì¼ë³„ ìºì‹œ ë””ë ‰í† ë¦¬ ê°€ì ¸ì˜¤ê¸°
        cache_dirs = get_file_cache_dirs(file_path)
        varmax_dir = cache_dirs['root'] / 'varmax'
        
        # ë¡œë“œí•  íŒŒì¼ ê²½ë¡œ
        prediction_file = varmax_dir / f"varmax_prediction_{prediction_date}.json"
        
        if not prediction_file.exists():
            logger.info(f"VARMAX prediction file not found: {prediction_file}")
            return None
            
        # íŒŒì¼ì—ì„œ ë¡œë“œ
        with open(prediction_file, 'r', encoding='utf-8') as f:
            results = json.load(f)
        
        # ğŸ” ë¡œë“œëœ ë°ì´í„° íƒ€ì… ë° êµ¬ì¡° í™•ì¸
        logger.info(f"ğŸ” [VARMAX_LOAD] Loaded data type: {type(results)}")
        if isinstance(results, dict):
            logger.info(f"ğŸ” [VARMAX_LOAD] Loaded data keys: {list(results.keys())}")
            
            # ğŸ”§ ma_results í•„ë“œ íƒ€ì… í™•ì¸ ë° ìˆ˜ì •
            if 'ma_results' in results:
                ma_results = results['ma_results']
                logger.info(f"ğŸ” [VARMAX_LOAD] MA results type: {type(ma_results)}")
                
                if isinstance(ma_results, str):
                    logger.warning(f"âš ï¸ [VARMAX_LOAD] MA results is string, attempting to parse as JSON...")
                    try:
                        results['ma_results'] = json.loads(ma_results)
                        logger.info(f"ğŸ”§ [VARMAX_LOAD] Successfully parsed ma_results from string to dict")
                    except Exception as e:
                        logger.error(f"âŒ [VARMAX_LOAD] Failed to parse ma_results string as JSON: {e}")
                        results['ma_results'] = {}
                elif not isinstance(ma_results, dict):
                    logger.warning(f"âš ï¸ [VARMAX_LOAD] MA results has unexpected type: {type(ma_results)}, setting empty dict")
                    results['ma_results'] = {}
                    
        elif isinstance(results, str):
            logger.warning(f"âš ï¸ [VARMAX_LOAD] Loaded data is string, not dict: {results[:100]}...")
            # ë¬¸ìì—´ì¸ ê²½ìš° ë‹¤ì‹œ JSON íŒŒì‹± ì‹œë„
            try:
                results = json.loads(results)
                logger.info(f"ğŸ”§ [VARMAX_LOAD] Re-parsed string as JSON: {type(results)}")
            except:
                logger.error(f"âŒ [VARMAX_LOAD] Failed to re-parse string as JSON")
                return None
        else:
            logger.warning(f"âš ï¸ [VARMAX_LOAD] Unexpected data type: {type(results)}")
        
        logger.info(f"âœ… VARMAX prediction loaded: {prediction_file}")
        return results
        
    except Exception as e:
        logger.error(f"âŒ Failed to load VARMAX prediction: {e}")
        logger.error(traceback.format_exc())
        return None

def update_varmax_predictions_index(metadata):
    """
    VARMAX ì˜ˆì¸¡ ì¸ë±ìŠ¤ë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” í•¨ìˆ˜
    """
    try:
        file_path = prediction_state.get('current_file', None)
        if not file_path:
            return False
            
        cache_dirs = get_file_cache_dirs(file_path)
        varmax_dir = cache_dirs['root'] / 'varmax'
        varmax_dir.mkdir(exist_ok=True)
        
        index_file = varmax_dir / 'varmax_index.json'
        
        # ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ
        if index_file.exists():
            with open(index_file, 'r', encoding='utf-8') as f:
                index = json.load(f)
        else:
            index = {'predictions': []}
        
        # ìƒˆ ì˜ˆì¸¡ ì¶”ê°€ (ì¤‘ë³µ ì œê±°)
        prediction_date = metadata['prediction_date']
        index['predictions'] = [p for p in index['predictions'] if p['prediction_date'] != prediction_date]
        index['predictions'].append(metadata)
        
        # ë‚ ì§œìˆœ ì •ë ¬ (ìµœì‹ ìˆœ)
        index['predictions'].sort(key=lambda x: x['prediction_date'], reverse=True)
        
        # ìµœëŒ€ 100ê°œ ìœ ì§€
        index['predictions'] = index['predictions'][:100]
        
        # ì¸ë±ìŠ¤ ì €ì¥
        with open(index_file, 'w', encoding='utf-8') as f:
            json.dump(index, f, ensure_ascii=False, indent=2)
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ Failed to update VARMAX predictions index: {e}")
        return False

def get_saved_varmax_predictions_list(limit=100):
    """
    ì €ì¥ëœ VARMAX ì˜ˆì¸¡ ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
    """
    try:
        file_path = prediction_state.get('current_file', None)
        if not file_path:
            logger.warning("No current file path for VARMAX predictions list")
            return []
            
        cache_dirs = get_file_cache_dirs(file_path)
        varmax_dir = cache_dirs['root'] / 'varmax'
        index_file = varmax_dir / 'varmax_index.json'
        
        if not index_file.exists():
            return []
            
        with open(index_file, 'r', encoding='utf-8') as f:
            index = json.load(f)
        
        predictions = index.get('predictions', [])[:limit]
        
        logger.info(f"âœ… Found {len(predictions)} saved VARMAX predictions")
        return predictions
        
    except Exception as e:
        logger.error(f"âŒ Failed to get saved VARMAX predictions list: {e}")
        return []

def delete_saved_varmax_prediction(prediction_date):
    """
    ì €ì¥ëœ VARMAX ì˜ˆì¸¡ì„ ì‚­ì œí•˜ëŠ” í•¨ìˆ˜
    """
    try:
        file_path = prediction_state.get('current_file', None)
        if not file_path:
            return False
            
        cache_dirs = get_file_cache_dirs(file_path)
        varmax_dir = cache_dirs['root'] / 'varmax'
        
        # ì˜ˆì¸¡ íŒŒì¼ ì‚­ì œ
        prediction_file = varmax_dir / f"varmax_prediction_{prediction_date}.json"
        if prediction_file.exists():
            prediction_file.unlink()
        
        # ì¸ë±ìŠ¤ì—ì„œ ì œê±°
        index_file = varmax_dir / 'varmax_index.json'
        if index_file.exists():
            with open(index_file, 'r', encoding='utf-8') as f:
                index = json.load(f)
            
            index['predictions'] = [p for p in index['predictions'] if p['prediction_date'] != prediction_date]
            
            with open(index_file, 'w', encoding='utf-8') as f:
                json.dump(index, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ… VARMAX prediction deleted: {prediction_date}")
        return True
        
    except Exception as e:
        logger.error(f"âŒ Failed to delete VARMAX prediction: {e}")
        return False

#######################################################################
# VARMAX ê´€ë ¨ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
#######################################################################

def varmax_decision(file_path):
    """Varmax ì˜ì‚¬ê²°ì • ê´€ë ¨"""
    fp = pd.read_csv(file_path)
    df = pd.DataFrame(fp, columns=fp.columns)
    col = df.columns
    # 1) ë¶„ì„ì— ì‚¬ìš©í•  ë³€ìˆ˜ ë¦¬ìŠ¤íŠ¸
    vars_pct = ['max_pct2', 'min_pct2', 'mean_pct2', 'max_pct3', 'min_pct3', 'mean_pct3']
    logger.info(f'ë°ì´í„°í”„ë ˆì„{df}')
    rename_dict = {
    'max_pct2': '[í˜„ ë°˜ì›” ìµœëŒ€ ì¦ê°€ìœ¨]',
    'min_pct2': '[í˜„ ë°˜ì›” ìµœëŒ€ ê°ì†Œìœ¨]',
    'mean_pct2': '[í˜„ ë°˜ì›” í‰ê·  ë³€ë™ë¥ ]',
    'max_pct3': '[ì´ì „ ë°˜ì›” ìµœëŒ€ ì¦ê°€ìœ¨]',
    'min_pct3': '[ì´ì „ ë°˜ì›” ìµœëŒ€ ê°ì†Œìœ¨]',
    'mean_pct3': '[ì´ì „ ë°˜ì›” í‰ê·  ë³€ë™ë¥ ]'
    }
    rename_col = list(rename_dict.values())
    df = df.rename(columns=rename_dict)
    logger.info(f'ì—´{col}')
    # 2) Case ì •ì˜
    case1 = df['saving_rate'] < 0
    abs_thresh = df['saving_rate'].abs().quantile(0.9)
    case2 = df['saving_rate'].abs() >= abs_thresh

    # 3) ìµœì  ì¡°ê±´ íƒìƒ‰ í•¨ìˆ˜
    def find_best_condition(df, case_mask, var):
        best = None
        for direction in ['greater', 'less']:
            for p in np.linspace(0.1, 0.9, 9):
                th = df[var].quantile(p)
                if direction == 'greater':
                    mask = df[var] > th
                else:
                    mask = df[var] < th
                # ìƒ˜í”Œ ìˆ˜ê°€ ë„ˆë¬´ ì ì€ ê²½ìš° ì œì™¸
                if mask.sum() < 5:
                    continue
                prop = case_mask[mask].mean()
                if best is None or prop > best[4]:
                    best = (direction, p, th, mask.sum(), prop)
        return best

    # 5) ê° ë³€ìˆ˜ë³„ ìµœì  ì¡°ê±´ ì°¾ê¸°
    results_case1 = {var: find_best_condition(df, case1, var) for var in rename_col}
    results_case2 = {var: find_best_condition(df, case2, var) for var in rename_col}

    from itertools import combinations
    # 6) ë‘ ë³€ìˆ˜ ì¡°í•©ì„ ì‚¬ìš©í•˜ì—¬ saving_rate < 0 ë¶„ë¥˜ ì„±ëŠ¥ í‰ê°€ (ìƒ˜í”Œ ìˆ˜ â‰¥ 30)
    combi_results_case1 = []

    for var1, var2 in combinations(rename_col, 2):
        for d1 in ['greater', 'less']:
            for d2 in ['greater', 'less']:
                for p1 in np.linspace(0.1, 0.9, 9):
                    for p2 in np.linspace(0.1, 0.9, 9):
                        th1 = df[var1].quantile(p1)
                        th2 = df[var2].quantile(p2)
                        mask1 = df[var1] > th1 if d1 == 'greater' else df[var1] < th1
                        mask2 = df[var2] > th2 if d2 == 'greater' else df[var2] < th2
                        mask = mask1 & mask2
                        n = mask.sum()
                        if n < 30:
                            continue
                        rate = case1[mask].mean()
                        combi_results_case1.append({
                            "ì¡°ê±´1": f"{var1} { '>' if d1 == 'greater' else '<' } {round(th1, 3)}%",
                            "ì¡°ê±´2": f"{var2} { '>' if d2 == 'greater' else '<' } {round(th2, 3)}%",
                            "ìƒ˜í”Œ ìˆ˜": n,
                            "ìŒìˆ˜ ë¹„ìœ¨ [%]": round(rate*100, 3)
                        })
    column_order1 = ["ì¡°ê±´1", "ì¡°ê±´2", "ìƒ˜í”Œ ìˆ˜", "ìŒìˆ˜ ë¹„ìœ¨ [%]"]
    combi_df_case1 = pd.DataFrame(combi_results_case1).sort_values(by="ìŒìˆ˜ ë¹„ìœ¨ [%]", ascending=False)
    combi_df_case1 = combi_df_case1.reindex(columns=column_order1)

    # 7) ë‘ ë³€ìˆ˜ ì¡°í•©ì„ ì‚¬ìš©í•˜ì—¬ ì ˆëŒ“ê°’ ìƒìœ„ 10% ë¶„ë¥˜ ì„±ëŠ¥ í‰ê°€
    combi_results_case2 = []

    for var1, var2 in combinations(rename_col, 2):
        for d1 in ['greater', 'less']:
            for d2 in ['greater', 'less']:
                for p1 in np.linspace(0.1, 0.9, 9):
                    for p2 in np.linspace(0.1, 0.9, 9):
                        th1 = df[var1].quantile(p1)
                        th2 = df[var2].quantile(p2)
                        mask1 = df[var1] > th1 if d1 == 'greater' else df[var1] < th1
                        mask2 = df[var2] > th2 if d2 == 'greater' else df[var2] < th2
                        mask = mask1 & mask2
                        n = mask.sum()
                        if n < 30:
                            continue
                        rate = case2[mask].mean()
                        combi_results_case2.append({
                            "ì¡°ê±´1": f"{var1} { '>' if d1 == 'greater' else '<' } {round(th1, 3)}%",
                            "ì¡°ê±´2": f"{var2} { '>' if d2 == 'greater' else '<' } {round(th2, 3)}%",
                            "ìƒ˜í”Œ ìˆ˜": n,
                            "ìƒìœ„ ë³€ë™ì„± í™•ë¥  [%]": round(rate*100, 3)
                        })
    column_order2 = ["ì¡°ê±´1", "ì¡°ê±´2", "ìƒ˜í”Œ ìˆ˜", "ìƒìœ„ ë³€ë™ì„± í™•ë¥  [%]"]
    combi_df_case2 = pd.DataFrame(combi_results_case2).sort_values(by="ìƒìœ„ ë³€ë™ì„± í™•ë¥  [%]", ascending=False)
    combi_df_case2 = combi_df_case2.reindex(columns=column_order2)
    return {
        'case_1': combi_df_case1.to_dict(orient='records'),
        'case_2': combi_df_case2.to_dict(orient='records')
    }

def background_varmax_prediction(file_path, current_date, pred_days, use_cache=True):
    """ë°±ê·¸ë¼ìš´ë“œì—ì„œ VARMAX ì˜ˆì¸¡ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜"""
    global prediction_state
    
    try:
        # ì¼ê´€ëœ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ìœ„í•œ ì‹œë“œ ê³ ì •
        set_seed()
        # í˜„ì¬ íŒŒì¼ ìƒíƒœ ì—…ë°ì´íŠ¸
        prediction_state['current_file'] = file_path
        
        # ğŸ” ê¸°ì¡´ ì €ì¥ëœ ì˜ˆì¸¡ í™•ì¸ (use_cache=Trueì¸ ê²½ìš°)
        if use_cache:
            logger.info(f"ğŸ” [VARMAX_CACHE] Checking for existing prediction for date: {current_date}")
            existing_prediction = load_varmax_prediction(current_date)
            
            if existing_prediction:
                logger.info(f"âœ… [VARMAX_CACHE] Found existing VARMAX prediction for {current_date}")
                logger.info(f"ğŸ” [VARMAX_CACHE] Cached data keys: {list(existing_prediction.keys())}")
                logger.info(f"ğŸ” [VARMAX_CACHE] MA results available: {bool(existing_prediction.get('ma_results'))}")
                ma_results = existing_prediction.get('ma_results')
                if ma_results:
                    logger.info(f"ğŸ” [VARMAX_CACHE] MA results type: {type(ma_results)}")
                    if isinstance(ma_results, dict):
                        logger.info(f"ğŸ” [VARMAX_CACHE] MA results keys: {list(ma_results.keys())}")
                    else:
                        logger.warning(f"âš ï¸ [VARMAX_CACHE] MA results is not a dict: {type(ma_results)}")
                
                # ğŸ”‘ ìƒíƒœ ë³µì› (ìˆœì°¨ì ìœ¼ë¡œ)
                logger.info(f"ğŸ”„ [VARMAX_CACHE] Restoring state from cached prediction...")
                
                # ê¸°ì¡´ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ìƒíƒœì— ë¡œë“œ (ì•ˆì „í•œ íƒ€ì… ê²€ì‚¬)
                prediction_state['varmax_predictions'] = existing_prediction.get('predictions', [])
                prediction_state['varmax_half_month_averages'] = existing_prediction.get('half_month_averages', [])
                prediction_state['varmax_metrics'] = existing_prediction.get('metrics', {})
                
                # MA results ì•ˆì „í•œ ë¡œë“œ
                ma_results = existing_prediction.get('ma_results', {})
                if isinstance(ma_results, dict):
                    prediction_state['varmax_ma_results'] = ma_results
                else:
                    logger.warning(f"âš ï¸ [VARMAX_CACHE] Invalid ma_results type: {type(ma_results)}, setting empty dict")
                    prediction_state['varmax_ma_results'] = {}
                
                prediction_state['varmax_selected_features'] = existing_prediction.get('selected_features', [])
                prediction_state['varmax_current_date'] = existing_prediction.get('current_date', current_date)
                prediction_state['varmax_model_info'] = existing_prediction.get('model_info', {})
                prediction_state['varmax_plots'] = existing_prediction.get('plots', {})
                
                # ì¦‰ì‹œ ì™„ë£Œ ìƒíƒœë¡œ ì„¤ì •
                prediction_state['varmax_is_predicting'] = False
                prediction_state['varmax_prediction_progress'] = 100
                prediction_state['varmax_error'] = None
                
                logger.info(f"âœ… [VARMAX_CACHE] State restoration completed")
                
                logger.info(f"âœ… [VARMAX_CACHE] Successfully loaded existing prediction for {current_date}")
                logger.info(f"ğŸ” [VARMAX_CACHE] State restored - predictions: {len(prediction_state['varmax_predictions'])}, MA results: {len(prediction_state['varmax_ma_results'])}")
                
                # ğŸ” ìµœì¢… ê²€ì¦
                logger.info(f"ğŸ” [VARMAX_CACHE] Final verification - is_predicting: {prediction_state.get('varmax_is_predicting')}")
                logger.info(f"ğŸ” [VARMAX_CACHE] Final verification - predictions count: {len(prediction_state.get('varmax_predictions', []))}")
                logger.info(f"ğŸ” [VARMAX_CACHE] Final verification - ma_results count: {len(prediction_state.get('varmax_ma_results', {}))}")
                
                # ğŸ›¡ï¸ ìƒíƒœ ì•ˆì •í™”ë¥¼ ìœ„í•œ ì§§ì€ ëŒ€ê¸°
                import time
                time.sleep(1.0)
                
                logger.info(f"ğŸ¯ [VARMAX_CACHE] Cache loading process completed for {current_date}")
                return
        
        # ğŸš€ ìƒˆë¡œìš´ ì˜ˆì¸¡ ìˆ˜í–‰
        logger.info(f"ğŸš€ [VARMAX_NEW] Starting new VARMAX prediction for {current_date}")
        forecaster = VARMAXSemiMonthlyForecaster(file_path, pred_days=pred_days)
        prediction_state['varmax_is_predicting'] = True
        prediction_state['varmax_prediction_progress'] = 10
        prediction_state['varmax_prediction_start_time'] = time.time()  # VARMAX ì‹œì‘ ì‹œê°„ ê¸°ë¡
        prediction_state['varmax_error'] = None
        
        # VARMAX ì˜ˆì¸¡ ìˆ˜í–‰
        prediction_state['varmax_prediction_progress'] = 30
        logger.info(f"ğŸ”„ [VARMAX_NEW] Starting prediction generation (30% complete)")
        
        try:
            min_index = 1 # ì„ì‹œ ì¸ë±ìŠ¤
            logger.info(f"ğŸ”„ [VARMAX_NEW] Calling generate_predictions_varmax with current_date={current_date}, var_num={min_index+2}")
            
            # ì˜ˆì¸¡ ì§„í–‰ë¥ ì„ 30%ë¡œ ì„¤ì • (ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ)
            prediction_state['varmax_prediction_progress'] = 30

            mape_list=[]
            for var_num in range(2,8):
                mape_value = forecaster.generate_variables_varmax(current_date, var_num)
                mape_list.append(mape_value)
            min_index = mape_list.index(min(mape_list))
            logger.info(f"Var {min_index+2} model is selected, MAPE:{mape_list[min_index]}%")
            
            results = forecaster.generate_predictions_varmax(current_date, min_index+2)
            logger.info(f"âœ… [VARMAX_NEW] Prediction generation completed successfully")
            
            # ìµœì¢… ì§„í–‰ë¥  95%ë¡œ ì„¤ì • (ì‹œê°í™” ìƒì„± ì „)
            prediction_state['varmax_prediction_progress'] = 95
            
        except Exception as prediction_error:
            logger.error(f"âŒ [VARMAX_NEW] Error during prediction generation: {str(prediction_error)}")
            logger.error(f"âŒ [VARMAX_NEW] Prediction error traceback: {traceback.format_exc()}")
            
            # ì˜ˆì¸¡ ì‹¤íŒ¨ ìƒíƒœë¡œ ì„¤ì •
            prediction_state['varmax_error'] = f"Prediction generation failed: {str(prediction_error)}"
            prediction_state['varmax_is_predicting'] = False
            prediction_state['varmax_prediction_progress'] = 0
            logger.error(f"âŒ [VARMAX_NEW] Prediction state reset due to error")
            return
        
        if results['success']:
            logger.info(f"ğŸ”„ [VARMAX_NEW] Updating state with new prediction results...")
            
            # ìƒíƒœì— ê²°ê³¼ ì €ì¥ (ê¸°ì¡´ LSTM ê²°ê³¼ì™€ ë¶„ë¦¬)
            prediction_state['varmax_predictions'] = results['predictions']
            prediction_state['varmax_half_month_averages'] = results.get('half_month_averages', [])
            prediction_state['varmax_metrics'] = results['metrics']
            prediction_state['varmax_ma_results'] = results['ma_results']
            prediction_state['varmax_selected_features'] = results['selected_features']
            prediction_state['varmax_current_date'] = results['current_date']
            prediction_state['varmax_model_info'] = results['model_info']
            
            # ì‹œê°í™” ìƒì„± (ê¸°ì¡´ app.py ë°©ì‹ í™œìš©)
            plots_info = create_varmax_visualizations(results)
            prediction_state['varmax_plots'] = plots_info
            
            prediction_state['varmax_prediction_progress'] = 100
            prediction_state['varmax_is_predicting'] = False
            prediction_state['varmax_error'] = None
            
            # VARMAX ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
            save_varmax_prediction(results, current_date)
            
            logger.info("âœ… [VARMAX_NEW] Prediction completed successfully")
            logger.info(f"ğŸ” [VARMAX_NEW] Final state - predictions: {len(prediction_state['varmax_predictions'])}")
        else:
            prediction_state['varmax_error'] = results['error']
            prediction_state['varmax_is_predicting'] = False
            prediction_state['varmax_prediction_progress'] = 0
            
    except Exception as e:
        logger.error(f"âŒ [VARMAX_BG] Error in background VARMAX prediction: {str(e)}")
        logger.error(f"âŒ [VARMAX_BG] Full traceback: {traceback.format_exc()}")
        
        # ì—ëŸ¬ ìƒíƒœë¡œ ì„¤ì •í•˜ê³  ìì„¸í•œ ë¡œê¹…
        prediction_state['varmax_error'] = f"Background prediction failed: {str(e)}"
        prediction_state['varmax_is_predicting'] = False
        prediction_state['varmax_prediction_progress'] = 0
        
        logger.error(f"âŒ [VARMAX_BG] VARMAX prediction failed completely. Current state reset.")
        logger.error(f"âŒ [VARMAX_BG] Error type: {type(e).__name__}")
        logger.error(f"âŒ [VARMAX_BG] Error details: {str(e)}")
        
        # ì—ëŸ¬ ë°œìƒ ì‹œ ëª¨ë“  VARMAX ê´€ë ¨ ìƒíƒœ ì´ˆê¸°í™”
        prediction_state['varmax_predictions'] = []
        prediction_state['varmax_metrics'] = {}
        prediction_state['varmax_ma_results'] = {}
        prediction_state['varmax_selected_features'] = []
        prediction_state['varmax_current_date'] = None
        prediction_state['varmax_model_info'] = {}
        prediction_state['varmax_plots'] = {}
        prediction_state['varmax_half_month_averages'] = []

def plot_varmax_prediction_basic(sequence_df, sequence_start_date, start_day_value, 
                                f1, accuracy, mape, weighted_score, 
                                save_prefix=None, title_prefix="VARMAX Semi-monthly Prediction", file_path=None):
    """VARMAX ê¸°ë³¸ ì˜ˆì¸¡ ê·¸ë˜í”„ ì‹œê°í™” (ê¸°ì¡´ plot_prediction_basicê³¼ ë™ì¼í•œ ìŠ¤íƒ€ì¼)"""
    try:
        logger.info(f"Creating VARMAX prediction graph for {sequence_start_date}")
        
        # íŒŒì¼ë³„ ìºì‹œ ë””ë ‰í† ë¦¬ ê°€ì ¸ì˜¤ê¸°
        if save_prefix is None:
            cache_dirs = get_file_cache_dirs(file_path)
            save_prefix = cache_dirs['plots']
        
        # ì˜ˆì¸¡ê°’ë§Œ ìˆëŠ” ë°ì´í„° ì²˜ë¦¬
        pred_df = sequence_df.dropna(subset=['Prediction'])
        valid_df = sequence_df.dropna(subset=['Actual']) if 'Actual' in sequence_df.columns else pd.DataFrame()
        
        # ê·¸ë˜í”„ ìƒì„±
        fig = plt.figure(figsize=(16, 9))
        plt.subplots_adjust(top=0.85)
        gs = GridSpec(2, 1, height_ratios=[3, 1], hspace=0.3, figure=fig)
        
        # ì œëª© ì„¤ì •
        main_title = f"{title_prefix} - {sequence_start_date}"
        subtitle = f"F1: {f1:.2f}, Acc: {accuracy:.2f}%, MAPE: {mape:.2f}%, Score: {weighted_score:.2f}%"
        
        fig.text(0.5, 0.94, main_title, ha='center', fontsize=14, weight='bold')
        fig.text(0.5, 0.90, subtitle, ha='center', fontsize=12)
        
        # ìƒë‹¨: ì˜ˆì¸¡ vs ì‹¤ì œ (ìˆëŠ” ê²½ìš°)
        ax1 = fig.add_subplot(gs[0])
        ax1.set_title("VARMAX Long-term Prediction")
        ax1.grid(True, linestyle='--', alpha=0.5)
        
        # ì˜ˆì¸¡ê°’ í”Œë¡¯
        ax1.plot(pred_df['Date'], pred_df['Prediction'],
                marker='o', color='red', label='VARMAX Predicted', linewidth=2)
        
        # ì‹¤ì œê°’ í”Œë¡¯ (ìˆëŠ” ê²½ìš°)
        if len(valid_df) > 0:
            ax1.plot(valid_df['Date'], valid_df['Actual'],
                    marker='o', color='blue', label='Actual', linewidth=2)
            
            # ë°©í–¥ì„± ì¼ì¹˜ ì—¬ë¶€ ë°°ê²½ ìƒ‰ì¹ 
            for i in range(1, len(valid_df)):
                if i < len(pred_df):
                    actual_dir = np.sign(valid_df['Actual'].iloc[i] - valid_df['Actual'].iloc[i-1])
                    pred_dir = np.sign(pred_df['Prediction'].iloc[i] - pred_df['Prediction'].iloc[i-1])
                    color = 'blue' if actual_dir == pred_dir else 'red'
                    ax1.axvspan(valid_df['Date'].iloc[i-1], valid_df['Date'].iloc[i], alpha=0.1, color=color)
        
        ax1.set_xlabel("Date")
        ax1.set_ylabel("MOPJ Price")
        ax1.legend()
        ax1.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d'))
        
        # í•˜ë‹¨: ì˜¤ì°¨ (ì‹¤ì œê°’ì´ ìˆëŠ” ê²½ìš°ë§Œ)
        ax2 = fig.add_subplot(gs[1])
        ax2.grid(True, linestyle='--', alpha=0.5)
        
        if len(valid_df) > 0:
            # ì˜¤ì°¨ ê³„ì‚° ë° í”Œë¡¯
            errors = valid_df['Actual'] - valid_df['Prediction']
            ax2.bar(valid_df['Date'], errors, alpha=0.7, color='orange', width=0.8)
            ax2.axhline(y=0, color='black', linestyle='-', alpha=0.8)
            ax2.set_title(f"Prediction Error (MAE: {abs(errors).mean():.2f})")
        else:
            ax2.text(0.5, 0.5, 'No actual data for error calculation', 
                    ha='center', va='center', transform=ax2.transAxes, fontsize=12)
            ax2.set_title("Prediction Error (No validation data)")
        
        ax2.set_xlabel("Date")
        ax2.set_ylabel("Error")
        ax2.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d'))
        
        # íŒŒì¼ ì €ì¥
        os.makedirs(save_prefix, exist_ok=True)
        filename = f"varmax_prediction_{sequence_start_date}.png"
        filepath = os.path.join(save_prefix, filename)
        plt.savefig(filepath, dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info(f"VARMAX prediction graph saved: {filepath}")
        return filepath
        
    except Exception as e:
        logger.error(f"Error creating VARMAX prediction graph: {str(e)}")
        logger.error(traceback.format_exc())
        return None

def create_varmax_visualizations(results):
    """VARMAX ê²°ê³¼ì— ëŒ€í•œ ì‹œê°í™” ìƒì„±"""
    try:
        # ê¸°ë³¸ ì˜ˆì¸¡ ê·¸ë˜í”„
        sequence_df = pd.DataFrame(results['predictions'])
        sequence_df['Date'] = pd.to_datetime(sequence_df['Date'])
        
        metrics = results['metrics']
        current_date = results['current_date']
        start_day_value = sequence_df['Prediction'].iloc[0] if len(sequence_df) > 0 else 0
        
        # ê¸°ë³¸ ê·¸ë˜í”„
        basic_plot = plot_varmax_prediction_basic(
            sequence_df, current_date, start_day_value,
            metrics['f1'], metrics['accuracy'], metrics['mape'], metrics['weighted_score']
        )
        
        # ì´ë™í‰ê·  ê·¸ë˜í”„
        ma_plot = plot_varmax_moving_average_analysis(
            results['ma_results'], current_date
        )
        
        plots_info = {
            'basic_plot': basic_plot,
            'ma_plot': ma_plot
        }
        
        logger.info("VARMAX visualizations created successfully")
        return plots_info
        
    except Exception as e:
        logger.error(f"Error creating VARMAX visualizations: {str(e)}")
        logger.error(traceback.format_exc())
        return {}

def plot_varmax_moving_average_analysis(ma_results, sequence_start_date, save_prefix=None,
                                        title_prefix="VARMAX Moving Average Analysis", file_path=None):
    """VARMAX ì´ë™í‰ê·  ë¶„ì„ ê·¸ë˜í”„"""
    try:
        logger.info(f"Creating VARMAX moving average analysis for {sequence_start_date}")
        
        # íŒŒì¼ë³„ ìºì‹œ ë””ë ‰í† ë¦¬ ê°€ì ¸ì˜¤ê¸°
        if save_prefix is None:
            cache_dirs = get_file_cache_dirs(file_path)
            save_prefix = cache_dirs['ma_plots']
        
        if not ma_results:
            logger.warning("No moving average results to plot")
            return None
        
        windows = list(ma_results.keys())
        n_windows = len(windows)
        
        if n_windows == 0:
            logger.warning("No moving average windows found")
            return None
        
        # ê·¸ë˜í”„ ìƒì„± (2x2 ê·¸ë¦¬ë“œë¡œ ìµœëŒ€ 4ê°œ ìœˆë„ìš° í‘œì‹œ)
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle(f"{title_prefix} - {sequence_start_date}", fontsize=16, weight='bold')
        axes = axes.flatten()
        
        for i, window in enumerate(windows[:4]):  # ìµœëŒ€ 4ê°œê¹Œì§€ë§Œ
            ax = axes[i]
            ma_data = ma_results[window]
            
            if not ma_data:
                ax.text(0.5, 0.5, f'No data for {window}', ha='center', va='center', transform=ax.transAxes)
                ax.set_title(f"{window} (No Data)")
                continue
            
            # ë°ì´í„°í”„ë ˆì„ ë³€í™˜
            df = pd.DataFrame(ma_data)
            df['date'] = pd.to_datetime(df['date'])
            df = df.sort_values('date')
            
            # ì˜ˆì¸¡ê°’ê³¼ ì´ë™í‰ê·  í”Œë¡¯
            ax.plot(df['date'], df['prediction'], marker='o', color='red', 
                   label='Prediction', linewidth=2, markersize=4)
            ax.plot(df['date'], df['ma'], marker='s', color='blue', 
                   label=f'MA-{window.replace("ma", "")}', linewidth=2, markersize=4)
            
            # ì‹¤ì œê°’ í”Œë¡¯ (ìˆëŠ” ê²½ìš°)
            actual_data = df.dropna(subset=['actual'])
            if len(actual_data) > 0:
                ax.plot(actual_data['date'], actual_data['actual'], 
                       marker='^', color='green', label='Actual', linewidth=2, markersize=4)
            
            ax.set_title(f"{window.upper()} Moving Average")
            ax.grid(True, linestyle='--', alpha=0.5)
            ax.legend()
            ax.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d'))
            plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)
        
        # ë¹ˆ subplot ìˆ¨ê¸°ê¸°
        for i in range(n_windows, 4):
            axes[i].set_visible(False)
        
        plt.tight_layout()
        
        # íŒŒì¼ ì €ì¥
        os.makedirs(save_prefix, exist_ok=True)
        filename = f"varmax_ma_analysis_{sequence_start_date}.png"
        filepath = os.path.join(save_prefix, filename)
        plt.savefig(filepath, dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info(f"VARMAX moving average analysis saved: {filepath}")
        return filepath
        
    except Exception as e:
        logger.error(f"Error creating VARMAX moving average analysis: {str(e)}")
        logger.error(traceback.format_exc())
        return None

#######################################################################
# VARMAX API ì—”ë“œí¬ì¸íŠ¸
#######################################################################

# 1) VARMAX ë°˜ì›”ë³„ ì˜ˆì¸¡ ì‹œì‘
@app.route('/api/varmax/predict', methods=['POST', 'OPTIONS'])
def varmax_semimonthly_predict():
    """VARMAX ë°˜ì›”ë³„ ì˜ˆì¸¡ ì‹œì‘ API"""
    # 1) ë¨¼ì €, OPTIONS(preflight) ìš”ì²­ì´ ë“¤ì–´ì˜¤ë©´ ë°”ë¡œ 200ì„ ë¦¬í„´
    if request.method == 'OPTIONS':
        # CORS(app) ë¡œ ì„¤ì •í•´ë’€ìœ¼ë©´ ì´ë¯¸ Access-Control-Allow-Origin ë“±ì´ ë¶™ì–´ ìˆì„ ê²ƒì…ë‹ˆë‹¤.
        return make_response(('', 200))
    global prediction_state
    
    # ğŸ”§ VARMAX ë…ë¦½ ìƒíƒœ í™•ì¸ - hangëœ ìƒíƒœë©´ ìë™ ë¦¬ì…‹
    if prediction_state.get('varmax_is_predicting', False):
        current_progress = prediction_state.get('varmax_prediction_progress', 0)
        current_error = prediction_state.get('varmax_error')
        
        logger.warning(f"âš ï¸ [VARMAX_API] Prediction already in progress (progress: {current_progress}%, error: {current_error})")
        
        # ğŸ”§ ê°œì„ ëœ ìë™ ë¦¬ì…‹ ì¡°ê±´: ì—ëŸ¬ê°€ ìˆê±°ë‚˜ ì§„í–‰ë¥ ì´ ë§¤ìš° ë‚®ì€ ê²½ìš°ë§Œ ë¦¬ì…‹
        should_reset = False
        reset_reason = ""
        
        if current_error:
            should_reset = True
            reset_reason = f"error detected: {current_error}"
        elif current_progress > 0 and current_progress < 15:
            should_reset = True  
            reset_reason = f"very low progress stuck: {current_progress}%"
        
        if should_reset:
            logger.warning(f"ğŸ”„ [VARMAX_API] Auto-resetting stuck prediction - {reset_reason}")
            
            # ìƒíƒœ ë¦¬ì…‹
            prediction_state['varmax_is_predicting'] = False
            prediction_state['varmax_prediction_progress'] = 0
            prediction_state['varmax_error'] = None
            prediction_state['varmax_predictions'] = []
            prediction_state['varmax_half_month_averages'] = []
            prediction_state['varmax_metrics'] = {}
            prediction_state['varmax_ma_results'] = {}
            prediction_state['varmax_selected_features'] = []
            prediction_state['varmax_current_date'] = None
            prediction_state['varmax_model_info'] = {}
            prediction_state['varmax_plots'] = {}
            
            logger.info(f"âœ… [VARMAX_API] Stuck state auto-reset completed, proceeding with new prediction")
        else:
            # ì •ìƒì ìœ¼ë¡œ ì§„í–‰ ì¤‘ì¸ ê²½ìš° 409 ë°˜í™˜
            return jsonify({
                'success': False,
                'error': 'VARMAX prediction already in progress',
                'progress': current_progress
            }), 409
    
    data = request.get_json(force=True)
    filepath     = data.get('filepath')
    current_date = data.get('date')
    pred_days    = data.get('pred_days', 50)
    use_cache    = data.get('use_cache', True)  # ğŸ†• ìºì‹œ ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
    
    if not filepath or not os.path.exists(filepath):
        return jsonify({'error': 'Invalid file path'}), 400
    if not current_date:
        return jsonify({'error': 'Date is required'}), 400
    
    logger.info(f"ğŸš€ [VARMAX_API] Starting VARMAX prediction (use_cache={use_cache}) for {current_date}")
    
    thread = Thread(
        target=background_varmax_prediction,
        args=(filepath, current_date, pred_days, use_cache)
    )
    thread.daemon = True
    thread.start()
    
    return jsonify({
        'success': True,
        'message': 'VARMAX semi-monthly prediction started',
        'status_url': '/api/varmax/status',
        'use_cache': use_cache
    })

# 2) VARMAX ì˜ˆì¸¡ ìƒíƒœ ì¡°íšŒ
@app.route('/api/varmax/status', methods=['GET'])
def varmax_prediction_status():
    """VARMAX ì˜ˆì¸¡ ìƒíƒœ í™•ì¸ API (ë‚¨ì€ ì‹œê°„ ì¶”ê°€)"""
    global prediction_state
    
    is_predicting = prediction_state.get('varmax_is_predicting', False)
    progress = prediction_state.get('varmax_prediction_progress', 0)
    error = prediction_state.get('varmax_error', None)
    
    logger.info(f"ğŸ” [VARMAX_STATUS] Current status - predicting: {is_predicting}, progress: {progress}%, error: {error}")
    
    status = {
        'is_predicting': is_predicting,
        'progress': progress,
        'error': error
    }
    
    # VARMAX ì˜ˆì¸¡ ì¤‘ì¸ ê²½ìš° ë‚¨ì€ ì‹œê°„ ê³„ì‚°
    if is_predicting and prediction_state.get('varmax_prediction_start_time'):
        time_info = calculate_estimated_time_remaining(
            prediction_state['varmax_prediction_start_time'], 
            progress
        )
        status.update(time_info)
    
    if not is_predicting and prediction_state.get('varmax_current_date'):
        status['current_date'] = prediction_state['varmax_current_date']
        logger.info(f"ğŸ” [VARMAX_STATUS] Prediction completed for date: {status['current_date']}")
    
    return jsonify(status)

# 3) VARMAX ì „ì²´ ê²°ê³¼ ì¡°íšŒ
@app.route('/api/varmax/results', methods=['GET'])
def get_varmax_results():
    """VARMAX ì˜ˆì¸¡ ê²°ê³¼ ì¡°íšŒ API"""
    global prediction_state
    
    # ğŸ” ìƒíƒœ ë””ë²„ê¹…
    logger.info(f"ğŸ” [VARMAX_API] Current prediction_state keys: {list(prediction_state.keys())}")
    logger.info(f"ğŸ” [VARMAX_API] varmax_is_predicting: {prediction_state.get('varmax_is_predicting', 'NOT_SET')}")
    logger.info(f"ğŸ” [VARMAX_API] varmax_predictions available: {bool(prediction_state.get('varmax_predictions'))}")
    logger.info(f"ğŸ” [VARMAX_API] varmax_ma_results available: {bool(prediction_state.get('varmax_ma_results'))}")
    
    if prediction_state.get('varmax_predictions'):
        logger.info(f"ğŸ” [VARMAX_API] Predictions count: {len(prediction_state['varmax_predictions'])}")
    
    if prediction_state.get('varmax_ma_results'):
        logger.info(f"ğŸ” [VARMAX_API] MA results keys: {list(prediction_state['varmax_ma_results'].keys())}")
    
    # ğŸ›¡ï¸ ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œ ì™„ë£Œ ëŒ€ê¸°
    if prediction_state.get('varmax_is_predicting', False):
        logger.warning(f"âš ï¸ [VARMAX_API] Prediction still in progress: {prediction_state.get('varmax_prediction_progress', 0)}%")
        return jsonify({
            'success': False,
            'error': 'VARMAX prediction in progress',
            'progress': prediction_state.get('varmax_prediction_progress', 0)
        }), 409
    
    # ğŸ¯ ìƒíƒœì— ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ìºì‹œì—ì„œ ì§ì ‘ ë¡œë“œ (ì‹ ë¢°ì„± ê°œì„ )
    if not prediction_state.get('varmax_predictions'):
        logger.warning(f"âš ï¸ [VARMAX_API] No VARMAX predictions in state, attempting direct cache load")
        logger.info(f"ğŸ” [VARMAX_API] Current file: {prediction_state.get('current_file')}")
        
        try:
            # ìµœê·¼ ì €ì¥ëœ VARMAX ì˜ˆì¸¡ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
            saved_predictions = get_saved_varmax_predictions_list(limit=1)
            logger.info(f"ğŸ” [VARMAX_API] Found {len(saved_predictions)} saved predictions")
            
            if saved_predictions:
                latest_date = saved_predictions[0]['prediction_date']
                logger.info(f"ğŸ”§ [VARMAX_API] Loading latest prediction: {latest_date}")
                
                # ì§ì ‘ ë¡œë“œí•˜ê³  ìƒíƒœ ë³µì›
                cached_prediction = load_varmax_prediction(latest_date)
                if cached_prediction and cached_prediction.get('predictions'):
                    logger.info(f"âœ… [VARMAX_API] Successfully loaded from cache ({len(cached_prediction.get('predictions', []))} predictions)")
                    
                    # ğŸ”‘ ì¦‰ì‹œ ìƒíƒœ ë³µì› (ë” ì•ˆì „í•˜ê²Œ)
                    prediction_state['varmax_predictions'] = cached_prediction.get('predictions', [])
                    prediction_state['varmax_half_month_averages'] = cached_prediction.get('half_month_averages', [])
                    prediction_state['varmax_metrics'] = cached_prediction.get('metrics', {})
                    prediction_state['varmax_ma_results'] = cached_prediction.get('ma_results', {})
                    prediction_state['varmax_selected_features'] = cached_prediction.get('selected_features', [])
                    prediction_state['varmax_current_date'] = cached_prediction.get('current_date')
                    prediction_state['varmax_model_info'] = cached_prediction.get('model_info', {})
                    prediction_state['varmax_plots'] = cached_prediction.get('plots', {})
                    
                    logger.info(f"ğŸ¯ [VARMAX_API] State restored from cache - {len(prediction_state['varmax_predictions'])} predictions")
                    
                    return jsonify({
                        'success': True,
                        'current_date': cached_prediction.get('current_date'),
                        'predictions': cached_prediction.get('predictions', []),
                        'half_month_averages': cached_prediction.get('half_month_averages', []),
                        'metrics': cached_prediction.get('metrics', {}),
                        'ma_results': cached_prediction.get('ma_results', {}),
                        'selected_features': cached_prediction.get('selected_features', []),
                        'model_info': cached_prediction.get('model_info', {}),
                        'plots': cached_prediction.get('plots', {})
                    })
                else:
                    logger.warning(f"âš ï¸ [VARMAX_API] Cached prediction is empty or invalid")
            else:
                logger.warning(f"âš ï¸ [VARMAX_API] No saved predictions found")
                
        except Exception as e:
            logger.error(f"âŒ [VARMAX_API] Direct cache load failed: {e}")
            import traceback
            logger.error(f"âŒ [VARMAX_API] Cache load traceback: {traceback.format_exc()}")
        
        # ìºì‹œ ë¡œë“œë„ ì‹¤íŒ¨í•œ ê²½ìš° ëª…í™•í•œ ë©”ì‹œì§€
        logger.error(f"âŒ [VARMAX_API] No VARMAX results available in state or cache")
        return jsonify({
            'success': False,
            'error': 'No VARMAX prediction results available. Please run a new prediction.'
        }), 404
    
    logger.info(f"âœ… [VARMAX_API] Returning VARMAX results successfully from state")
    return jsonify({
        'success': True,
        'current_date':      prediction_state.get('varmax_current_date'),
        'predictions':       prediction_state.get('varmax_predictions', []),
        'half_month_averages': prediction_state.get('varmax_half_month_averages', []),
        'metrics':           prediction_state.get('varmax_metrics', {}),
        'ma_results':        prediction_state.get('varmax_ma_results', {}),
        'selected_features': prediction_state.get('varmax_selected_features', []),
        'model_info':        prediction_state.get('varmax_model_info', {}),
        'plots':             prediction_state.get('varmax_plots', {})
    })

# 4) VARMAX ì˜ˆì¸¡ê°’ë§Œ ì¡°íšŒ
@app.route('/api/varmax/predictions', methods=['GET'])
def get_varmax_predictions_only():
    """VARMAX ì˜ˆì¸¡ ê°’ë§Œ ì¡°íšŒ API"""
    global prediction_state
    
    if not prediction_state.get('varmax_predictions'):
        return jsonify({'error': 'No VARMAX prediction results available'}), 404
    
    return jsonify({
        'success': True,
        'current_date':      prediction_state['varmax_current_date'],
        'predictions':       prediction_state['varmax_predictions'],
        'model_info':        prediction_state['varmax_model_info']
    })

# 5) VARMAX ì´ë™í‰ê·  ì¡°íšŒ - ì¦‰ì„ ê³„ì‚° ë°©ì‹
@app.route('/api/varmax/moving-averages', methods=['GET'])
def get_varmax_moving_averages():
    """VARMAX ì´ë™í‰ê·  ì¡°íšŒ API - ì˜ˆì¸¡ ê²°ê³¼ë¡œ ì¦‰ì„ ê³„ì‚°"""
    global prediction_state
    
    # ğŸ¯ ìƒíƒœì— MA ë°ì´í„°ê°€ ìˆìœ¼ë©´ ë°”ë¡œ ë°˜í™˜
    if prediction_state.get('varmax_ma_results'):
        return jsonify({
            'success': True,
            'current_date': prediction_state['varmax_current_date'],
            'ma_results': prediction_state['varmax_ma_results']
        })
    
    # ğŸš€ ì˜ˆì¸¡ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ì¦‰ì„ì—ì„œ MA ê³„ì‚°
    varmax_predictions = prediction_state.get('varmax_predictions')
    current_date = prediction_state.get('varmax_current_date')
    current_file = prediction_state.get('current_file')
    
    # ìƒíƒœì— ì˜ˆì¸¡ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ìºì‹œì—ì„œ ë¡œë“œ
    if not varmax_predictions or not current_date:
        logger.info(f"ğŸ”§ [VARMAX_MA_API] No predictions in state, loading from cache")
        try:
            saved_predictions = get_saved_varmax_predictions_list(limit=1)
            if saved_predictions:
                latest_date = saved_predictions[0]['prediction_date']
                cached_prediction = load_varmax_prediction(latest_date)
                if cached_prediction and cached_prediction.get('predictions'):
                    varmax_predictions = cached_prediction.get('predictions')
                    current_date = cached_prediction.get('current_date', latest_date)
                    # current_fileì€ prediction_stateì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ ì¶”ì •
                    if not current_file:
                        current_file = prediction_state.get('current_file')
                    logger.info(f"âœ… [VARMAX_MA_API] Loaded predictions from cache: {len(varmax_predictions)} items")
        except Exception as e:
            logger.error(f"âŒ [VARMAX_MA_API] Failed to load from cache: {e}")
    
    # ì˜ˆì¸¡ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì—ëŸ¬
    if not varmax_predictions or not current_date:
        return jsonify({
            'success': False,
            'error': 'No VARMAX predictions available for MA calculation'
        }), 404
    
    # ğŸ¯ ì¦‰ì„ì—ì„œ MA ê³„ì‚°
    try:
        logger.info(f"ğŸ”„ [VARMAX_MA_API] Calculating MA on-the-fly for {len(varmax_predictions)} predictions")
        
        # VARMAX í´ë˜ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (MA ê³„ì‚°ìš©)
        if not current_file or not os.path.exists(current_file):
            logger.error(f"âŒ [VARMAX_MA_API] File not found: {current_file}")
            return jsonify({
                'success': False,
                'error': 'Original data file not available for MA calculation'
            }), 404
            
        forecaster = VARMAXSemiMonthlyForecaster(current_file, pred_days=50)
        forecaster.load_data()  # ê³¼ê±° ë°ì´í„° ë¡œë“œ
        
        # MA ê³„ì‚°
        ma_results = forecaster.calculate_moving_averages_varmax(
            varmax_predictions, 
            current_date, 
            windows=[5, 10, 20, 30]
        )
        
        logger.info(f"âœ… [VARMAX_MA_API] MA calculation completed: {len(ma_results)} windows")
        
        # ìƒíƒœì— ì €ì¥ (ë‹¤ìŒë²ˆ ìš”ì²­ì„ ìœ„í•´)
        prediction_state['varmax_ma_results'] = ma_results
        
        return jsonify({
            'success': True,
            'current_date': current_date,
            'ma_results': ma_results
        })
        
    except Exception as e:
        logger.error(f"âŒ [VARMAX_MA_API] MA calculation failed: {e}")
        logger.error(f"âŒ [VARMAX_MA_API] Traceback: {traceback.format_exc()}")
        return jsonify({
            'success': False,
            'error': f'MA calculation failed: {str(e)}'
        }), 500

# 6) VARMAX ì˜ì‚¬ê²°ì • ì¡°íšŒ
@app.route('/api/varmax/saved', methods=['GET'])
def get_saved_varmax_predictions():
    """ì €ì¥ëœ VARMAX ì˜ˆì¸¡ ëª©ë¡ì„ ë°˜í™˜í•˜ëŠ” API"""
    try:
        limit = request.args.get('limit', 100, type=int)
        predictions = get_saved_varmax_predictions_list(limit=limit)
        
        return jsonify({
            'success': True,
            'predictions': predictions,
            'count': len(predictions)
        })
        
    except Exception as e:
        logger.error(f"Error getting saved VARMAX predictions: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/varmax/saved/<date>', methods=['GET'])
def get_saved_varmax_prediction_by_date(date):
    """íŠ¹ì • ë‚ ì§œì˜ ì €ì¥ëœ VARMAX ì˜ˆì¸¡ì„ ë°˜í™˜í•˜ëŠ” API"""
    global prediction_state
    
    try:
        prediction = load_varmax_prediction(date)
        
        if prediction is None:
            return jsonify({
                'success': False,
                'error': f'Prediction not found for date: {date}'
            }), 404
        
        # ğŸ” ë¡œë“œëœ ì˜ˆì¸¡ ë°ì´í„° íƒ€ì… í™•ì¸
        logger.info(f"ğŸ” [VARMAX_API_LOAD] Prediction data type: {type(prediction)}")
        
        if not isinstance(prediction, dict):
            logger.error(f"âŒ [VARMAX_API_LOAD] Prediction is not a dictionary: {type(prediction)}")
            return jsonify({
                'success': False,
                'error': f'Invalid prediction data format: expected dict, got {type(prediction).__name__}'
            }), 500
        
        # ğŸ”§ ë°±ì—”ë“œ prediction_state ë³µì›
        logger.info(f"ğŸ”„ [VARMAX_LOAD] Restoring prediction_state for date: {date}")
        logger.info(f"ğŸ” [VARMAX_LOAD] Available prediction keys: {list(prediction.keys())}")
        
        # VARMAX ìƒíƒœ ë³µì› (ì•ˆì „í•œ ì ‘ê·¼)
        prediction_state['varmax_is_predicting'] = False
        prediction_state['varmax_prediction_progress'] = 100
        prediction_state['varmax_error'] = None
        prediction_state['varmax_current_date'] = prediction.get('current_date', date)
        prediction_state['varmax_predictions'] = prediction.get('predictions', [])
        prediction_state['varmax_half_month_averages'] = prediction.get('half_month_averages', [])
        prediction_state['varmax_metrics'] = prediction.get('metrics', {})
        prediction_state['varmax_ma_results'] = prediction.get('ma_results', {})
        prediction_state['varmax_selected_features'] = prediction.get('selected_features', [])
        prediction_state['varmax_model_info'] = prediction.get('model_info', {})
        prediction_state['varmax_plots'] = prediction.get('plots', {})
        
        logger.info(f"âœ… [VARMAX_LOAD] prediction_state restored successfully")
        logger.info(f"ğŸ” [VARMAX_LOAD] Restored predictions count: {len(prediction_state['varmax_predictions'])}")
        logger.info(f"ğŸ” [VARMAX_LOAD] MA results keys: {list(prediction_state['varmax_ma_results'].keys()) if prediction_state['varmax_ma_results'] else 'None'}")
        
        return jsonify({
            'success': True,
            'prediction': prediction
        })
        
    except Exception as e:
        logger.error(f"Error getting saved VARMAX prediction for {date}: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/varmax/saved/<date>', methods=['DELETE'])
def delete_saved_varmax_prediction_api(date):
    """íŠ¹ì • ë‚ ì§œì˜ ì €ì¥ëœ VARMAX ì˜ˆì¸¡ì„ ì‚­ì œí•˜ëŠ” API"""
    try:
        success = delete_saved_varmax_prediction(date)
        
        if not success:
            return jsonify({
                'success': False,
                'error': f'Failed to delete prediction for date: {date}'
            }), 404
        
        return jsonify({
            'success': True,
            'message': f'Prediction for {date} deleted successfully'
        })
        
    except Exception as e:
        logger.error(f"Error deleting saved VARMAX prediction for {date}: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

# 6) VARMAX ì˜ì‚¬ê²°ì • ì¡°íšŒ
# 7) VARMAX ìƒíƒœ ë¦¬ì…‹ API (ìƒˆë¡œ ì¶”ê°€)
@app.route('/api/varmax/reset', methods=['POST', 'OPTIONS'])
@cross_origin()
def reset_varmax_state():
    """VARMAX ì˜ˆì¸¡ ìƒíƒœë¥¼ ë¦¬ì…‹í•˜ëŠ” API (hangëœ ì˜ˆì¸¡ í•´ê²°ìš©)"""
    global prediction_state
    
    if request.method == 'OPTIONS':
        return make_response('', 200)
    
    try:
        logger.info("ğŸ”„ [VARMAX_RESET] Resetting VARMAX prediction state...")
        
        # VARMAX ìƒíƒœ ì™„ì „ ë¦¬ì…‹
        prediction_state['varmax_is_predicting'] = False
        prediction_state['varmax_prediction_progress'] = 0
        prediction_state['varmax_error'] = None
        prediction_state['varmax_predictions'] = []
        prediction_state['varmax_half_month_averages'] = []
        prediction_state['varmax_metrics'] = {}
        prediction_state['varmax_ma_results'] = {}
        prediction_state['varmax_selected_features'] = []
        prediction_state['varmax_current_date'] = None
        prediction_state['varmax_model_info'] = {}
        prediction_state['varmax_plots'] = {}
        
        logger.info("âœ… [VARMAX_RESET] VARMAX state reset completed")
        
        return jsonify({
            'success': True,
            'message': 'VARMAX state reset successfully',
            'current_state': {
                'is_predicting': prediction_state.get('varmax_is_predicting', False),
                'progress': prediction_state.get('varmax_prediction_progress', 0),
                'error': prediction_state.get('varmax_error')
            }
        })
        
    except Exception as e:
        logger.error(f"âŒ [VARMAX_RESET] Error resetting VARMAX state: {str(e)}")
        return jsonify({
            'success': False,
            'error': f'Failed to reset VARMAX state: {str(e)}'
        }), 500

@app.route('/api/varmax/decision', methods=['POST', 'OPTIONS'])
@cross_origin() 
def get_varmax_decision():
    """VARMAX ì˜ì‚¬ ê²°ì • ì¡°íšŒ API"""
    # 1) OPTIONS(preflight) ìš”ì²­ ì²˜ë¦¬
    if request.method == 'OPTIONS':
        return make_response('', 200)
    if 'file' not in request.files:
        return jsonify({'success': False, 'error': 'íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.'}), 400

    file = request.files['file']
    # íŒŒì¼ ì €ì¥ ê²½ë¡œ ì„¤ì •
    save_dir = '/path/to/models'
    os.makedirs(save_dir, exist_ok=True)  # í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±
    filepath = os.path.join(save_dir, secure_filename(file.filename))
    file.save(filepath)

    logger.info("POST /api/varmax/decision ë¡œ ì§„ì…")
    #data = request.get_json()
    #filepath = data.get('filepath')
    """# ìœ íš¨ì„± ê²€ì‚¬
    if not filepath or not os.path.exists(os.path.normpath(filepath)):
        return jsonify({'success': False, 'error': 'Invalid file path'}), 400"""

    results = varmax_decision(filepath)
    logger.info("ê²°ê³¼ ë°ì´í„°í”„ë ˆì„ í˜•ì„± ì™„ë£Œ")
    column_order1 = ["ì¡°ê±´1", "ì¡°ê±´2", "ìƒ˜í”Œ ìˆ˜", "ìŒìˆ˜ ë¹„ìœ¨ [%]"]
    column_order2 = ["ì¡°ê±´1", "ì¡°ê±´2", "ìƒ˜í”Œ ìˆ˜", "ìƒìœ„ ë³€ë™ì„± í™•ë¥  [%]"]

    return jsonify({
        'success': True,
        'filepath': filepath,  # â† íŒŒì¼ ê²½ë¡œ ì¶”ê°€
        'filename': file.filename,
        'columns1': column_order1,
        'columns2': column_order2,
        'case_1':      results['case_1'],
        'case_2':      results['case_2'],
    })

@app.route('/api/market-status', methods=['GET'])
def get_market_status():
    """ìµœê·¼ 30ì¼ê°„ì˜ ì‹œì¥ ê°€ê²© ë°ì´í„°ë¥¼ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë°˜í™˜í•˜ëŠ” API"""
    try:
        # íŒŒì¼ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
        file_path = request.args.get('file_path')
        if not file_path:
            return jsonify({
                'success': False,
                'error': 'File path is required'
            }), 400
        
        # URL ë””ì½”ë”© ë° íŒŒì¼ ê²½ë¡œ ì •ê·œí™” (Windows ë°±ìŠ¬ë˜ì‹œ ì²˜ë¦¬)
        import urllib.parse
        file_path = urllib.parse.unquote(file_path)  # URL ë””ì½”ë”©
        file_path = os.path.normpath(file_path)
        logger.info(f"ğŸ“Š [MARKET_STATUS] Normalized file path: {file_path}")
        
        # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        if not os.path.exists(file_path):
            logger.error(f"âŒ [MARKET_STATUS] File not found: {file_path}")
            return jsonify({
                'success': False,
                'error': f'File not found: {file_path}'
            }), 400
        
        # ì›ë³¸ ë°ì´í„° ì§ì ‘ ë¡œë“œ (Date ì»¬ëŸ¼ ìœ ì§€ë¥¼ ìœ„í•´) - Excel/CSV íŒŒì¼ ëª¨ë‘ ì§€ì›
        try:
            file_ext = os.path.splitext(file_path.lower())[1]
            if file_ext == '.csv':
                df = pd.read_csv(file_path)
                logger.info(f"ğŸ“Š [MARKET_STATUS] CSV data loaded: {df.shape}")
            elif file_ext in ['.xlsx', '.xls']:
                # Excel íŒŒì¼ì˜ ê²½ìš° ë³´ì•ˆ ë¬¸ì œë¥¼ ê³ ë ¤í•œ ì•ˆì „í•œ ë¡œë”© ì‚¬ìš©
                df = load_data_safe(file_path, use_cache=True, use_xlwings_fallback=True)
                # ì¸ë±ìŠ¤ê°€ Dateì¸ ê²½ìš° ì»¬ëŸ¼ìœ¼ë¡œ ë³µì›
                if df.index.name == 'Date':
                    df = df.reset_index()
                logger.info(f"ğŸ“Š [MARKET_STATUS] Excel data loaded with security bypass: {df.shape}")
            else:
                logger.error(f"âŒ [MARKET_STATUS] Unsupported file format: {file_ext}")
                return jsonify({
                    'success': False,
                    'error': f'Unsupported file format: {file_ext}. Only CSV and Excel files are supported.'
                }), 400
        except Exception as e:
            logger.error(f"âŒ [MARKET_STATUS] Failed to load data file: {str(e)}")
            return jsonify({
                'success': False,
                'error': f'Failed to load data file: {str(e)}'
            }), 400
        
        if df is None or df.empty:
            logger.error(f"âŒ [MARKET_STATUS] No data available or empty dataframe")
            return jsonify({
                'success': False,
                'error': 'No data available'
            }), 400
        
        # ë‚ ì§œ ì»¬ëŸ¼ í™•ì¸ ë° ì •ë ¬
        logger.info(f"ğŸ“Š [MARKET_STATUS] Columns in dataframe: {list(df.columns)}")
        if 'Date' not in df.columns:
            logger.error(f"âŒ [MARKET_STATUS] Date column not found. Available columns: {list(df.columns)}")
            return jsonify({
                'success': False,
                'error': 'Date column not found in data'
            }), 400
        
        # ë‚ ì§œë¡œ ì •ë ¬
        df = df.sort_values('Date')
        
        # íœ´ì¼ ì •ë³´ ë¡œë“œ
        holidays = get_combined_holidays(df=df)
        holiday_dates = set([h['date'] if isinstance(h, dict) else h for h in holidays])
        
        # ì˜ì—…ì¼ë§Œ í•„í„°ë§
        def is_business_day(date_str):
            date_obj = pd.to_datetime(date_str).date()
            weekday = date_obj.weekday()  # 0=ì›”ìš”ì¼, 6=ì¼ìš”ì¼
            return weekday < 5 and date_str not in holiday_dates  # ì›”~ê¸ˆ & íœ´ì¼ ì•„ë‹˜
        
        logger.info(f"ğŸ“Š [MARKET_STATUS] Total rows before business day filtering: {len(df)}")
        logger.info(f"ğŸ“Š [MARKET_STATUS] Holiday dates count: {len(holiday_dates)}")
        
        business_days_df = df[df['Date'].apply(is_business_day)]
        logger.info(f"ğŸ“Š [MARKET_STATUS] Business days after filtering: {len(business_days_df)}")
        
        if business_days_df.empty:
            logger.error(f"âŒ [MARKET_STATUS] No business days found after filtering")
            return jsonify({
                'success': False,
                'error': 'No business days found in data'
            }), 400
        
        # ìµœê·¼ 30ì¼ ì˜ì—…ì¼ ë°ì´í„° ì¶”ì¶œ
        recent_30_days = business_days_df.tail(30)

        # ì¹´í…Œê³ ë¦¬ë³„ ì»¬ëŸ¼ ë¶„ë¥˜ (ì‹¤ì œ ë°ì´í„° ì»¬ëŸ¼ëª…ì— ë§ê²Œ ìˆ˜ì •)
        categories = {
            'ì›ìœ  ê°€ê²©': [
                'WTI', 'Brent', 'Dubai'
            ],
            'ê°€ì†”ë¦° ê°€ê²©': [
                'Gasoline_92RON', 'Gasoline_95RON', 'Europe_M.G_10ppm', 'RBOB (NYMEX)_M1'
            ],
            'ë‚˜í”„íƒ€ ê°€ê²©': [
                'MOPJ', 'MOPAG', 'MOPS', 'Europe_CIF NWE'
            ],
            'LPG ê°€ê²©': [
                'C3_LPG', 'C4_LPG'
            ],
            'ì„ìœ í™”í•™ ì œí’ˆ ê°€ê²©': [
                'EL_CRF NEA', 'EL_CRF SEA', 'PL_FOB Korea', 'BZ_FOB Korea', 'BZ_FOB SEA', 'BZ_FOB US M1', 'BZ_FOB US M2', 
                'TL_FOB Korea', 'TL_FOB US M1', 'TL_FOB US M2','MX_FOB Korea', 'PX_FOB Korea', 'SM_FOB Korea', 'RPG Value_FOB PG', 
                'FO_HSFO 180 CST', 'MTBE_FOB Singapore'
            ]
        }
        
        # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ í•„í„°ë§
        available_columns = set(recent_30_days.columns)
        filtered_categories = {}
        
        logger.info(f"ğŸ“Š [MARKET_STATUS] Available columns: {sorted(available_columns)}")
        
        for category, columns in categories.items():
            existing_columns = [col for col in columns if col in available_columns]
            if existing_columns:
                filtered_categories[category] = existing_columns
                logger.info(f"ğŸ“Š [MARKET_STATUS] Category '{category}': found {len(existing_columns)} columns: {existing_columns}")
            else:
                logger.warning(f"âš ï¸ [MARKET_STATUS] Category '{category}': no matching columns found from {columns}")
        
        if not filtered_categories:
            logger.error(f"âŒ [MARKET_STATUS] No categories found! Expected columns don't match available columns")
            return jsonify({
                'success': False,
                'error': 'No matching columns found for market status categories',
                'debug_info': {
                    'available_columns': sorted(available_columns),
                    'expected_categories': categories
                }
            }), 400
        
        # ì¹´í…Œê³ ë¦¬ë³„ ë°ì´í„° êµ¬ì„±
        result = {
            'success': True,
            'date_range': {
                'start_date': recent_30_days['Date'].iloc[0],
                'end_date': recent_30_days['Date'].iloc[-1],
                'total_days': len(recent_30_days)
            },
            'categories': {}
        }
        
        for category, columns in filtered_categories.items():
            category_data = {
                'columns': columns,
                'data': []
            }
            
            for _, row in recent_30_days.iterrows():
                data_point = {
                    'date': row['Date'],
                    'values': {}
                }
                
                for col in columns:
                    if pd.notna(row[col]):
                        data_point['values'][col] = float(row[col])
                    else:
                        data_point['values'][col] = None
                
                category_data['data'].append(data_point)
            
            result['categories'][category] = category_data
        
        logger.info(f"âœ… [MARKET_STATUS] Returned {len(recent_30_days)} business days data for {len(filtered_categories)} categories")
        return jsonify(result)
        
    except Exception as e:
        logger.error(f"âŒ [MARKET_STATUS] Error: {str(e)}")
        return jsonify({
            'success': False,
            'error': f'Failed to get market status: {str(e)}'
        }), 500

@app.route('/api/gpu-info', methods=['GET'])
def get_gpu_info():
    """GPU ë° ë””ë°”ì´ìŠ¤ ì •ë³´ë¥¼ ë°˜í™˜í•˜ëŠ” API"""
    try:
        # ì‹¤ì‹œê°„ GPU í…ŒìŠ¤íŠ¸ ì—¬ë¶€ í™•ì¸
        run_test = request.args.get('test', 'false').lower() == 'true'
        
        # GPU ì •ë³´ ìˆ˜ì§‘
        device_info = {
            'cuda_available': torch.cuda.is_available(),
            'pytorch_version': torch.__version__,
            'default_device': str(DEFAULT_DEVICE),
            'current_device_info': {},
            'test_performed': False,
            'test_results': {}
        }
        
        if torch.cuda.is_available():
            gpu_count = torch.cuda.device_count()
            current_device = torch.cuda.current_device()
            
            # ì‹¤ì‹œê°„ GPU í™œìš©ë¥  í™•ì¸ (ìƒì„¸ ë²„ì „)
            gpu_utilization_stats = get_detailed_gpu_utilization()
            
            device_info.update({
                'gpu_count': gpu_count,
                'current_gpu_device': current_device,
                'cudnn_version': torch.backends.cudnn.version(),
                'cudnn_enabled': torch.backends.cudnn.enabled,
                'detailed_utilization': gpu_utilization_stats,
                'gpus': []
            })
            
            # ê° GPU ì •ë³´
            for i in range(gpu_count):
                gpu_props = torch.cuda.get_device_properties(i)
                allocated = torch.cuda.memory_allocated(i) / 1024**3
                cached = torch.cuda.memory_reserved(i) / 1024**3
                total = gpu_props.total_memory / 1024**3
                
                # PyTorch ë²„ì „ í˜¸í™˜ì„±ì„ ìœ„í•œ ì•ˆì „í•œ ì†ì„± ì ‘ê·¼
                gpu_info = {
                    'device_id': i,
                    'name': getattr(gpu_props, 'name', 'Unknown GPU'),
                    'total_memory_gb': round(total, 2),
                    'allocated_memory_gb': round(allocated, 2),
                    'cached_memory_gb': round(cached, 2),
                    'memory_usage_percent': round((allocated / total) * 100, 2),
                    'compute_capability': f"{getattr(gpu_props, 'major', 0)}.{getattr(gpu_props, 'minor', 0)}",
                    'is_current': i == current_device
                }
                
                # ì„ íƒì  ì†ì„±ë“¤ (PyTorch ë²„ì „ì— ë”°ë¼ ì¡´ì¬í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ)
                if hasattr(gpu_props, 'multiprocessor_count'):
                    gpu_info['multiprocessor_count'] = gpu_props.multiprocessor_count
                elif hasattr(gpu_props, 'multi_processor_count'):
                    gpu_info['multiprocessor_count'] = gpu_props.multi_processor_count
                else:
                    gpu_info['multiprocessor_count'] = 'N/A'
                
                # ì¶”ê°€ GPU ì†ì„±ë“¤ (ì¡´ì¬í•˜ëŠ” ê²½ìš°ì—ë§Œ)
                optional_attrs = {
                    'max_threads_per_block': 'max_threads_per_block',
                    'max_threads_per_multiprocessor': 'max_threads_per_multiprocessor',
                    'warp_size': 'warp_size',
                    'memory_clock_rate': 'memory_clock_rate'
                }
                
                for attr_name, prop_name in optional_attrs.items():
                    if hasattr(gpu_props, prop_name):
                        gpu_info[attr_name] = getattr(gpu_props, prop_name)
                
                device_info['gpus'].append(gpu_info)
            
            # í˜„ì¬ ë””ë°”ì´ìŠ¤ ìƒì„¸ ì •ë³´
            current_gpu_props = torch.cuda.get_device_properties(current_device)
            device_info['current_device_info'] = {
                'name': current_gpu_props.name,
                'total_memory_gb': round(current_gpu_props.total_memory / 1024**3, 2),
                'allocated_memory_gb': round(torch.cuda.memory_allocated(current_device) / 1024**3, 2),
                'cached_memory_gb': round(torch.cuda.memory_reserved(current_device) / 1024**3, 2)
            }
            
            # GPU í…ŒìŠ¤íŠ¸ ìˆ˜í–‰ (ìš”ì²­ëœ ê²½ìš°)
            if run_test:
                try:
                    logger.info("ğŸ§ª APIì—ì„œ GPU í…ŒìŠ¤íŠ¸ ìˆ˜í–‰ ì¤‘...")
                    
                    # í…ŒìŠ¤íŠ¸ ì „ ë©”ëª¨ë¦¬ ìƒíƒœ
                    memory_before = {
                        'allocated': torch.cuda.memory_allocated(current_device) / 1024**3,
                        'cached': torch.cuda.memory_reserved(current_device) / 1024**3
                    }
                    
                    # ê°„ë‹¨í•œ GPU ì—°ì‚° í…ŒìŠ¤íŠ¸
                    test_size = 500
                    test_tensor = torch.randn(test_size, test_size, device=current_device, dtype=torch.float32)
                    test_result = torch.matmul(test_tensor, test_tensor.T)
                    computation_result = torch.sum(test_result).item()
                    
                    # í…ŒìŠ¤íŠ¸ í›„ ë©”ëª¨ë¦¬ ìƒíƒœ
                    memory_after = {
                        'allocated': torch.cuda.memory_allocated(current_device) / 1024**3,
                        'cached': torch.cuda.memory_reserved(current_device) / 1024**3
                    }
                    
                    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì°¨ì´ ê³„ì‚°
                    memory_diff = {
                        'allocated_diff': memory_after['allocated'] - memory_before['allocated'],
                        'cached_diff': memory_after['cached'] - memory_before['cached']
                    }
                    
                    device_info['test_performed'] = True
                    device_info['test_results'] = {
                        'test_tensor_size': f"{test_size}x{test_size}",
                        'computation_result': round(computation_result, 4),
                        'memory_before_gb': {
                            'allocated': round(memory_before['allocated'], 4),
                            'cached': round(memory_before['cached'], 4)
                        },
                        'memory_after_gb': {
                            'allocated': round(memory_after['allocated'], 4),
                            'cached': round(memory_after['cached'], 4)
                        },
                        'memory_diff_gb': {
                            'allocated': round(memory_diff['allocated_diff'], 4),
                            'cached': round(memory_diff['cached_diff'], 4)
                        },
                        'test_success': True
                    }
                    
                    # í…ŒìŠ¤íŠ¸ í…ì„œ ì •ë¦¬
                    del test_tensor, test_result
                    torch.cuda.empty_cache()
                    
                    # ì •ë¦¬ í›„ ë©”ëª¨ë¦¬ ìƒíƒœ
                    memory_final = {
                        'allocated': torch.cuda.memory_allocated(current_device) / 1024**3,
                        'cached': torch.cuda.memory_reserved(current_device) / 1024**3
                    }
                    
                    device_info['test_results']['memory_after_cleanup_gb'] = {
                        'allocated': round(memory_final['allocated'], 4),
                        'cached': round(memory_final['cached'], 4)
                    }
                    
                    logger.info(f"âœ… GPU í…ŒìŠ¤íŠ¸ ì™„ë£Œ: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë³€í™” {memory_diff['allocated_diff']:.4f}GB")
                    
                except Exception as test_e:
                    logger.error(f"âŒ GPU í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(test_e)}")
                    device_info['test_performed'] = True
                    device_info['test_results'] = {
                        'test_success': False,
                        'error': str(test_e)
                    }
        else:
            device_info.update({
                'gpu_count': 0,
                'reason': 'CUDA not available - using CPU'
            })
        
        # ë¡œê·¸ì—ë„ ì •ë³´ ì¶œë ¥
        logger.info(f"ğŸ” GPU Info API í˜¸ì¶œ:")
        logger.info(f"  ğŸ”§ CUDA ì‚¬ìš© ê°€ëŠ¥: {device_info['cuda_available']}")
        logger.info(f"  âš¡ ê¸°ë³¸ ë””ë°”ì´ìŠ¤: {device_info['default_device']}")
        if device_info['cuda_available']:
            logger.info(f"  ğŸ® GPU ê°œìˆ˜: {device_info.get('gpu_count', 0)}")
            if 'current_gpu_device' in device_info:
                logger.info(f"  ğŸ¯ í˜„ì¬ GPU: {device_info['current_gpu_device']}")
        
        # í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¡œê¹…
        if device_info.get('test_performed', False):
            test_results = device_info.get('test_results', {})
            if test_results.get('test_success', False):
                logger.info(f"  âœ… GPU í…ŒìŠ¤íŠ¸ ì„±ê³µ")
            else:
                logger.warning(f"  âŒ GPU í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {test_results.get('error', 'Unknown error')}")
        
        return jsonify({
            'success': True,
            'device_info': device_info,
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"âŒ GPU ì •ë³´ API ì˜¤ë¥˜: {str(e)}")
        return jsonify({
            'success': False,
            'error': f'Failed to get GPU info: {str(e)}'
        }), 500

@app.route('/api/gpu-monitoring-comparison', methods=['GET'])
def get_gpu_monitoring_comparison():
    """ë‹¤ì–‘í•œ GPU ëª¨ë‹ˆí„°ë§ ë°©ë²•ì„ ë¹„êµí•˜ëŠ” API"""
    try:
        comparison_data = compare_gpu_monitoring_methods()
        
        # ì¶”ê°€ì ì¸ ì„¤ëª… ì •ë³´
        explanation = {
            'why_different_readings': [
                "Windows ì‘ì—… ê´€ë¦¬ìëŠ” ì£¼ë¡œ 3D ê·¸ë˜í”½ ì—”ì§„ í™œìš©ë¥ ì„ í‘œì‹œí•©ë‹ˆë‹¤",
                "nvidia-smiëŠ” CUDA ì—°ì‚° í™œìš©ë¥ ì„ ì¸¡ì •í•˜ë¯€ë¡œ ML/AI ì‘ì—…ì— ë” ì •í™•í•©ë‹ˆë‹¤",
                "ì¸¡ì • ì‹œì ì˜ ì°¨ì´ë¡œ ì¸í•´ ìˆœê°„ì ì¸ ê°’ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤",
                "GPUëŠ” ì—¬ëŸ¬ ì—”ì§„(Compute, 3D, Encoder, Decoder)ì„ ê°€ì§€ê³  ìˆì–´ ê°ê° ë‹¤ë¥¸ í™œìš©ë¥ ì„ ë³´ì…ë‹ˆë‹¤"
            ],
            'recommendations': [
                "ML/AI ì‘ì—…: nvidia-smiì˜ GPU í™œìš©ë¥  í™•ì¸",
                "ê²Œì„/3D ë Œë”ë§: Windows ì‘ì—… ê´€ë¦¬ìì˜ 3D í™œìš©ë¥  í™•ì¸", 
                "ë¹„ë””ì˜¤ ì²˜ë¦¬: nvidia-smiì˜ Encoder/Decoder í™œìš©ë¥  í™•ì¸",
                "ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: PyTorch CUDA ì •ë³´ì™€ nvidia-smi ëª¨ë‘ í™•ì¸"
            ],
            'task_manager_vs_nvidia_smi': {
                "ì‘ì—… ê´€ë¦¬ì GPU": "ì£¼ë¡œ 3D ê·¸ë˜í”½ ì›Œí¬ë¡œë“œ (DirectX, OpenGL)",
                "nvidia-smi GPU": "CUDA ì—°ì‚° ì›Œí¬ë¡œë“œ (ML, AI, GPGPU)",
                "ì™œ ë‹¤ë¥¸ê°€": "ì„œë¡œ ë‹¤ë¥¸ GPU ì—”ì§„ì„ ì¸¡ì •í•˜ê¸° ë•Œë¬¸",
                "ì–´ëŠ ê²ƒì´ ì •í™•í•œê°€": "ì‘ì—… ìœ í˜•ì— ë”°ë¼ ë‹¤ë¦„ - ML/AIëŠ” nvidia-smiê°€ ì •í™•"
            }
        }
        
        # í˜„ì¬ ìƒí™© ë¶„ì„
        current_analysis = {
            'status': 'monitoring_successful',
            'notes': []
        }
        
        if comparison_data.get('nvidia_smi'):
            nvidia_util = comparison_data['nvidia_smi'].get('gpu_utilization', '0')
            try:
                util_value = float(nvidia_util)
                if util_value < 10:
                    current_analysis['notes'].append(f"í˜„ì¬ CUDA í™œìš©ë¥ ì´ ë§¤ìš° ë‚®ìŠµë‹ˆë‹¤ ({util_value}%)")
                    current_analysis['notes'].append("ì´ëŠ” ì •ìƒì ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤ - ML ì‘ì—…ì´ ì§„í–‰ ì¤‘ì´ ì•„ë‹ ë•Œ")
                elif util_value > 50:
                    current_analysis['notes'].append(f"í˜„ì¬ CUDA í™œìš©ë¥ ì´ ë†’ìŠµë‹ˆë‹¤ ({util_value}%)")
                    current_analysis['notes'].append("ML/AI ì‘ì—…ì´ í™œë°œíˆ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤")
            except:
                pass
        
        if comparison_data.get('torch_cuda'):
            memory_usage = comparison_data['torch_cuda'].get('memory_usage_percent', 0)
            if memory_usage > 1:
                current_analysis['notes'].append(f"PyTorchê°€ GPU ë©”ëª¨ë¦¬ë¥¼ ì‚¬ìš© ì¤‘ì…ë‹ˆë‹¤ ({memory_usage:.1f}%)")
            else:
                current_analysis['notes'].append("PyTorchê°€ í˜„ì¬ GPU ë©”ëª¨ë¦¬ë¥¼ ê±°ì˜ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
        
        return jsonify({
            'success': True,
            'comparison_data': comparison_data,
            'explanation': explanation,
            'current_analysis': current_analysis,
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"âŒ GPU ëª¨ë‹ˆí„°ë§ ë¹„êµ API ì˜¤ë¥˜: {str(e)}")
        return jsonify({
            'success': False,
            'error': f'Failed to compare GPU monitoring methods: {str(e)}'
        }), 500

# ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„ ì—…ë°ì´íŠ¸
if __name__ == '__main__':
    # í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜ í™•ì¸
    try:
        import optuna
    except ImportError:
        logger.warning("Optuna íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”ë¥¼ ìœ„í•´ https://inthiswork.com/archives/226539ì„¤ì¹˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        logger.warning("pip install optuna ëª…ë ¹ìœ¼ë¡œ ì„¤ì¹˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
    # ğŸ¯ íŒŒì¼ë³„ ìºì‹œ ì‹œìŠ¤í…œ - ë ˆê±°ì‹œ ë””ë ‰í† ë¦¬ ë° ì¸ë±ìŠ¤ íŒŒì¼ ìƒì„± ì œê±°
    # ëª¨ë“  ë°ì´í„°ëŠ” ì´ì œ íŒŒì¼ë³„ ìºì‹œ ë””ë ‰í† ë¦¬ì— ì €ì¥ë©ë‹ˆë‹¤
    logger.info("ğŸš€ Starting with file-based cache system - no legacy directories needed")
    
    # ë¼ìš°íŠ¸ ë“±ë¡ í™•ì¸ì„ ìœ„í•œ ë””ë²„ê¹…
    print("Registered routes:")
    for rule in app.url_map.iter_rules():
        print(f"  {rule.endpoint}: {rule.rule} {list(rule.methods)}")
    
    print("Starting Flask app with attention-map endpoint...")
    app.run(debug=True, host='0.0.0.0', port=5000, threaded=True)
